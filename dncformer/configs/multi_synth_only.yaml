# dncformer/configs/multi_synth_only.yaml
# synthetic only curriculum
seed: 2027
label: multi_synth_only
base_model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0

# runtime / training
precision: bf16
batch_size: 8
train_steps: 100
warmup_steps: 150
lr: 2.0e-4
weight_decay: 0.01
grad_clip: 1.0

# LoRA (narrow, safe defaults)
lora_enable: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
# If your LoRA patch exposes target modules, keep it default or set explicitly:
# lora_target_modules: ["q_proj","k_proj","v_proj","o_proj","fc1","fc2","wq","wk","wv","wo","gate_proj","up_proj","down_proj"]

# anti‑collapse (router/gates); conservative but on
router_balance_lambda: 3.0e-3     # A1: soft balance across parallel experts
router_noise_std: 0.03            # A2: tiny Gaussian noise on router logits (train only)
expert_dropout_p: 0.02            # drop an expert’s contribution at small prob
block_memdrop_prob: 0.00          # (sequential multi‑block only; keep zero here)
cross_block_balance_lambda: 0.0   # (sequential multi‑block only)

data:
  # Hold a task for a short chunk; helps convergence & stabilizes batch stats
  sticky_mix: 20  # steps per task “chunk” (set 0/omit to disable)

  # How many examples to pull from each HF source at build time (not a hard cap per epoch)
  hf_max_items: 8000

  # Task blend — **weights are normalized** internally.
  # Names appear in logs; type selects loader; params map to generator args.
  tasks:
    # MEMORY
    - name: copy
      type: synth
      kind: copy

    - name: repeat
      type: synth
      kind: repeat

    - name: n-back
      type: synth
      kind: n-back

    # ALGORITHMIC
    - name: dyck
      type: synth
      kind: dyck
      params: { depth: 20, T: 128 }

    - name: stack
      type: synth
      kind: stack_ops
      params: { ops: 64, T: 128 }

    - name: sort
      type: synth
      kind: sort_list
      params: { length: 100, vocab: 500, T: 128 }

