label: E18b_sequential
seed: 1337

# model
base_model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
n_blocks: 2
mem_experts: 1
per_block_cfg:
  - { N: 192, W: 32, R: 1, gate_temp: 1.0, free_bias: 0.20 }   # block 0 (many/shallow)
  - { N:  64, W: 64, R: 1, gate_temp: 0.9, free_bias: -0.20 }  # block 1 (few/deep)

# anti-collapse (sequential)
router_balance_lambda: 0.0
router_noise_std: 0.02
expert_dropout_p: 0.0
block_memdrop_prob: 0.03
cross_block_balance_lambda: 0.002

# LoRA
lora_enable: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj","k_proj","v_proj","o_proj","fc1","fc2","wq","wk","wv","wo","gate_proj","up_proj","down_proj"]

# train
precision: bf16
device: cuda
train_steps: 1000
batch_size: 8
warmup_steps: 50
lr: 2.0e-4
weight_decay: 0.01
lr_min_ratio: 0.10

# data
mixture: [0.0, 0.34, 0.33, 0.33]   # memory-first curriculum
hf_dataset: null
hf_max_items: 0
sticky_mix: 200
