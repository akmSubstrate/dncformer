{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "This notebook implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:48.822588100Z",
     "start_time": "2025-08-18T01:13:46.537079100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n",
    "\n",
    "# SDPA configured via SDPA_CTX() above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7891e",
   "metadata": {},
   "source": [
    "## 1. Config & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:48.864587200Z",
     "start_time": "2025-08-18T01:13:48.825587900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n",
      "AMP dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "import os, math, random, torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "print(\"AMP dtype:\", amp_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# --- Config patch toggles ---\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class _Tmp: pass\n",
    "    CFG = _Tmp()\n",
    "# safe defaults if not already present\n",
    "if not hasattr(CFG, 'batch_size'): CFG.batch_size = 8\n",
    "if not hasattr(CFG, 'gate_reg_lambda'): CFG.gate_reg_lambda = 0.0   # only applied on memory-tagged batches\n",
    "if not hasattr(CFG, 'hist_every'): CFG.hist_every = 200             # histogram cadence\n",
    "if not hasattr(CFG, 'force_g'): CFG.force_g = None                  # None, or 0.0 or 1.0\n",
    "if not hasattr(CFG, 'gate_temp'): CFG.gate_temp = 1.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:48.877587Z",
     "start_time": "2025-08-18T01:13:48.855587700Z"
    }
   },
   "id": "682d4e57"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:49.150587700Z",
     "start_time": "2025-08-18T01:13:49.133587100Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- SDPA selection (prefer PyTorch SDPA; avoid flash-attn) ---\n",
    "import contextlib\n",
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:50.280622400Z",
     "start_time": "2025-08-18T01:13:50.268622400Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n",
    "\n",
    "def get_or_build_model_and_tokenizer():\n",
    "    mid = CFG.base_model_id\n",
    "    if _GLOBAL.get(\"head\") is not None and _GLOBAL.get(\"mid\") == mid:\n",
    "        # Reuse the already-loaded GPU model\n",
    "        return _GLOBAL[\"tok\"], _GLOBAL[\"head\"]\n",
    "    # Otherwise build fresh\n",
    "    tok, base = load_base_model(mid)             # your existing GPU-only loader\n",
    "    head = DNCFormerHead(base, CFG).to(device)   # enrichment head on CUDA\n",
    "    _GLOBAL.update({\"tok\": tok, \"head\": head, \"mid\": mid})\n",
    "    return tok, head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 3. DNC Memory (compact reference implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:51.139646200Z",
     "start_time": "2025-08-18T01:13:51.106647500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        # aggregated lightweight stats (per-step)\n",
    "        try:\n",
    "            _stats = {}\n",
    "            with torch.no_grad():\n",
    "                _stats[\"u_mean\"] = state[\"u\"].mean().detach()\n",
    "                try:\n",
    "                    _stats[\"M_norm_mean\"] = state[\"M\"].norm(dim=-1).mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"rw_max_mean\"] = rw.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"ww_max_mean\"] = ww.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            state[\"stats\"] = _stats\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe(\n",
    "                {\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1), # (B,)\n",
    "                }\n",
    "            )\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "## 4. Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:51.847647100Z",
     "start_time": "2025-08-18T01:13:51.834646600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "## 5. DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:52.551646100Z",
     "start_time": "2025-08-18T01:13:52.518646400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "## 6. Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:53.035706600Z",
     "start_time": "2025-08-18T01:13:53.013707200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None, gate_override: None = None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None, gate_override: None = None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        \n",
    "        # collect per-block metrics if requested\n",
    "        if self.collect_metrics:\n",
    "            try:\n",
    "                import math, torch as _t\n",
    "                eps = 1e-6\n",
    "                g_clamp = g.clamp(min=eps, max=1-eps)\n",
    "                g_entropy = (-(g_clamp*_t.log(g_clamp) + (1-g_clamp)*_t.log(1-g_clamp))).mean()\n",
    "                vt_norm = vt.norm(dim=-1).mean()\n",
    "                dt_norm = dt.norm(dim=-1).mean()\n",
    "                _stats = dnc_state.get(\"stats\", {}) if isinstance(dnc_state, dict) else {}\n",
    "                # ensure CPU-detached tiny tensors\n",
    "                self.last_metrics = {\n",
    "                    \"g_mean\": g.mean().detach(),\n",
    "                    \"g_entropy\": g_entropy.detach(),\n",
    "                    \"vt_norm\": vt_norm.detach(),\n",
    "                    \"dt_norm\": dt_norm.detach(),\n",
    "                    **_stats\n",
    "                }\n",
    "            except Exception:\n",
    "                self.last_metrics = None\n",
    "        else:\n",
    "            self.last_metrics = None\n",
    "            return out, dnc_state, g \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "## 7. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:56.717737700Z",
     "start_time": "2025-08-18T01:13:53.664737200Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def spda_ctx():\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception as e:\n",
    "        return f\"error {e} encountered ->\\n{contextlib.nullcontext()}\"\n",
    "\n",
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with spda_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n",
    "\n",
    "\n",
    "\n",
    "    def forward_with_metrics(self, input_ids: torch.Tensor, attention_mask: \"Optional[torch.Tensor]\" = None,\n",
    "                             gate_override: \"Optional[float]\" = None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1].to(device)  # ensure CUDA\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates_det = []\n",
    "        gates_raw = []\n",
    "        per_block = []\n",
    "        # enable metrics collection per block\n",
    "        for blk in self.blocks:\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = True\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i], gate_override=gate_override)\n",
    "            gates_raw.append(g)\n",
    "            gates_det.append(g.detach())\n",
    "            if hasattr(blk, \"last_metrics\") and blk.last_metrics is not None:\n",
    "                per_block.append(blk.last_metrics)\n",
    "            else:\n",
    "                per_block.append({})\n",
    "            # reset flag to avoid overhead elsewhere\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = False\n",
    "        # lm head on its own device, then back to CUDA\n",
    "        lm_dev = self.base.lm_head.weight.device\n",
    "        y = self.proj_out(h).to(lm_dev, dtype=self.base.lm_head.weight.dtype)\n",
    "        logits = self.base.lm_head(y).to(device)\n",
    "        aux = {\"per_block\": per_block, \"gates_raw\": gates_raw, \"gates_detached\": gates_det}\n",
    "        return logits, gates_det, aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 8. Data: Synthetic tasks + simple instruction-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:56.734737600Z",
     "start_time": "2025-08-18T01:13:56.717737700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0322be9",
   "metadata": {},
   "source": [
    "## 9. Training loop (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:57.955938400Z",
     "start_time": "2025-08-18T01:13:57.933940800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Synthetic generators + HF dataset integration + MixtureSampler (with tagging) ---\n",
    "import random, torch\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n",
    "\n",
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\"); out = ex.get(o_field, \"\")\n",
    "        if instr and out: pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items: break\n",
    "    random.shuffle(pairs); return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]; out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n",
    "\n",
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float], names: Optional[List[str]] = None):\n",
    "        self.gens = gens\n",
    "        import torch as _t\n",
    "        self.p = _t.tensor(weights, dtype=_t.float)\n",
    "        self.p /= self.p.sum()\n",
    "        self.names = names if names is not None else [f\"g{i}\" for i in range(len(gens))]\n",
    "        self.last_name = None\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch as _t\n",
    "        idx = _t.multinomial(self.p, 1).item()\n",
    "        self.last_name = self.names[idx]\n",
    "        return self.gens[idx](batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6be4ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:58.642940300Z",
     "start_time": "2025-08-18T01:13:58.616940400Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b44d73c717b8b58c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:13:59.409975200Z",
     "start_time": "2025-08-18T01:13:59.239974400Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- TensorBoard Logger (lightweight) ---\n",
    "import os, time\n",
    "from typing import List, Optional\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        if not self.enabled:\n",
    "            self.writer = None\n",
    "            return\n",
    "        if logdir is None:\n",
    "            logdir = \"./runs\"\n",
    "        if run_name is None:\n",
    "            run_name = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not self.enabled: return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not self.enabled: return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n",
    "\n",
    "\n",
    "def add_histogram(self, tag: str, values: \"torch.Tensor\", step: int, bins: int = 50):\n",
    "    if not self.enabled: return\n",
    "    import torch\n",
    "    v = values\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        v = v.detach()\n",
    "        if v.device.type != \"cpu\":\n",
    "            v = v.cpu()\n",
    "        v = v.reshape(-1).float()\n",
    "    self.writer.add_histogram(tag, v, global_step=step, bins=bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70a7bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:00.042005500Z",
     "start_time": "2025-08-18T01:14:00.031005400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    #print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(head, tok):\n",
    "    head.eval()\n",
    "    prompt = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(enc.input_ids)\n",
    "    print(\"Gates (mean):\", [g.mean().item() for g in gates])\n",
    "    return gates\n",
    "\n",
    "def train_small():\n",
    "    tok, head = get_or_build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, CFG.warmup_steps, CFG.train_steps, min_lr_ratio=0.10)\n",
    "    head.train()\n",
    "\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    for step in range(1, CFG.train_steps+1):\n",
    "        in_ids, lab_ids = tokenize_instruction_pairs(tok, INSTR_PAIRS, max_len=min(CFG.max_seq_len, 256))\n",
    "        in_ids = in_ids.to(device)\n",
    "        \n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            logits, gates = head(in_ids)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step()\n",
    "            \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % CFG.log_every == 0:\n",
    "            gm = [g.mean().item() for g in gates]\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e} | gates={gm}\")\n",
    "            tb.log_scalars(step, float(loss.item()), float(lr), gm)\n",
    "            \n",
    "    evaluate_simple(head, tok)\n",
    "    return head, tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9ee90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 10. Eval harness (needle-in-haystack, copy, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "028e39a3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:01.231041900Z",
     "start_time": "2025-08-18T01:14:01.221041400Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Memory tracer & TensorBoard visualizer ---\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62ef5c5785017b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GPU allocation sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2af2ed601f9c7606",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:02.190040400Z",
     "start_time": "2025-08-18T01:14:02.180044200Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- GPU VRAM diagnostics ---\n",
    "import torch, gc, contextlib\n",
    "\n",
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "\n",
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dc850ece5144e62",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:03.018041900Z",
     "start_time": "2025-08-18T01:14:02.856043300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n",
      "[liveness] DNCFormerHead instances alive: 0\n",
      "[liveness] no global 'head'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_128372\\2359170240.py:15: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n"
     ]
    }
   ],
   "source": [
    "cuda_report(\"before\")\n",
    "list_head_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be0b78c71554906c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:03.749041500Z",
     "start_time": "2025-08-18T01:14:03.731043400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_copy batch shape: torch.Size([3, 128]) | dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# generator smoke test\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f0be150819cf745",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:10.074597500Z",
     "start_time": "2025-08-18T01:14:04.243070400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d003bedfc18743b4a835780503dbaea4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=7.64 GB | reserved=7.64 GB | free=16.79 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e27677e1220b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Medium-size training loop test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d629080a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:11.660624300Z",
     "start_time": "2025-08-18T01:14:11.649625700Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000) -> MixtureSampler:\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "\n",
    "    pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\",\"output\"), max_items=hf_max_items)\n",
    "    gens = []; wts = []; names = []\n",
    "\n",
    "    if pairs:\n",
    "        def gen_hf(b): return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "        gens.append(gen_hf); wts.append(weights[0]); names.append(\"hf\")\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        print(\"HF dataset unavailable or empty; using synthetic only.\")\n",
    "        s_w = [1.0, 1.0, 1.0]\n",
    "\n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100, pad_id=pad_id, device=\"cpu\")\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback]); wts.extend(s_w); names.extend([\"copy\",\"repeat\",\"nback\"])\n",
    "    return MixtureSampler(gens, wts, names=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a83b2d08c4a137e7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:13.164682100Z",
     "start_time": "2025-08-18T01:14:13.023681700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e53881",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:13.854681300Z",
     "start_time": "2025-08-18T01:14:13.828683100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train_medium(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.10,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 2000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = False,   # keep off by default; use visualize_memory_tb() ad‑hoc if desired\n",
    "    viz_prompt: str = \"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "):\n",
    "    # --- resolve config / defaults robustly ---\n",
    "    cfg = CFG\n",
    "    steps = int(steps if steps is not None else getattr(cfg, \"train_steps\", 200))\n",
    "    batch_size = int(batch_size if batch_size is not None else getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "    grad_clip = float(getattr(cfg, \"grad_clip\", 1.0))\n",
    "    hist_every = int(getattr(cfg, \"hist_every\", 200))\n",
    "    force_g = getattr(cfg, \"force_g\", None)\n",
    "    gate_reg_lambda = float(getattr(cfg, \"gate_reg_lambda\", 0.0))\n",
    "\n",
    "    # --- build model, optimizer, scheduler ---\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "    head.train()\n",
    "\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype != torch.float32)):\n",
    "            # forward with metrics + optional ablation override of gate\n",
    "            logits, gates, aux = head.forward_with_metrics(in_ids, gate_override=force_g)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "            # optional: encourage memory usage on memory-tagged batches only\n",
    "            if gate_reg_lambda > 0.0 and mixer.last_name in (\"copy\", \"repeat\", \"nback\"):\n",
    "                try:\n",
    "                    g_mean_all = torch.stack([g.mean() for g in aux[\"gates_raw\"]]).mean()\n",
    "                    loss = loss + gate_reg_lambda * (1.0 - g_mean_all)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), grad_clip)\n",
    "            scaler.step(optim); scaler.update(); scheduler.step()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), grad_clip)\n",
    "            optim.step(); scheduler.step()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            gm = [g.mean().item() for g in gates]\n",
    "            lr = optim.param_groups[0][\"lr\"]\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e} | gates={gm} | mix={mixer.last_name}\")\n",
    "            tb.log_scalars(step, float(loss.item()), float(lr), gm)\n",
    "            \n",
    "            # Tag current batch\n",
    "            mix_name = mixer.last_name or \"unknown\"\n",
    "            # Break down + log loss per task for per-task curve views\n",
    "            tb.writer.add_scalar(f\"loss_by_task/{mix_name}\", float(loss.item()), step)\n",
    "            \n",
    "            # Per-block gate means, \"token fractions with g>0.5\" per task, and mean gate by temporal quartile for block 0\n",
    "            try:\n",
    "                for bi, g in enumerate(aux[\"gates detached\"]):\n",
    "                    g_mean_task = float(g.mean().item())\n",
    "                    g_frac = float((g >0.5).float().mean().item())\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_mean/{mix_name}\", g_mean_task, step)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_frac>0.5/{mix_name}\", g_frac, step)\n",
    "                    \n",
    "                    g0 = aux[\"gates_detached\"][0]\n",
    "                    if g0.dim() == 3:\n",
    "                        g0 = g0.mean(dim=-1)\n",
    "                    T = g0.size(1)\n",
    "                    q = T // 4 if T >= 4 else 1\n",
    "                    for qi, s1 in enumerate([(0,q), (q,2*q), (2*q,3*q), (3*q,T)]):\n",
    "                        s, e = s1\n",
    "                        tb.writer.add_scalar(f\"gates/block0_q{qi+1}_mean/{mix_name}\", float(g0[:, s:e].mean().item()), step)\n",
    "                    \n",
    "            except Exception as exception:\n",
    "                print(f\"Exception '{exception}' encountered while reporting pre-block gate means and g>0.5 token fracions \\n This is a non-breaking exception\") # extra context, may not be informative\n",
    "                pass\n",
    "                \n",
    "                    \n",
    "            # extra per‑block scalars (guarded)\n",
    "            try:\n",
    "                for bi, pm in enumerate(aux.get(\"per_block\", [])):\n",
    "                    if \"g_entropy\"   in pm: tb.writer.add_scalar(f\"gates/block_{bi}_entropy\",   float(pm[\"g_entropy\"].item()), step)\n",
    "                    if \"u_mean\"      in pm: tb.writer.add_scalar(f\"memory/block_{bi}_u_mean\",    float(pm[\"u_mean\"].item()), step)\n",
    "                    if \"rw_max_mean\" in pm: tb.writer.add_scalar(f\"memory/block_{bi}_rw_max_mean\",float(pm[\"rw_max_mean\"].item()), step)\n",
    "                    if \"ww_max_mean\" in pm: tb.writer.add_scalar(f\"memory/block_{bi}_ww_max_mean\",float(pm[\"ww_max_mean\"].item()), step)\n",
    "                    if \"M_norm_mean\" in pm: tb.writer.add_scalar(f\"memory/block_{bi}_Mnorm_mean\", float(pm[\"M_norm_mean\"].item()), step)\n",
    "                    if \"vt_norm\"     in pm: tb.writer.add_scalar(f\"paths/block_{bi}_vt_norm\",     float(pm[\"vt_norm\"].item()), step)\n",
    "                    if \"dt_norm\"     in pm: tb.writer.add_scalar(f\"paths/block_{bi}_dt_norm\",     float(pm[\"dt_norm\"].item()), step)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # gate histograms (cadence)\n",
    "            try:\n",
    "                if hist_every > 0 and (step % hist_every == 0):\n",
    "                    for bi, g in enumerate(aux[\"gates_detached\"]):\n",
    "                        tb.add_histogram(f\"gates/block_{bi}_hist\", g, step)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # optional memory snapshot into the same TB run (off by default)\n",
    "    if viz_memory_after and TB_AVAILABLE:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "\n",
    "    tb.flush(); tb.close()\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:15.284682Z",
     "start_time": "2025-08-18T01:14:15.255681300Z"
    }
   },
   "id": "f8879a898faf706a"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# # --- Patch: robust forward for ParallelEnrichmentBlock ---\n",
    "def _peb_forward(self, x, dnc_state=None, gate_override=None):\n",
    "    \"\"\"\n",
    "    Returns: (out, dnc_state, g)\n",
    "      out: (B,T,D), mixed from vanilla (vt) and DNC (dt)\n",
    "      dnc_state: updated state dict from DNC path\n",
    "      g: (B,T,D) or (B,T,1) gate values in [0,1]\n",
    "    \"\"\"\n",
    "    # 1) Vanilla transformer path\n",
    "    T = x.size(1)\n",
    "    mask = causal_mask(T, device=x.device)  # (T,T) causal attn mask\n",
    "    vt = self.vanilla(x, attn_mask=mask)    # (B,T,D)\n",
    "\n",
    "    # 2) DNC path (controller+memory)\n",
    "    dt, dnc_state = self.dncblock(x, state=dnc_state)  # dt: (B,T,D)\n",
    "\n",
    "    # 3) Gate computation (optionally LN before gate)\n",
    "    import torch as _t\n",
    "    z = _t.cat([vt, dt], dim=-1)  # (B,T,2D)\n",
    "    h = self.pre_gate_ln(z) if hasattr(self, \"pre_gate_ln\") and self.pre_gate_ln is not None else z\n",
    "    g_pre = self.gate(h)                 # typically (B,T,D) or (B,T,1)\n",
    "    tau = float(getattr(CFG, 'gate_temp', 1.0))\n",
    "    g = torch.sigmoid(g_pre / max(tau, 1e-6))\n",
    "\n",
    "    # Optional ablation override: force g to 0.0 (vanilla) or 1.0 (DNC)\n",
    "    if gate_override is not None:\n",
    "        g = _t.full_like(g, float(gate_override))\n",
    "\n",
    "    # 4) Blend paths\n",
    "    out = g * dt + (1.0 - g) * vt        # (B,T,D)\n",
    "\n",
    "    # 5) Lightweight metrics (only if requested)\n",
    "    self.last_metrics = None\n",
    "    if getattr(self, \"collect_metrics\", False):\n",
    "        try:\n",
    "            eps = 1e-6\n",
    "            gc = g.clamp(min=eps, max=1.0 - eps)\n",
    "            g_entropy = (-(gc * gc.log() + (1 - gc) * (1 - gc).log())).mean()\n",
    "            vt_norm = vt.norm(dim=-1).mean()\n",
    "            dt_norm = dt.norm(dim=-1).mean()\n",
    "            mstats = {}\n",
    "            # Pull aggregated memory stats if present\n",
    "            if isinstance(dnc_state, dict) and isinstance(dnc_state.get(\"stats\", None), dict):\n",
    "                for k in (\"u_mean\", \"rw_max_mean\", \"ww_max_mean\", \"M_norm_mean\"):\n",
    "                    if k in dnc_state[\"stats\"]:\n",
    "                        mstats[k] = dnc_state[\"stats\"][k]\n",
    "            self.last_metrics = {\n",
    "                \"g_mean\": g.mean().detach(),\n",
    "                \"g_entropy\": g_entropy.detach(),\n",
    "                \"vt_norm\": vt_norm.detach(),\n",
    "                \"dt_norm\": dt_norm.detach(),\n",
    "                **mstats,\n",
    "            }\n",
    "        except Exception:\n",
    "            self.last_metrics = None\n",
    "\n",
    "    return out, dnc_state, g\n",
    "\n",
    "# Apply the patch\n",
    "ParallelEnrichmentBlock.forward = _peb_forward\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:15.927680500Z",
     "start_time": "2025-08-18T01:14:15.915681800Z"
    }
   },
   "id": "3eabc7991c7e743d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# --- Evaluator unit tests (no heavy model) ---\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:17.954682900Z",
     "start_time": "2025-08-18T01:14:17.929682100Z"
    }
   },
   "id": "db24ba31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Unit-like tests (sanity)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3036ac"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\n",
    "def run_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:20.450680800Z",
     "start_time": "2025-08-18T01:14:20.436684600Z"
    }
   },
   "id": "2cb70f70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unit tests/basic sanity checks\n",
    "currently all passing, but self-reminder here to comment out if architecture changes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f24f2641cb05940"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# --- Smoke tests ---\n",
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:22.167681Z",
     "start_time": "2025-08-18T01:14:22.137682800Z"
    }
   },
   "id": "d842ad22"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[ 0.2946,  1.3153,  0.4913,  ..., -0.5015,  0.0606,  1.3821],\n",
      "         [ 1.0736,  0.3288, -0.0846,  ..., -0.3198, -1.2993, -0.6523],\n",
      "         [ 0.9915,  0.1268, -1.9989,  ..., -0.1103, -1.3248, -1.1391],\n",
      "         [-0.0569,  0.3488,  1.7448,  ...,  0.0448,  1.1685, -0.8049]],\n",
      "\n",
      "        [[ 0.4288, -0.4819,  0.4153,  ...,  1.4628,  0.6916,  0.0974],\n",
      "         [ 0.4180, -1.5419,  0.4681,  ...,  0.1858,  1.7110, -0.3419],\n",
      "         [ 0.3899, -0.2150, -0.0096,  ..., -1.7715,  0.0029,  0.5766],\n",
      "         [-0.6167,  0.2484,  0.8239,  ...,  0.8809,  0.4929, -0.6685]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "s2: {'M': tensor([[[ 2.2475e-03,  9.5308e-02,  1.3153e-01,  ...,  9.7984e-02,\n",
      "           4.0287e-01, -3.2455e-02],\n",
      "         [-1.7167e-03,  1.3582e-05,  2.2045e-03,  ..., -8.8032e-04,\n",
      "           1.8157e-03, -1.3368e-03],\n",
      "         [-1.7168e-03,  1.3498e-05,  2.2043e-03,  ..., -8.8044e-04,\n",
      "           1.8153e-03, -1.3367e-03],\n",
      "         ...,\n",
      "         [-1.7168e-03,  1.3498e-05,  2.2043e-03,  ..., -8.8044e-04,\n",
      "           1.8153e-03, -1.3367e-03],\n",
      "         [-1.7168e-03,  1.3498e-05,  2.2043e-03,  ..., -8.8044e-04,\n",
      "           1.8153e-03, -1.3367e-03],\n",
      "         [-1.7168e-03,  1.3498e-05,  2.2043e-03,  ..., -8.8044e-04,\n",
      "           1.8153e-03, -1.3367e-03]],\n",
      "\n",
      "        [[-1.2394e-01,  1.6811e-02,  1.4920e-01,  ..., -1.0521e-03,\n",
      "           5.7473e-01, -2.8524e-01],\n",
      "         [-1.2177e-03, -6.8403e-04,  1.0919e-04,  ...,  4.9189e-04,\n",
      "           1.8086e-03, -1.0281e-03],\n",
      "         [-1.2176e-03, -6.8398e-04,  1.0900e-04,  ...,  4.9186e-04,\n",
      "           1.8079e-03, -1.0278e-03],\n",
      "         ...,\n",
      "         [-1.2176e-03, -6.8398e-04,  1.0900e-04,  ...,  4.9186e-04,\n",
      "           1.8079e-03, -1.0278e-03],\n",
      "         [-1.2176e-03, -6.8398e-04,  1.0900e-04,  ...,  4.9186e-04,\n",
      "           1.8079e-03, -1.0278e-03],\n",
      "         [-1.2176e-03, -6.8398e-04,  1.0900e-04,  ...,  4.9186e-04,\n",
      "           1.8079e-03, -1.0278e-03]]], device='cuda:0', grad_fn=<AddBackward0>), 'u': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>), 'L': tensor([[[0.0000e+00, 8.2069e-04, 8.2042e-04,  ..., 8.2042e-04,\n",
      "          8.2042e-04, 8.2042e-04],\n",
      "         [1.3603e-03, 0.0000e+00, 5.8459e-06,  ..., 5.8459e-06,\n",
      "          5.8459e-06, 5.8459e-06],\n",
      "         [1.3600e-03, 5.8465e-06, 0.0000e+00,  ..., 5.8447e-06,\n",
      "          5.8447e-06, 5.8447e-06],\n",
      "         ...,\n",
      "         [1.3600e-03, 5.8465e-06, 5.8447e-06,  ..., 0.0000e+00,\n",
      "          5.8447e-06, 5.8447e-06],\n",
      "         [1.3600e-03, 5.8465e-06, 5.8447e-06,  ..., 5.8447e-06,\n",
      "          0.0000e+00, 5.8447e-06],\n",
      "         [1.3600e-03, 5.8465e-06, 5.8447e-06,  ..., 5.8447e-06,\n",
      "          5.8447e-06, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 7.0006e-04, 6.9977e-04,  ..., 6.9977e-04,\n",
      "          6.9977e-04, 6.9977e-04],\n",
      "         [1.1054e-03, 0.0000e+00, 3.2981e-06,  ..., 3.2981e-06,\n",
      "          3.2981e-06, 3.2981e-06],\n",
      "         [1.1051e-03, 3.2984e-06, 0.0000e+00,  ..., 3.2971e-06,\n",
      "          3.2971e-06, 3.2971e-06],\n",
      "         ...,\n",
      "         [1.1051e-03, 3.2984e-06, 3.2971e-06,  ..., 0.0000e+00,\n",
      "          3.2971e-06, 3.2971e-06],\n",
      "         [1.1051e-03, 3.2984e-06, 3.2971e-06,  ..., 3.2971e-06,\n",
      "          0.0000e+00, 3.2971e-06],\n",
      "         [1.1051e-03, 3.2984e-06, 3.2971e-06,  ..., 3.2971e-06,\n",
      "          3.2971e-06, 0.0000e+00]]], device='cuda:0', grad_fn=<MulBackward0>), 'p': tensor([[0.5619, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "         0.0016, 0.0016, 0.0016, 0.0016],\n",
      "        [0.6194, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "         0.0014, 0.0014, 0.0014, 0.0014]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'rw': tensor([[[0.0025, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         [0.0011, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009]],\n",
      "\n",
      "        [[0.0021, 0.0019, 0.0019,  ..., 0.0019, 0.0019, 0.0019],\n",
      "         [0.0009, 0.0007, 0.0007,  ..., 0.0007, 0.0007, 0.0007]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'r': tensor([[[-1.0183e-03,  2.4264e-04,  1.6384e-03,  1.3775e-03, -5.9612e-05,\n",
      "          -8.6413e-04, -4.0018e-04,  3.1763e-03,  8.9244e-04, -3.3444e-03,\n",
      "           2.1051e-03,  1.9476e-03, -1.6531e-04,  1.5845e-03, -1.7379e-03,\n",
      "          -9.6461e-05, -1.1310e-03,  6.3701e-04,  3.1170e-03, -2.0158e-03,\n",
      "           6.9279e-04,  2.5878e-03,  9.5572e-04,  7.5202e-06, -1.4728e-03,\n",
      "           7.5198e-04,  6.9173e-04, -1.0651e-03, -1.0931e-03, -9.9794e-04,\n",
      "           6.2111e-04,  1.3391e-04,  2.0659e-03,  1.2539e-03,  2.2905e-03,\n",
      "          -3.4339e-04,  1.3354e-03, -1.5085e-03,  1.0545e-03, -1.6664e-03,\n",
      "          -3.4603e-04,  2.0586e-04, -2.1096e-04, -4.2385e-04, -1.4364e-05,\n",
      "          -2.7357e-03,  2.4568e-03,  3.0139e-03, -1.2938e-04, -1.0617e-04,\n",
      "          -9.8072e-04, -2.2004e-03, -3.9422e-04,  6.9465e-04, -1.1967e-05,\n",
      "          -7.6918e-04,  2.0373e-03,  2.2152e-03,  1.4046e-04,  8.9290e-04,\n",
      "          -6.3146e-04, -2.8392e-04,  2.0742e-03, -8.7706e-04],\n",
      "         [-4.0876e-04,  1.0746e-04,  6.7185e-04,  5.8188e-04, -5.0653e-05,\n",
      "          -3.7250e-04, -1.4702e-04,  1.3477e-03,  3.8584e-04, -1.4117e-03,\n",
      "           8.8822e-04,  8.3188e-04, -8.1435e-05,  6.6457e-04, -7.2329e-04,\n",
      "          -3.8959e-05, -4.5511e-04,  2.7055e-04,  1.3430e-03, -8.3018e-04,\n",
      "           2.8749e-04,  1.0563e-03,  3.9339e-04,  2.1081e-05, -6.0794e-04,\n",
      "           2.6868e-04,  3.1135e-04, -4.5688e-04, -4.7018e-04, -4.2188e-04,\n",
      "           2.5491e-04,  4.5911e-05,  8.6090e-04,  5.0491e-04,  9.4708e-04,\n",
      "          -1.4010e-04,  5.5206e-04, -6.2222e-04,  4.4718e-04, -6.9488e-04,\n",
      "          -1.3649e-04,  7.9386e-05, -1.2953e-04, -1.5579e-04,  8.5541e-06,\n",
      "          -1.1341e-03,  1.0402e-03,  1.2615e-03, -4.4355e-05, -2.9901e-05,\n",
      "          -3.9507e-04, -9.1330e-04, -1.8570e-04,  3.1137e-04, -1.9800e-05,\n",
      "          -3.1330e-04,  8.3737e-04,  9.3714e-04,  6.9793e-05,  3.7474e-04,\n",
      "          -2.6498e-04, -1.0374e-04,  8.7538e-04, -3.5567e-04]],\n",
      "\n",
      "        [[-8.5427e-04, -2.9915e-04,  3.6527e-04,  5.1107e-04, -3.9375e-04,\n",
      "          -1.4397e-03,  9.0814e-04,  5.8730e-04,  4.4361e-04,  1.1705e-03,\n",
      "          -1.2905e-03,  2.4266e-03,  3.3420e-03, -7.6643e-04, -1.1887e-03,\n",
      "          -1.3882e-03,  3.1349e-04, -2.8928e-04,  1.4394e-04,  2.1745e-04,\n",
      "          -1.8282e-03, -1.6734e-03, -1.7575e-03, -2.0839e-04, -2.4405e-03,\n",
      "          -4.4778e-04,  8.6497e-04,  2.3407e-03, -2.1735e-04, -1.2345e-04,\n",
      "          -6.7424e-04, -2.1069e-03, -5.4736e-04,  1.3544e-04, -6.3445e-04,\n",
      "          -2.1510e-03,  9.3934e-05, -9.3643e-04,  1.1724e-03,  4.7140e-04,\n",
      "          -7.3562e-04, -2.5545e-04, -4.7845e-04,  1.4929e-03,  9.7129e-04,\n",
      "           1.8205e-03,  7.8095e-04,  9.8116e-04,  1.8436e-04, -1.4599e-03,\n",
      "           2.1187e-03, -4.8026e-04, -6.4179e-04, -5.4532e-04,  1.5313e-04,\n",
      "          -2.1138e-03, -3.6832e-03, -4.3894e-04,  3.2310e-03,  1.8886e-03,\n",
      "          -1.4467e-04,  2.3820e-04,  2.0854e-03, -1.0988e-03],\n",
      "         [-3.4075e-04, -1.1203e-04,  1.5734e-04,  2.2441e-04, -1.7027e-04,\n",
      "          -5.8179e-04,  3.7810e-04,  2.3990e-04,  1.7545e-04,  4.8276e-04,\n",
      "          -5.4178e-04,  9.9575e-04,  1.3879e-03, -3.1636e-04, -5.1095e-04,\n",
      "          -5.8388e-04,  1.2785e-04, -1.1531e-04,  7.8637e-05,  9.8348e-05,\n",
      "          -7.7024e-04, -6.9541e-04, -7.4301e-04, -6.8001e-05, -1.0053e-03,\n",
      "          -1.9392e-04,  3.5891e-04,  9.6770e-04, -8.0001e-05, -5.7609e-05,\n",
      "          -2.8799e-04, -9.0030e-04, -2.0106e-04,  3.9289e-05, -2.6596e-04,\n",
      "          -8.8322e-04,  4.7771e-05, -3.8426e-04,  4.7296e-04,  2.1415e-04,\n",
      "          -3.1278e-04, -9.4093e-05, -1.8532e-04,  6.0756e-04,  4.0186e-04,\n",
      "           7.6044e-04,  3.3294e-04,  4.1782e-04,  9.1323e-05, -6.1791e-04,\n",
      "           8.7553e-04, -1.8939e-04, -2.8643e-04, -2.4955e-04,  7.8187e-05,\n",
      "          -8.4715e-04, -1.5284e-03, -1.6932e-04,  1.3378e-03,  7.8575e-04,\n",
      "          -5.1879e-05,  9.0701e-05,  8.6479e-04, -4.5353e-04]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), 'stats': {'u_mean': tensor(0., device='cuda:0'), 'M_norm_mean': tensor(0.0217, device='cuda:0'), 'rw_max_mean': tensor(0.0016, device='cuda:0'), 'ww_max_mean': tensor(0.4010, device='cuda:0')}}\n",
      "g: tensor([[[0.1762, 0.1667, 0.1875,  ..., 0.2014, 0.4251, 0.2451],\n",
      "         [0.2495, 0.3125, 0.2340,  ..., 0.2956, 0.3305, 0.3475],\n",
      "         [0.2110, 0.3299, 0.3304,  ..., 0.4642, 0.2403, 0.3586],\n",
      "         [0.2087, 0.2483, 0.3239,  ..., 0.2611, 0.2001, 0.3399]],\n",
      "\n",
      "        [[0.2185, 0.2840, 0.1423,  ..., 0.1886, 0.3506, 0.1068],\n",
      "         [0.2633, 0.3685, 0.4763,  ..., 0.4083, 0.2470, 0.3248],\n",
      "         [0.2655, 0.5042, 0.3939,  ..., 0.3596, 0.4095, 0.4416],\n",
      "         [0.3969, 0.3408, 0.3334,  ..., 0.1180, 0.1577, 0.2725]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "All unit-like tests passed.\n"
     ]
    }
   ],
   "source": [
    "#run_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:25.086250100Z",
     "start_time": "2025-08-18T01:14:24.855744600Z"
    }
   },
   "id": "7aac46d77e635252"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy acc: 0.0 gates: [0.49580350518226624, 0.50152587890625]\n",
      "reverse acc: 0.0 gates: [0.49599286913871765, 0.5039244890213013]\n",
      "needle acc: 0.0 gates: [0.489719033241272, 0.4984525442123413]\n",
      "Evaluator unit tests passed.\n"
     ]
    }
   ],
   "source": [
    "#run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:26.406587900Z",
     "start_time": "2025-08-18T01:14:26.400588300Z"
    }
   },
   "id": "480f5d04e5aaed2a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:29.981168500Z",
     "start_time": "2025-08-18T01:14:29.967168700Z"
    }
   },
   "id": "38cc88e726027b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training run Sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867e3f96209f238d"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# small training test\n",
    "#head, tok = train_small()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:31.481167800Z",
     "start_time": "2025-08-18T01:14:31.442169300Z"
    }
   },
   "id": "b72c8e3e4a7e3237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medium size training sweeps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d5516ca50b4309"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a9cc2ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:14:32.459168700Z",
     "start_time": "2025-08-18T01:14:32.442168500Z"
    }
   },
   "outputs": [],
   "source": [
    "# single medium training test/sanity check\n",
    "# free_head_and_cache()\n",
    "# cuda_report(\"snapshot: before train_medium\")\n",
    "# head, tok = train_medium(steps=100, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "# cuda_report(\"snapshot: after train_medium\")\n",
    "# Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium training sweep, testing parameter / dataset variants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81cbb7a830e97c75"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E0_baseline ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=0.01 GB | reserved=0.02 GB | free=24.36 GB | total=25.77 GB\n",
      "[before E0_baseline] alloc=0.01 GB | reserved=0.02 GB | free=24.36 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61403d06376b4a1ebe585e69863641a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 10.5249 | lr 2.00e-05 | gates=[0.259765625, 0.2890625] | mix=hf\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E0_baseline] alloc=9.79 GB | reserved=26.01 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.90 GB | free=14.47 GB | total=25.77 GB\n",
      "\n",
      "=== E1_memory_lean ===\n",
      "mixture=(0.4, 0.3, 0.2, 0.1), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E1_memory_lean] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd278d8b78f543108041bf50929d4839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 10.6371 | lr 2.00e-05 | gates=[0.2734375, 0.294921875] | mix=hf\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E1_memory_lean] alloc=9.79 GB | reserved=22.99 GB | free=0.21 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "\n",
      "=== E2_gate_reg_low ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0002, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E2_gate_reg_low] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e578abdbd5334da7838fafc615ca107f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 19.1560 | lr 2.00e-05 | gates=[0.306640625, 0.31640625] | mix=copy\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E2_gate_reg_low] alloc=9.79 GB | reserved=23.13 GB | free=0.08 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.83 GB | free=14.54 GB | total=25.77 GB\n",
      "\n",
      "=== E3_gate_reg_high ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0005, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E3_gate_reg_high] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16753d288b304b95ba46e1d682abb9fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 62.0292 | lr 2.00e-05 | gates=[0.3125, 0.30859375] | mix=nback\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E3_gate_reg_high] alloc=9.79 GB | reserved=27.25 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.83 GB | free=14.54 GB | total=25.77 GB\n",
      "\n",
      "=== E4_gate_temp_0p7 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=0.7, force_g=None\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E4_gate_temp_0p7] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76f2aa96dd064a9aa3f79b03e46c3f3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 8.8115 | lr 2.00e-05 | gates=[0.1806640625, 0.220703125] | mix=hf\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E4_gate_temp_0p7] alloc=9.79 GB | reserved=22.67 GB | free=0.54 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.83 GB | free=14.54 GB | total=25.77 GB\n",
      "\n",
      "=== E5a_force_g_0 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=0.0\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E5a_force_g_0] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "608637562b954f0e9be502d88bbfed5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 26.0708 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E5a_force_g_0] alloc=9.79 GB | reserved=22.38 GB | free=0.82 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.83 GB | free=14.54 GB | total=25.77 GB\n",
      "\n",
      "=== E5b_force_g_1 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=1.0\n",
      "[after free] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n",
      "[before E5b_force_g_1] alloc=0.02 GB | reserved=0.12 GB | free=24.24 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bba82d1689f466d9afc43f8107c6a83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 19.9843 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "Exception ''gates detached'' encountered while reporting pre-block gate means and g>0.5 token fracions \n",
      " This is a non-breaking exception\n",
      "[after  E5b_force_g_1] alloc=9.79 GB | reserved=24.63 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.79 GB | reserved=9.83 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compact experiment driver: E0..E5 ---\n",
    "import time, torch, gc\n",
    "\n",
    "# === user-tunable defaults for quick smoke; bump steps for deeper runs ===\n",
    "EXP_STEPS   = 10       # try 1000–5000 for longer curves\n",
    "EXP_WARMUP  = 10       # keep ~steps/20\n",
    "BASE_MIX    = (0.4, 0.2, 0.2, 0.2)  # (hf, copy, repeat, nback)\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_isolate_test(label,\n",
    "            mixture_weights=BASE_MIX,\n",
    "            gate_reg_lambda=None,\n",
    "            gate_temp=None,\n",
    "            force_g=None,\n",
    "            steps=EXP_STEPS,\n",
    "            warmup=EXP_WARMUP):\n",
    "    \"\"\"Run a single train_medium experiment with explicit knobs.\"\"\"\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    # set experiment-specific knobs (others come from CFG defaults)\n",
    "    if gate_reg_lambda is not None: set_cfg(gate_reg_lambda=float(gate_reg_lambda))\n",
    "    if gate_temp is not None:       set_cfg(gate_temp=float(gate_temp))\n",
    "    set_cfg(force_g=force_g)  # may be None, 0.0, or 1.0\n",
    "\n",
    "    print(f\"mixture={mixture_weights}, gate_reg_lambda={getattr(CFG,'gate_reg_lambda', 0.0)}, \"\n",
    "          f\"gate_temp={getattr(CFG,'gate_temp', 1.0)}, force_g={getattr(CFG,'force_g', None)}\")\n",
    "\n",
    "    # clean slate for VRAM and allocator fragmentation between runs\n",
    "    free_head_and_cache()\n",
    "    cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # run\n",
    "    head, tok = train_medium(\n",
    "        steps=steps,\n",
    "        warmup_steps=warmup,\n",
    "        mixture_weights=mixture_weights,\n",
    "        viz_memory_after=False,   # keep quick; use visualize_memory_tb() ad-hoc\n",
    "    )\n",
    "\n",
    "    # post-run snapshot + cleanup\n",
    "    cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# === E0 baseline: identical to your sanity run ===\n",
    "run_isolate_test(\"E0_baseline\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E1 memory-leaning mix: more algorithmic exposure ===\n",
    "run_isolate_test(\"E1_memory_lean\",\n",
    "        mixture_weights=(0.4, 0.3, 0.2, 0.1),  # HF, copy, repeat, nback\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E2 gate regularizer (low) ===\n",
    "run_isolate_test(\"E2_gate_reg_low\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=2e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E3 gate regularizer (high) ===\n",
    "run_isolate_test(\"E3_gate_reg_high\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=5e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E4 sharper routing via lower gate temperature ===\n",
    "run_isolate_test(\"E4_gate_temp_0p7\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=0.7,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E5 ablations: disable/force DNC path ===\n",
    "run_isolate_test(\"E5a_force_g_0\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=0.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "run_isolate_test(\"E5b_force_g_1\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=1.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:19:16.124113300Z",
     "start_time": "2025-08-18T01:14:52.339646100Z"
    }
   },
   "id": "771c919035840bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=10.22 GB | reserved=10.33 GB | free=14.03 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "gc.collect(); torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T20:56:42.549228900Z",
     "start_time": "2025-08-17T20:56:41.509231500Z"
    }
   },
   "id": "f07fc074b58caaa0"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 12. Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask. A step-wise cached mode can be added for streaming scenarios.\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks. You can plug in larger corpora or evaluation suites later.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984435d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-17T19:09:55.631811500Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "\n",
    "print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "_run(\"conda env export --from-history > environment.yml\")\n",
    "_run(\"conda env export > environment.lock.yml\")\n",
    "_run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
