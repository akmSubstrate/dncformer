{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "This notebook implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c8c8802779fe7dd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.900129100Z",
     "start_time": "2025-08-12T07:20:31.797130300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "SDPA_CTX = contextlib.nullcontext()  # no-op by default\n",
    "\n",
    "try:\n",
    "    # select math + mem-efficient SDPA, disable flash\n",
    "    from torch.backends.cuda import sdp_kernel\n",
    "    SDPA_CTX = sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "except Exception as e:\n",
    "    print(\"SDPA context not set (falling back to default):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.947129400Z",
     "start_time": "2025-08-12T07:20:31.811129500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n",
    "\n",
    "# SDPA configured via SDPA_CTX() above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7891e",
   "metadata": {},
   "source": [
    "## 1. Config & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.964128500Z",
     "start_time": "2025-08-12T07:20:31.827131700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n",
      "AMP dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "import os, math, random, torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "print(\"AMP dtype:\", amp_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.965129Z",
     "start_time": "2025-08-12T07:20:31.842130800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 3. DNC Memory (compact reference implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.984130900Z",
     "start_time": "2025-08-12T07:20:31.858129500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "## 4. Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:31.995129300Z",
     "start_time": "2025-08-12T07:20:31.873130900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "## 5. DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.007130Z",
     "start_time": "2025-08-12T07:20:31.895129800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "## 6. Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.007130Z",
     "start_time": "2025-08-12T07:20:31.904129200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        return out, dnc_state, g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "## 7. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:25:53.682889700Z",
     "start_time": "2025-08-12T07:25:53.669889700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def spda_ctx():\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception as e:\n",
    "        return f\"error {e} encountered ->\\n{contextlib.nullcontext()}\"\n",
    "\n",
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with spda_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 8. Data: Synthetic tasks + simple instruction-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.008130200Z",
     "start_time": "2025-08-12T07:20:31.937129100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0322be9",
   "metadata": {},
   "source": [
    "## 9. Training loop (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6be4ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.009130400Z",
     "start_time": "2025-08-12T07:20:31.951130200Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Returns a LambdaLR that:\n",
    "      - warms up linearly from 0 -> 1 over warmup_steps\n",
    "      - then cosine decays from 1 -> min_lr_ratio over the remaining steps\n",
    "    Uses step_idx+1 so the first call after the first optimizer.step() is nonzero.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1  # <-- important to avoid zero lr on first step\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- TensorBoard Logger (no matplotlib) ---\n",
    "import os, time\n",
    "from typing import List, Optional\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        if not self.enabled:\n",
    "            self.writer = None\n",
    "            return\n",
    "        if logdir is None:\n",
    "            logdir = \"./runs\"\n",
    "        if run_name is None:\n",
    "            run_name = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not self.enabled: return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def log_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        \"\"\"\n",
    "        img_hw: float tensor [H,W] on CPU; will be normalized to [0,1] and logged as [1,H,W]\n",
    "        \"\"\"\n",
    "        if not self.enabled: return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\":\n",
    "            x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            if (M - m) > 1e-8:\n",
    "                x = (x - m) / (M - m)\n",
    "            else:\n",
    "                x = torch.zeros_like(x)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer:\n",
    "            self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer:\n",
    "            self.writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44d73c717b8b58c"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a70a7bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.058299800Z",
     "start_time": "2025-08-12T07:20:31.966129600Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(head, tok):\n",
    "    head.eval()\n",
    "    prompt = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(enc.input_ids)\n",
    "    print(\"Gates (mean):\", [g.mean().item() for g in gates])\n",
    "    return gates\n",
    "\n",
    "def train_small():\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, CFG.warmup_steps, CFG.train_steps)\n",
    "    head.train()\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    for step in range(1, CFG.train_steps+1):\n",
    "        in_ids, lab_ids = tokenize_instruction_pairs(tok, INSTR_PAIRS, max_len=min(CFG.max_seq_len, 256))\n",
    "        in_ids = in_ids.to(device)\n",
    "        \n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            logits, gates = head(in_ids)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step()\n",
    "            \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % CFG.log_every == 0:\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {optim.param_groups[0]['lr']:.2e} | gates={[g.mean().item() for g in gates]}\")\n",
    "\n",
    "    evaluate_simple(head, tok)\n",
    "    return head, tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9ee90",
   "metadata": {},
   "source": [
    "## 10. Eval harness (needle-in-haystack, copy, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80e53881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.071300400Z",
     "start_time": "2025-08-12T07:20:31.981129100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db24ba31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.073300700Z",
     "start_time": "2025-08-12T07:20:31.996129900Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Evaluator unit tests (no heavy model) ---\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3036ac",
   "metadata": {},
   "source": [
    "## 11. Unit-like tests (sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cb70f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.089300Z",
     "start_time": "2025-08-12T07:20:32.013129700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24f2641cb05940",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Unit tests/basic sanity checks\n",
    "currently all passing, but self-reminder here to comment out if architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aac46d77e635252",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.089300Z",
     "start_time": "2025-08-12T07:20:32.028131900Z"
    }
   },
   "outputs": [],
   "source": [
    "# run_unit_tests()ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy acc: 0.0 gates: [0.495686799287796, 0.508457362651825]\n",
      "reverse acc: 0.0 gates: [0.5013509392738342, 0.5040897130966187]\n",
      "needle acc: 0.0 gates: [0.49810758233070374, 0.4982784688472748]\n",
      "Evaluator unit tests passed.\n"
     ]
    }
   ],
   "source": [
    "run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.090300500Z",
     "start_time": "2025-08-12T07:20:32.043300800Z"
    }
   },
   "id": "480f5d04e5aaed2a"
  },
  {
   "cell_type": "markdown",
   "id": "867e3f96209f238d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training run Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c8e3e4a7e3237",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-12T07:26:01.815711800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1be565ccb8a14517b484d752eb3d7153"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in head: 4353384090\n",
      "step 10 | loss 4.0979 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 20 | loss 4.0489 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 30 | loss 4.0508 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 40 | loss 3.9879 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 50 | loss 4.0820 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 60 | loss 4.0673 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 70 | loss 4.0587 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 80 | loss 3.9646 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 90 | loss 4.0439 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 100 | loss 4.0179 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 110 | loss 4.0453 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 120 | loss 4.1005 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n",
      "step 130 | loss 4.1182 | lr 0.00e+00 | gates=[0.283203125, 0.283203125]\n",
      "step 140 | loss 3.9805 | lr 0.00e+00 | gates=[0.283203125, 0.28515625]\n"
     ]
    }
   ],
   "source": [
    "head, tok = train_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 12. Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask. A step-wise cached mode can be added for streaming scenarios.\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks. You can plug in larger corpora or evaluation suites later.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
