{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "Implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, config, and environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f6d41fd5a7206b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.1 Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5acb18ebaa7c254"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:55.512274900Z",
     "start_time": "2025-08-24T04:58:55.476275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "import os, math, random, time, numpy as np\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import contextlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from typing import List, Optional\n",
    "import torch, gc\n",
    "\n",
    "\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.2 Configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8fc3ee8a8a120f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba66b7664168088"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:56.148279400Z",
     "start_time": "2025-08-24T04:58:56.097272100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1              # attention layer dropout fraction\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2                # number of DNC read heads\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       # learning rate\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "    seed: int = 42\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "def cfg_to_json(cfg=None) -> str:\n",
    "    cfg = cfg or CFG\n",
    "    d = {}\n",
    "    for k,v in getattr(cfg, \"__dict__\", {}).items():\n",
    "        if not k.startswith(\"_\"):\n",
    "            try:\n",
    "                json.dumps(v); d[k]=v\n",
    "            except TypeError:\n",
    "                d[k]=str(v)\n",
    "    return json.dumps(d, indent=2)\n",
    "\n",
    "def echo_cfg(to_console: bool = True, to_tb: bool = True, tag: str = \"cfg/json\"):\n",
    "    s = cfg_to_json(CFG)\n",
    "    if to_console:\n",
    "        print(\"=== CFG ===\")\n",
    "        print(s)\n",
    "    if to_tb and 'tb' in globals() and getattr(tb, \"writer\", None):\n",
    "        with contextlib.suppress(Exception):\n",
    "            tb.writer.add_text(tag, s, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.3 Config patch toggles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "926f2d40052c24aa"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class _Tmp: pass\n",
    "    CFG = _Tmp()\n",
    "# safe defaults if not already present\n",
    "if not hasattr(CFG, 'batch_size'): CFG.batch_size = 8\n",
    "if not hasattr(CFG, 'gate_reg_lambda'): CFG.gate_reg_lambda = 0.0   # only applied on memory-tagged batches\n",
    "if not hasattr(CFG, 'hist_every'): CFG.hist_every = 200             # histogram cadence\n",
    "if not hasattr(CFG, 'force_g'): CFG.force_g = None                  # None, or 0.0 or 1.0\n",
    "if not hasattr(CFG, 'gate_temp'): CFG.gate_temp = 1.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:56.533266Z",
     "start_time": "2025-08-24T04:58:56.507267200Z"
    }
   },
   "id": "682d4e57"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CFG] E10–E14 flags added with safe defaults.\n"
     ]
    }
   ],
   "source": [
    "# CFG extensions for E10–E14\n",
    "def extend_cfg_defaults():\n",
    "    # E10: multi-memory experts (shared controller)\n",
    "    setattr(CFG, \"mem_experts\", getattr(CFG, \"mem_experts\", 1))                   # 1 -> current behavior\n",
    "    setattr(CFG, \"expert_gate_temp\", getattr(CFG, \"expert_gate_temp\", 1.0))\n",
    "    setattr(CFG, \"expert_diversity_lambda\", getattr(CFG, \"expert_diversity_lambda\", 0.0))\n",
    "    setattr(CFG, \"expert_W\", getattr(CFG, \"expert_W\", getattr(CFG, \"W\", 64)))\n",
    "    setattr(CFG, \"expert_R\", getattr(CFG, \"expert_R\", getattr(CFG, \"R\", 1)))\n",
    "    setattr(CFG, \"expert_N\", getattr(CFG, \"expert_N\", [getattr(CFG,\"N\",64), getattr(CFG,\"N\",64)]))\n",
    "\n",
    "    # E11: per-block memory configs (list of dicts or None -> use global N/W/R)\n",
    "    setattr(CFG, \"per_block_cfg\", getattr(CFG, \"per_block_cfg\", None))\n",
    "    # Optional: per-block free gate bias (+ retains less, - retains more)\n",
    "    setattr(CFG, \"per_block_free_bias\", getattr(CFG, \"per_block_free_bias\", None))  # e.g., [ +0.3, -0.2 ]\n",
    "\n",
    "    # E12: read-to-attention fusion (light)\n",
    "    setattr(CFG, \"fusion_enable\", getattr(CFG, \"fusion_enable\", False))\n",
    "    setattr(CFG, \"fusion_hidden_mult\", getattr(CFG, \"fusion_hidden_mult\", 2.0))\n",
    "    setattr(CFG, \"fusion_drop\", getattr(CFG, \"fusion_drop\", 0.0))\n",
    "    setattr(CFG, \"fusion_bias_queries\", getattr(CFG, \"fusion_bias_queries\", False))  # keep False initially\n",
    "\n",
    "    # E13: write sparsity / overlap regs (overlap wired later when keys are exposed)\n",
    "    setattr(CFG, \"write_reg_lambda\", getattr(CFG, \"write_reg_lambda\", 0.0))\n",
    "    setattr(CFG, \"key_overlap_lambda\", getattr(CFG, \"key_overlap_lambda\", 0.0))\n",
    "    setattr(CFG, \"key_overlap_window\", getattr(CFG, \"key_overlap_window\", 1))\n",
    "    setattr(CFG, \"reg_only_on_memory_batches\", getattr(CFG, \"reg_only_on_memory_batches\", True))\n",
    "\n",
    "extend_cfg_defaults()\n",
    "print(\"[CFG] E10–E14 flags added with safe defaults.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:56.774100300Z",
     "start_time": "2025-08-24T04:58:56.729099900Z"
    }
   },
   "id": "625cbaed7f0e9864"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.4 SDPA selection\n",
    "- prefer PyTorch SDPA\n",
    "- avoid flash-attn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc51d39d0304bef"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:57.174465500Z",
     "start_time": "2025-08-24T04:58:57.115066100Z"
    }
   },
   "outputs": [],
   "source": [
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.5 Determinism and seeds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f23192fc1053686"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import os, random, numpy as _np, torch, contextlib\n",
    "\n",
    "def set_determinism(seed: int = 42, deterministic: bool = True, cudnn_benchmark: bool = False):\n",
    "    \"\"\"\n",
    "    Set seeds across Python/NumPy/Torch and optionally toggle deterministic algorithms.\n",
    "    deterministic=True may slow kernels; Warn_only to avoid hard errors\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    _np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cuDNN settings\n",
    "    torch.backends.cudnn.benchmark = bool(cudnn_benchmark)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # Help cublas determinism for some kernels; warn only failure mode\n",
    "        os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "        with contextlib.suppress(Exception):\n",
    "            torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Convenience alias.\"\"\"\n",
    "    set_determinism(seed=seed, deterministic=False, cudnn_benchmark=False)\n",
    "\n",
    "# set CFG seed\n",
    "with contextlib.suppress(Exception):\n",
    "    if hasattr(CFG, \"seed\"):\n",
    "        set_seed(int(CFG.seed))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:57.551321100Z",
     "start_time": "2025-08-24T04:58:57.516320900Z"
    }
   },
   "id": "4f82989f7aae0850"
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 1. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Model Information and factory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a83e8350cc6d34f"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:58.176364200Z",
     "start_time": "2025-08-24T04:58:58.132854700Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n",
    "            \n",
    "def _mean_safely(seq):\n",
    "    xs = [x for x in seq if isinstance(x, (int,float)) and not (x != x)]  # drop NaN\n",
    "    return sum(xs)/len(xs) if xs else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Gate metrics utils (mean, frac>0.5, entropy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f19b2569a5560d"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def _reduce_gate_tensor(g: torch.Tensor) -> torch.Tensor:\n",
    "    # g: (B,T,*) -> (B,T)\n",
    "    if g.dim() == 3:\n",
    "        return g.mean(dim=-1)\n",
    "    return g\n",
    "\n",
    "@torch.no_grad()\n",
    "def _gate_metrics(g: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns scalar (mean, frac>0.5, entropy)\n",
    "    \"\"\"\n",
    "    g2 = _reduce_gate_tensor(g.detach())\n",
    "    mean_val = float(g2.mean().item())\n",
    "    frac = float((g2 > 0.5).float().mean().item())\n",
    "    eps = 1e-6\n",
    "    p = g2.clamp(eps, 1 - eps)\n",
    "    # binary entropy (natural log base)\n",
    "    ent = float((-(p * (p + eps).log() + (1 - p) * (1 - p + eps).log())).mean().item())\n",
    "    return mean_val, frac, ent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:58.564407100Z",
     "start_time": "2025-08-24T04:58:58.539407Z"
    }
   },
   "id": "dad9cdfcce225bcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Memory Freeing/handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfb25b111aaa708"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:58.971816100Z",
     "start_time": "2025-08-24T04:58:58.947808200Z"
    }
   },
   "id": "228ec3411b7b0b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Environment Export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f294b5269755df"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "        \n",
    "def export_current_env():\n",
    "    print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "    _run(\"conda env export --from-history > environment.yml\")\n",
    "    _run(\"conda env export > environment.lock.yml\")\n",
    "    _run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "    print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:59.374232900Z",
     "start_time": "2025-08-24T04:58:59.347232700Z"
    }
   },
   "id": "f14fc6a99cc0d7e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 Checkpoint helpers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f221f816cf7b6d63"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import json, os, contextlib, torch\n",
    "from pathlib import Path\n",
    "\n",
    "def _cfg_to_dict(cfg) -> dict:\n",
    "    out = {}\n",
    "    for k, v in getattr(cfg, \"__dict__\", {}).items():\n",
    "        if not k.startswith(\"_\"):\n",
    "            try:\n",
    "                json.dumps(v)  # test serializability\n",
    "                out[k] = v\n",
    "            except TypeError:\n",
    "                out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def save_head(head: torch.nn.Module, out_dir: str, cfg=None, run_label: str = None):\n",
    "    \"\"\"\n",
    "    Saves only the trainable head parameters (state_dict) to out_dir/{run_label or 'head'}.pt,\n",
    "    plus a metadata JSON with config and light model info.\n",
    "    \"\"\"\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    tag = run_label or \"head\"\n",
    "    ckpt_path = Path(out_dir) / f\"{tag}.pt\"\n",
    "    meta_path = Path(out_dir) / f\"{tag}.meta.json\"\n",
    "\n",
    "    # Only head params\n",
    "    sd = head.state_dict()\n",
    "\n",
    "    # Try to capture minimal head info\n",
    "    info = {\"type\": head.__class__.__name__}\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"blocks\"] = len(getattr(head, \"blocks\", []))\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"d_model\"] = int(getattr(CFG, \"d_model\"))\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"base_model_id\"] = getattr(CFG, \"base_model_id\", None)\n",
    "\n",
    "    meta = {\"config\": _cfg_to_dict(cfg or CFG), \"head_info\": info}\n",
    "    torch.save(sd, ckpt_path)\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[save_head] wrote {ckpt_path} and {meta_path}\")\n",
    "\n",
    "def load_head(head: torch.nn.Module, in_path: str, strict: bool = False, map_location=None):\n",
    "    \"\"\"\n",
    "    Loads a head checkpoint (state_dict) into an existing head instance.\n",
    "    \"\"\"\n",
    "    in_path = str(in_path)\n",
    "    sd = torch.load(in_path, map_location=map_location or \"cpu\")\n",
    "    missing, unexpected = head.load_state_dict(sd, strict=strict)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[load_head] missing={missing} unexpected={unexpected}\")\n",
    "    else:\n",
    "        print(\"[load_head] restored successfully.\")\n",
    "    return head"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:58:59.782172500Z",
     "start_time": "2025-08-24T04:58:59.758172500Z"
    }
   },
   "id": "e31ad229b8ebdd8c"
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1: DNC Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61be1839c9ebb66f"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:00.471155600Z",
     "start_time": "2025-08-24T04:59:00.395157900Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "        self.free_bias = getattr(self, \"free_bias\", 0.0)   # default to no bias\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # apply a clamped bias if config specifies (per-block or global)\n",
    "        free_g = x_if[\"free_gates\"]\n",
    "        if getattr(self, \"free_bias\", 0.0) != 0.0:\n",
    "            free_g = (free_g + self.free_bias).clamp(0.0, 0.1)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - free_g * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        # aggregated lightweight stats (per-step)\n",
    "        try:\n",
    "            _stats = {}\n",
    "            with torch.no_grad():\n",
    "                _stats[\"u_mean\"] = state[\"u\"].mean().detach()\n",
    "                try:\n",
    "                    _stats[\"M_norm_mean\"] = state[\"M\"].norm(dim=-1).mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"rw_max_mean\"] = rw.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"ww_max_mean\"] = ww.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            state[\"stats\"] = _stats\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe(\n",
    "                {\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1), # (B,)\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Mean write gate (scalar): shape-agnostic mean\n",
    "            wg = x_if.get(\"write_gate\", None)\n",
    "            write_gate_mean = float(wg.mean().detach().item()) if wg is not None else float(\"nan\")\n",
    "            # Optional: if you have read vectors available as 'r' (B,R,W) or (B,T,W), expose their norm\n",
    "            read_vecs = locals().get(\"r\", None)  # or however your code names it\n",
    "            read_norm = float(read_vecs.norm().detach().item()/max(1, read_vecs.numel())) if isinstance(read_vecs, torch.Tensor) else float(\"nan\")\n",
    "            # Stash for upstream (PEB/Head) to collect\n",
    "            self._last_metrics = {\"write_gate_mean\": write_gate_mean, \"read_vec_norm\": read_norm}\n",
    "        except Exception:\n",
    "            self._last_metrics = {}\n",
    "        \n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "#### 2.2 Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:00.866806100Z",
     "start_time": "2025-08-24T04:59:00.828301900Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "#### 2.3 DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:01.294587900Z",
     "start_time": "2025-08-24T04:59:01.273586700Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int,\n",
    "                 heads: int, dropout: float, ffn_mult: float,\n",
    "                 free_bias: float = 0.0):   # NEW: free_bias default\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        # Propagate free-bias down to memory (Patch B expects this attr)\n",
    "        try:\n",
    "            self.mem.free_bias = float(free_bias)\n",
    "        except Exception:\n",
    "            setattr(self.mem, \"free_bias\", float(free_bias))\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "        # scratch for metrics/fusion\n",
    "        self.last_metrics = {}\n",
    "        self.last_read_feat = None  # (B,T,W) pooled read vectors (per time)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)  # r_t: (B,R,W)\n",
    "            r_list.append(r_t)\n",
    "\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "\n",
    "        # --- metrics for fusion/regs/logging ---\n",
    "        try:\n",
    "            # pooled read features per time step (B,T,W) by averaging heads\n",
    "            self.last_read_feat = Rseq.mean(dim=2).detach()  # safe feature for fusion\n",
    "            # read norm (unitless average)\n",
    "            rnorm = float(self.last_read_feat.norm().item() / max(1, self.last_read_feat.numel()))\n",
    "        except Exception:\n",
    "            self.last_read_feat = None\n",
    "            rnorm = float(\"nan\")\n",
    "\n",
    "        # write gate mean (if exposed by DNCMemory last step)\n",
    "        wg_mean = float(\"nan\")\n",
    "        lm = getattr(self.mem, \"_last_metrics\", None)\n",
    "        if isinstance(lm, dict):\n",
    "            wg_mean = float(lm.get(\"write_gate_mean\", float(\"nan\")))\n",
    "\n",
    "        self.last_metrics = {\"read_vec_norm\": rnorm, \"write_gate_mean\": wg_mean}\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "#### 2.4 Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:01.755464Z",
     "start_time": "2025-08-24T04:59:01.714465500Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None, gate_override: None = None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int,\n",
    "                 heads: int = 4, dropout: float = 0.1, ffn_mult: float = 4.0,\n",
    "                 block_index: int = 0,               # (existing)\n",
    "                 gate_bias_init: float = -1.0):       # <<< NEW DEFAULT ADDED\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.block_index = block_index\n",
    "\n",
    "        # Vanilla branch\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "\n",
    "        # E11 per-block overrides\n",
    "        N_, W_, R_ = N, W, R\n",
    "        self.gate_temp = getattr(CFG, \"gate_temp\", 1.0)\n",
    "        fbias = 0.0\n",
    "        if isinstance(getattr(CFG, \"per_block_cfg\", None), (list, tuple)):\n",
    "            if self.block_index < len(CFG.per_block_cfg) and CFG.per_block_cfg[self.block_index] is not None:\n",
    "                blk = CFG.per_block_cfg[self.block_index]\n",
    "                N_ = int(blk.get(\"N\", N_)); W_ = int(blk.get(\"W\", W_)); R_ = int(blk.get(\"R\", R_))\n",
    "                self.gate_temp = float(blk.get(\"gate_temp\", self.gate_temp))\n",
    "                fbias = float(blk.get(\"free_bias\", 0.0))\n",
    "        if getattr(CFG, \"per_block_free_bias\", None) and self.block_index < len(CFG.per_block_free_bias):\n",
    "            fbias = float(CFG.per_block_free_bias[self.block_index])\n",
    "\n",
    "        # E10 multi-experts (shared controller IO signature)\n",
    "        K = int(getattr(CFG, \"mem_experts\", 1))\n",
    "        self.mem_experts = K\n",
    "        if K == 1:\n",
    "            self.dncblocks = nn.ModuleList([DNCformerBlock(d_in=d_in, d_model=d_model, R=R_, W=W_, N=N_,\n",
    "                                                           heads=heads, dropout=dropout, ffn_mult=ffn_mult,\n",
    "                                                           free_bias=fbias)])\n",
    "        else:\n",
    "            Ns = getattr(CFG, \"expert_N\", [N_]*K)\n",
    "            self.dncblocks = nn.ModuleList([\n",
    "                DNCformerBlock(d_in=d_in, d_model=d_model, R=R_, W=getattr(CFG,\"expert_W\", W_), N=Ns[i],\n",
    "                               heads=heads, dropout=dropout, ffn_mult=ffn_mult, free_bias=fbias)\n",
    "                for i in range(K)\n",
    "            ])\n",
    "\n",
    "        # Gate: (vanilla + K experts)\n",
    "        self.gate = nn.Linear((K+1) * d_model, (K+1))\n",
    "        nn.init.constant_(self.gate.bias, float(gate_bias_init))\n",
    "\n",
    "        # E12 fusion MLP\n",
    "        self.fusion_enable = bool(getattr(CFG, \"fusion_enable\", False))\n",
    "        if self.fusion_enable:\n",
    "            fuse_in = d_model + W_   # concat [x, pooled reads]\n",
    "            hidden = int(CFG.fusion_hidden_mult * d_model)\n",
    "            self.fuse_ln = nn.LayerNorm(fuse_in)\n",
    "            self.fuse_mlp = nn.Sequential(\n",
    "                nn.Linear(fuse_in, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(getattr(CFG, \"fusion_drop\", 0.0)),\n",
    "                nn.Linear(hidden, d_model),\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pre_gate_ln = nn.LayerNorm((K+1) * d_model)\n",
    "        self.last_metrics = {}\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None, gate_override: float = None):\n",
    "        B, T, D = x.shape\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "\n",
    "        # 1) vanilla path\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "\n",
    "        # 2) memory experts\n",
    "        states_in = dnc_state if isinstance(dnc_state, (list, tuple)) else [dnc_state]*self.mem_experts\n",
    "        dts, states_out, per_mem_metrics = [], [], []\n",
    "        for m, st in zip(self.dncblocks, states_in):\n",
    "            dt, st2 = m(x, state=st)\n",
    "            dts.append(dt); states_out.append(st2)\n",
    "            per_mem_metrics.append(getattr(m, \"last_metrics\", {}) or {})\n",
    "\n",
    "        # 3) fusion (real read features if available)\n",
    "        if self.fusion_enable:\n",
    "            r_feat = getattr(self.dncblocks[0], \"last_read_feat\", None)\n",
    "            if r_feat is None:\n",
    "                r_feat = torch.zeros(B,T,self.dncblocks[0].W, device=x.device, dtype=vt.dtype)\n",
    "            fuse_in = torch.cat([x, r_feat], dim=-1)\n",
    "            delta = self.fuse_mlp(self.fuse_ln(fuse_in))\n",
    "            vt = vt + delta\n",
    "            self.last_metrics[\"fusion_delta_norm\"] = float(delta.norm().detach().item() / max(1, delta.numel()))\n",
    "\n",
    "        # 4) gate mixture\n",
    "        paths = [vt] + dts\n",
    "        z = self.pre_gate_ln(torch.cat(paths, dim=-1))\n",
    "        logits = self.gate(z) / max(1e-6, float(getattr(CFG, \"expert_gate_temp\", getattr(self, \"gate_temp\", 1.0))))\n",
    "        if gate_override is not None:\n",
    "            g_mem = float(gate_override)\n",
    "            g_vec = torch.zeros_like(logits)\n",
    "            g_vec[...,0] = 1.0 - g_mem\n",
    "            if self.mem_experts > 0:\n",
    "                g_vec[...,1:] = g_mem / float(self.mem_experts)\n",
    "            pi = g_vec\n",
    "        else:\n",
    "            pi = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        g_mem_synth = pi[...,1:].sum(dim=-1, keepdim=True)\n",
    "        out = sum(pi[...,i:i+1]*p for i,p in enumerate(paths))\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pi_mean = pi.mean(dim=(0,1))     # (K+1,)\n",
    "            H = - (pi * (pi.clamp_min(1e-9).log())).sum(dim=-1).mean().item()\n",
    "            self.last_metrics.update({\n",
    "                \"experts_pi_mean\": [float(v) for v in pi_mean.detach().cpu()],\n",
    "                \"experts_pi_entropy\": float(H),\n",
    "                \"write_gate_mean\": float(_mean_safely([m.get(\"write_gate_mean\", float(\"nan\")) for m in per_mem_metrics])),\n",
    "            })\n",
    "        return out, (states_out if self.mem_experts>1 else states_out[0]), g_mem_synth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "#### 2.5. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:02.175609300Z",
     "start_time": "2025-08-24T04:59:02.153104400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None] * len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            st_in = dnc_states[i]\n",
    "            # Defensive wrap: if block has K>1 experts, ensure list state shape\n",
    "            K = int(getattr(blk, \"mem_experts\", 1))\n",
    "            if K > 1 and not isinstance(st_in, (list, tuple)):\n",
    "                st_in = [st_in] * K\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=st_in)\n",
    "            gates.append(g.detach())\n",
    "\n",
    "\n",
    "\n",
    "    def forward_with_metrics(self, input_ids: torch.Tensor, attention_mask: \"Optional[torch.Tensor]\" = None,\n",
    "                             gate_override: \"Optional[float]\" = None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1].to(device)  # ensure CUDA\n",
    "        dnc_states = [None] * len(self.blocks)\n",
    "        gates_det, gates_raw, per_block = [], [], []\n",
    "\n",
    "        # enable metrics collection per block (if supported)\n",
    "        for blk in self.blocks:\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = True\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            st_in = dnc_states[i]\n",
    "            K = int(getattr(blk, \"mem_experts\", 1))\n",
    "            if K > 1 and not isinstance(st_in, (list, tuple)):\n",
    "                st_in = [st_in] * K\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=st_in, gate_override=gate_override)\n",
    "            gates_raw.append(g)\n",
    "            gates_det.append(g.detach())\n",
    "            per_block.append(getattr(blk, \"last_metrics\", {}) or {})\n",
    "\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = False\n",
    "\n",
    "        # lm head on its own device, then back to CUDA\n",
    "        lm_dev = self.base.lm_head.weight.device\n",
    "        y = self.proj_out(h).to(lm_dev, dtype=self.base.lm_head.weight.dtype)\n",
    "        logits = self.base.lm_head(y).to(device)\n",
    "\n",
    "        # --- aux dict (add alias \"blocks\" for compatibility) ---\n",
    "        aux = {\"per_block\": per_block, \"blocks\": per_block, \"gates_raw\": gates_raw, \"gates_detached\": gates_det}\n",
    "        aux[\"g_entropy_block\"] = []\n",
    "        for g in gates_det:\n",
    "            _, _, ent = _gate_metrics(g)\n",
    "            aux[\"g_entropy_block\"].append(ent)\n",
    "        return logits, gates_det, aux\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 3. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Synthetic tasks + simple instruction-following"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372040027d86e088"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:02.859716100Z",
     "start_time": "2025-08-24T04:59:02.832714900Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 HF dataset integration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eaf6ff06d518fec"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:03.301586700Z",
     "start_time": "2025-08-24T04:59:03.276588800Z"
    }
   },
   "outputs": [],
   "source": [
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\"); out = ex.get(o_field, \"\")\n",
    "        if instr and out: pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items: break\n",
    "    random.shuffle(pairs); return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]; out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Haystack batch creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20de8c6a7c630b74"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# --- Haystack (needle) eval: long-span retrieval of a key's value ---\n",
    "def make_haystack_batch(batch: int, T: int = 256, vocab: int = 1024, sentinel: int = 3):\n",
    "    \"\"\"\n",
    "    Build sequences: ... K V ... K ?  (next token should be V)\n",
    "    Returns: input_ids [B,T], answer_ids [B], query_pos [B] (position of '?')\n",
    "    \"\"\"\n",
    "    assert T >= 12, \"T too small for haystack layout\"\n",
    "    x = torch.randint(5, vocab, (batch, T), dtype=torch.long)\n",
    "    K = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "    V = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "\n",
    "    # Place (K,V) in the first half\n",
    "    p1 = torch.randint(low=T//8, high=T//2 - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p1] = K\n",
    "    x[torch.arange(batch), p1 + 1] = V\n",
    "\n",
    "    # Place (K, sentinel) in the last quarter\n",
    "    p2 = torch.randint(low=3*T//4, high=T - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p2] = K\n",
    "    x[torch.arange(batch), p2 + 1] = sentinel  # '?'\n",
    "    query_pos = p2 + 1  # position of '?'; we will look at logits at this position\n",
    "\n",
    "    return x, V, query_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:03.740252800Z",
     "start_time": "2025-08-24T04:59:03.715256900Z"
    }
   },
   "id": "3ef03e6eaa8a61bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4 MixtureSampler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b4f90ad9bf9b7"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float], names: Optional[List[str]] = None):\n",
    "        self.gens = gens\n",
    "        import torch as _t\n",
    "        self.weights = list(map(float, weights))\n",
    "        self.p = _t.tensor(self.weights, dtype=_t.float32)  # CPU is fine for multinomial\n",
    "        self.p /= (self.p.sum() + 1e-8)\n",
    "        self.names = names if names is not None else [f\"g{i}\" for i in range(len(gens))]\n",
    "        self.last_name = None\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch as _t\n",
    "        idx = _t.multinomial(self.p, 1).item()\n",
    "        self.last_name = self.names[idx]\n",
    "        return self.gens[idx](batch)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Update mixture probabilities at runtime (used by schedules).\"\"\"\n",
    "        import torch as _t\n",
    "        ws = list(map(float, weights))\n",
    "        t = _t.tensor(ws, dtype=_t.float32, device=self.p.device)\n",
    "        s = float(t.sum().item())\n",
    "        if s <= 0:\n",
    "            raise ValueError(\"Mixture weights must sum to > 0\")\n",
    "        self.p = t / s\n",
    "        self.weights = ws\n",
    "        # Optional sanity: warn if names length mismatches weights\n",
    "        if hasattr(self, \"names\") and len(self.names) != len(ws):\n",
    "            print(f\"[MixtureSampler] Warning: len(names)={len(self.names)} != len(weights)={len(ws)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:04.212291Z",
     "start_time": "2025-08-24T04:59:04.172292500Z"
    }
   },
   "id": "c47d40c7b9fdc5ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.5 Mixture Builder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6db5be99034904"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000) -> MixtureSampler:\n",
    "    \"\"\"\n",
    "    Build a mixture of generators: [HF, copy, repeat, nback].\n",
    "    - If hf_dataset is Alpaca-like (has instruction/output), we use hf_instruction_loader + make_hf_batch.\n",
    "    - Else we treat it as text-only (e.g., roneneldan/TinyStories), tokenizing and making windowed sequences.\n",
    "    - If HF fails/empty, we drop it and renormalize over synthetics.\n",
    "    \"\"\"\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "\n",
    "    gens, wts, names = [], [], []\n",
    "\n",
    "    hf_ok = False\n",
    "    hf_reason = \"\"\n",
    "    gen_hf = None\n",
    "\n",
    "    if hf_dataset:\n",
    "        try:\n",
    "            # Heuristic: use instruction loader for alpaca-like IDs\n",
    "            if \"alpaca\" in hf_dataset.lower():\n",
    "                pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\", \"output\"),\n",
    "                                              max_items=hf_max_items)\n",
    "                if pairs:\n",
    "                    def gen_hf(b): \n",
    "                        return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "                    hf_ok = True\n",
    "                else:\n",
    "                    hf_reason = \"hf_instruction_loader returned 0 pairs\"\n",
    "            else:\n",
    "                # Text-only fallback (e.g., roneneldan/TinyStories)\n",
    "                from datasets import load_dataset\n",
    "                # Try streaming first, then non-streaming\n",
    "                try:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\", streaming=True)\n",
    "                except Exception:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\")\n",
    "\n",
    "                # Find a usable text key\n",
    "                text_key = None\n",
    "                common_keys = (\"text\", \"content\", \"story\", \"document\", \"body\", \"article\")\n",
    "\n",
    "                feats = getattr(ds, \"features\", None)\n",
    "                if feats:\n",
    "                    for k in common_keys:\n",
    "                        if k in feats:\n",
    "                            text_key = k\n",
    "                            break\n",
    "\n",
    "                if text_key is None:\n",
    "                    # Probe first example (works for streaming iterable)\n",
    "                    try:\n",
    "                        first_ex = next(iter(ds))\n",
    "                        for k in common_keys:\n",
    "                            if k in first_ex:\n",
    "                                text_key = k\n",
    "                                break\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "\n",
    "                if text_key is None:\n",
    "                    hf_reason = \"no usable text field (tried: %s)\" % \",\".join(common_keys)\n",
    "                else:\n",
    "                    import random as _rnd\n",
    "                    import torch\n",
    "                    samples = []\n",
    "                    for ex in ds:\n",
    "                        txt = ex.get(text_key, None)\n",
    "                        if not txt:\n",
    "                            continue\n",
    "                        ids = tok(txt, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "                        if ids.numel() < 8:\n",
    "                            continue\n",
    "                        samples.append(ids.cpu())\n",
    "                        if len(samples) >= int(hf_max_items):\n",
    "                            break\n",
    "\n",
    "                    if len(samples) == 0:\n",
    "                        hf_reason = \"collected 0 tokenized samples\"\n",
    "                    else:\n",
    "                        def gen_hf(b: int) -> torch.Tensor:\n",
    "                            out = []\n",
    "                            for _ in range(b):\n",
    "                                ids = _rnd.choice(samples)\n",
    "                                n = ids.numel()\n",
    "                                if n >= mx:\n",
    "                                    s = _rnd.randint(0, n - mx)\n",
    "                                    seq = ids[s:s+mx]\n",
    "                                else:\n",
    "                                    pad = torch.full((mx - n,), pad_id, dtype=torch.long)\n",
    "                                    seq = torch.cat([ids, pad], dim=0)\n",
    "                                out.append(seq.unsqueeze(0))\n",
    "                            return torch.cat(out, dim=0)\n",
    "                        hf_ok = True\n",
    "        except Exception as e:\n",
    "            hf_reason = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "    # Assemble mixture in the agreed order (hf, copy, repeat, nback)\n",
    "    if hf_ok and gen_hf is not None:\n",
    "        gens.append(gen_hf); wts.append(weights[0]); names.append(\"hf\")\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        if hf_dataset:\n",
    "            print(f\"HF dataset unavailable or empty; using synthetic only. reason={hf_reason}\")\n",
    "        s_w = list(weights[1:])  # keep caller's relative weights for synthetics\n",
    "\n",
    "    # Synthetic tasks (your existing closures)\n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100, pad_id=pad_id, device=\"cpu\")\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "\n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback])\n",
    "    wts.extend(s_w)\n",
    "    names.extend([\"copy\", \"repeat\", \"nback\"])\n",
    "\n",
    "    return MixtureSampler(gens, wts, names=names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:04.706380200Z",
     "start_time": "2025-08-24T04:59:04.652382500Z"
    }
   },
   "id": "d629080a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Logging utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a8d8e756c4c8a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 TensorBoard Logger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "189691151b04221f"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import os, time, json, re\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        self.writer = None\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        logdir = logdir or \"./runs\"\n",
    "        run_name = run_name or time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def add_histogram(self, tag: str, values: \"torch.Tensor\", step: int, bins: int = 50):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        v = values\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.detach()\n",
    "            if v.device.type != \"cpu\": v = v.cpu()\n",
    "            v = v.reshape(-1).float()\n",
    "        self.writer.add_histogram(tag, v, global_step=step, bins=bins)\n",
    "\n",
    "    def add_text(self, tag: str, text: str, step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_text(tag, text, step)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n",
    "        \n",
    "        \n",
    "def start_tb_run(label: str = None, logdir: str = \"./runs\"):\n",
    "    \"\"\"Close any existing TB writer and open a fresh run dir with timestamp + optional label.\"\"\"\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TensorBoard not available; skipping start_tb_run.\")\n",
    "        return False\n",
    "    global tb\n",
    "    # Close/flush a previous writer if present\n",
    "    try:\n",
    "        if 'tb' in globals() and isinstance(tb, TBLogger) and getattr(tb, \"writer\", None):\n",
    "            tb.flush(); tb.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ts = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "    run_name = ts\n",
    "    if label:\n",
    "        safe = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(label))\n",
    "        run_name = f\"{ts}-{safe}\"\n",
    "\n",
    "    tb = TBLogger(logdir=logdir, run_name=run_name)\n",
    "    tb.add_text(\"run/label\", str(label or \"unlabeled\"), 0)\n",
    "    print(\"TB run started:\", getattr(tb, \"path\", None))\n",
    "    return True\n",
    "\n",
    "# TODO: wire this into start_tb_run above at some stage\n",
    "# try:\n",
    "#     _orig_start_tb_run = start_tb_run\n",
    "#     def start_tb_run(label: str = None):\n",
    "#         tb_obj = _orig_start_tb_run(label)\n",
    "#         echo_cfg(to_console=False, to_tb=True, tag=\"cfg/json\")\n",
    "#         return tb_obj\n",
    "# except Exception:\n",
    "#     pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:05.382483700Z",
     "start_time": "2025-08-24T04:59:05.316483200Z"
    }
   },
   "id": "b44d73c717b8b58c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Memory tracer & TensorBoard memory visualizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6636493a9a0eb2"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:05.788764Z",
     "start_time": "2025-08-24T04:59:05.764774300Z"
    }
   },
   "id": "028e39a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0322be9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1 Schedulers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eac969641de0772d"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:06.626383200Z",
     "start_time": "2025-08-24T04:59:06.581383600Z"
    }
   },
   "id": "b6be4ba6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2 Training Utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7a398e8eff544"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    #print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:07.842234200Z",
     "start_time": "2025-08-24T04:59:07.806235400Z"
    }
   },
   "id": "a70a7bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.3 Experimental training loop, stage 1 experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597e27677e1220b4"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train_experiment(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 5000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = False,\n",
    "    viz_prompt: str = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "    # NEW: optional schedules (each a list of (until_step, value))\n",
    "    mixture_schedule=None,         # e.g., [(100, (0.3,0.3,0.2,0.2)), (None, (0.4,0.2,0.2,0.2))]\n",
    "    gate_temp_schedule=None,       # e.g., [(100, 0.8), (None, 1.0)]\n",
    "    gate_reg_schedule=None,        # e.g., [(100, 3e-4), (None, 2e-4)]\n",
    "):\n",
    "    cfg = CFG\n",
    "    steps = int(steps or cfg.train_steps)\n",
    "    batch_size = int(batch_size or getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "\n",
    "    # Model & tokenizer\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "\n",
    "    # Sampler (and allow schedules to change weights)\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    def _apply_schedules(step: int, mix_name_hint=None):\n",
    "        # mixture schedule\n",
    "        if mixture_schedule:\n",
    "            for until, ws in mixture_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    try:\n",
    "                        mixer.set_weights(ws)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "        # gate temp schedule\n",
    "        if gate_temp_schedule:\n",
    "            for until, temp in gate_temp_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_temp\", float(temp))\n",
    "                    break\n",
    "        # gate reg schedule\n",
    "        if gate_reg_schedule:\n",
    "            for until, lam in gate_reg_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_reg_lambda\", float(lam))\n",
    "                    break\n",
    "\n",
    "    # TB setup\n",
    "    global tb\n",
    "    if TB_AVAILABLE:\n",
    "        try:\n",
    "            tb  # NameError if not defined\n",
    "        except NameError:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        if not isinstance(tb, TBLogger) or getattr(tb, \"writer\", None) is None:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        tblog = True\n",
    "        tb.add_text(\"run/config\", json.dumps({\n",
    "            \"steps\": steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    else:\n",
    "        tblog = False\n",
    "\n",
    "    head.train()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(amp_dtype in (torch.float16, torch.bfloat16)))\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        _apply_schedules(step)\n",
    "\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "        with torch.autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype != torch.float32)):\n",
    "            logits, gates, aux = head.forward_with_metrics(in_ids, gate_override=getattr(CFG, \"force_g\", None))\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "            \n",
    "            # Optional: encourage expert usage diversity (higher entropy across (vanilla + K experts))\n",
    "            div_lam = float(getattr(CFG, \"expert_diversity_lambda\", 0.0))\n",
    "            if div_lam > 0.0 and isinstance(aux, dict) and \"per_block\" in aux:\n",
    "                try:\n",
    "                    ent_vals = []\n",
    "                    for m in aux[\"per_block\"]:\n",
    "                        ent = float(m.get(\"experts_pi_entropy\", float(\"nan\")))\n",
    "                        if not math.isnan(ent):\n",
    "                            ent_vals.append(ent)\n",
    "                    if ent_vals:\n",
    "                        loss = loss - div_lam * (sum(ent_vals) / len(ent_vals))\n",
    "                except Exception as _e:\n",
    "                    # keep robust, don’t crash training if a metric is missing\n",
    "                    pass\n",
    "\n",
    "            # Optional: mild gate-usage regularizer\n",
    "            lam = float(getattr(CFG, \"gate_reg_lambda\", 0.0))\n",
    "            if lam > 0 and isinstance(gates, (list, tuple)) and len(gates) > 0:\n",
    "                reg = 0.0\n",
    "                for g in gates:\n",
    "                    g2 = _reduce_gate_tensor(g)\n",
    "                    # Encourage decisive routing on memory batches only? For now, uniform\n",
    "                    reg = reg + (g2.mean() * 0.0 + (g2 * (1 - g2)).mean())  # small entropy-like penalty\n",
    "                loss = loss + lam * reg\n",
    "            \n",
    "            lam_w = float(getattr(CFG, \"write_reg_lambda\", 0.0))\n",
    "            if lam_w > 0.0 and isinstance(aux, dict) and \"blocks\" in aux:\n",
    "                # Only apply on memory-tagged batches if flag is on\n",
    "                apply_reg = True\n",
    "                if bool(getattr(CFG, \"reg_only_on_memory_batches\", True)):\n",
    "                    # crude heuristic: if mixture sampler last batch name contains 'copy'/'repeat'/'nback'\n",
    "                    bn = getattr(mixer, \"last_name\", \"\")\n",
    "                    apply_reg = any(tk in bn for tk in (\"copy\",\"repeat\",\"nback\"))\n",
    "                if apply_reg:\n",
    "                    w_means = [b.get(\"write_gate_mean\") for b in aux[\"blocks\"] if isinstance(b, dict)]\n",
    "                    w_means = [float(x) for x in w_means if isinstance(x, (float,int))]\n",
    "                    if w_means:\n",
    "                        loss = loss + lam_w * (sum(w_means)/len(w_means))\n",
    "\n",
    "        use_scaler = (amp_dtype in (torch.float16, torch.bfloat16))\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step(); optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # --- Logging (TB) ---\n",
    "        if step % log_every == 0 and tblog:\n",
    "            # global scalars\n",
    "            tb.writer.add_scalar(\"train/loss\", float(loss.item()), step)\n",
    "            tb.writer.add_scalar(\"train/lr\", float(scheduler.get_last_lr()[0]), step)\n",
    "\n",
    "            # gate metrics per block (global)\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_mean\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_frac>0.5\", g_frac, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_entropy\", g_ent, step)\n",
    "\n",
    "                # per-task metrics (using the sampler's last batch type)\n",
    "                mix_name = getattr(mixer, \"last_name\", None) or \"unknown\"\n",
    "                tb.writer.add_scalar(f\"loss_by_task/{mix_name}\", float(loss.item()), step)\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_mean/{mix_name}\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_frac>0.5/{mix_name}\", g_frac, step)\n",
    "\n",
    "                # optional: quartiles (block 0)\n",
    "                if len(gates) > 0:\n",
    "                    g0 = _reduce_gate_tensor(gates[0].detach())\n",
    "                    T = g0.size(1); q = max(1, T // 4)\n",
    "                    slices = [(0, q), (q, 2*q), (2*q, 3*q), (3*q, T)]\n",
    "                    for qi, (s, e) in enumerate(slices, start=1):\n",
    "                        tb.writer.add_scalar(f\"gates/block0_q{qi}_mean/{mix_name}\", float(g0[:, s:e].mean().item()), step)\n",
    "                \n",
    "                 # E10: experts distributions\n",
    "                for bi, b in enumerate(aux.get(\"blocks\", [])):\n",
    "                    if isinstance(b, dict) and \"experts_pi_mean\" in b:\n",
    "                        pi_mean = b[\"experts_pi_mean\"]  # list of length K+1\n",
    "                        for j, v in enumerate(pi_mean):\n",
    "                            tb.writer.add_scalar(f\"experts/block_{bi}/pi_mean_{j}\", float(v), step)\n",
    "                    if isinstance(b, dict) and \"experts_pi_entropy\" in b:\n",
    "                        tb.writer.add_scalar(f\"experts/block_{bi}/pi_entropy\", float(b[\"experts_pi_entropy\"]), step)\n",
    "            \n",
    "                    # E12: fusion delta norm\n",
    "                    if isinstance(b, dict) and \"fusion_delta_norm\" in b:\n",
    "                        tb.writer.add_scalar(f\"fusion/block_{bi}/delta_norm\", float(b[\"fusion_delta_norm\"]), step)\n",
    "            \n",
    "                    # E13: write sparsity signal we used\n",
    "                    if isinstance(b, dict) and \"write_gate_mean\" in b:\n",
    "                        tb.writer.add_scalar(f\"reg/block_{bi}/write_gate_mean\", float(b[\"write_gate_mean\"]), step)\n",
    "            \n",
    "                # (optional) print current mixture weights if schedule is active\n",
    "                if \"mixture_schedule\" in locals() or \"mixture_schedule\" in globals():\n",
    "                    ws = getattr(mixer, \"weights\", None)\n",
    "                    if ws:\n",
    "                        tb.writer.add_text(\"schedule/mixture_weights\", str(ws), step)\n",
    "            \n",
    "                # keep console echo compact (existing print is fine); optionally add top‑1 expert\n",
    "                if isinstance(aux.get(\"blocks\"), list) and aux[\"blocks\"]:\n",
    "                    b0 = aux[\"blocks\"][0]\n",
    "                    if \"experts_pi_mean\" in b0:\n",
    "                        top = int(max(range(len(b0[\"experts_pi_mean\"])), key=lambda k: b0[\"experts_pi_mean\"][k]))\n",
    "                        print(f\"  [experts] block0 top={top} pi={b0['experts_pi_mean']}\")\n",
    "                        \n",
    "            # Expert routing diagnostics (if available from per_block metrics)\n",
    "            try:\n",
    "                if isinstance(aux, dict) and \"per_block\" in aux:\n",
    "                    for bi, m in enumerate(aux[\"per_block\"]):\n",
    "                        ent = m.get(\"experts_pi_entropy\", None)\n",
    "                        if ent is not None:\n",
    "                            tb.writer.add_scalar(f\"experts/block_{bi}_pi_entropy\", float(ent), step)\n",
    "                        pi_mean = m.get(\"experts_pi_mean\", None)  # list length K+1 (vanilla + experts)\n",
    "                        if isinstance(pi_mean, (list, tuple)) and len(pi_mean) > 0:\n",
    "                            for j, v in enumerate(pi_mean):\n",
    "                                tb.writer.add_scalar(f\"experts/block_{bi}_pi_mean/path_{j}\", float(v), step)\n",
    "                        # optional: write activity\n",
    "                        if \"write_gate_mean\" in m:\n",
    "                            tb.writer.add_scalar(f\"memory/block_{bi}_write_gate_mean\", float(m[\"write_gate_mean\"]), step)\n",
    "            except Exception as _e:\n",
    "                pass  # keep logging robust\n",
    "\n",
    "                \n",
    "            tb.flush()\n",
    "\n",
    "        # --- Console echo (always) ---\n",
    "        if step % log_every == 0:\n",
    "            g_means_print = []\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for g in gates:\n",
    "                    g_means_print.append(float(_reduce_gate_tensor(g).mean().item()))\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {scheduler.get_last_lr()[0]:.2e} | \"\n",
    "                  f\"gates={g_means_print} | mix={getattr(mixer,'last_name','?')}\")\n",
    "\n",
    "    # Optional memory viz\n",
    "    if viz_memory_after:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "    \n",
    "    # flush log      \n",
    "    if tblog:\n",
    "        tb.flush()\n",
    "\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:09.129136500Z",
     "start_time": "2025-08-24T04:59:09.100136200Z"
    }
   },
   "id": "f8879a898faf706a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.4 Robust forward for ParallelEnrichmentBlock\n",
    "TODO: this is a monkeypatch, non-critical but fold in properly when I have time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569f513137ac4911"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched ParallelEnrichmentBlock.forward -> multi-expert aware (dncblocks) + dtype-harmonization\n"
     ]
    }
   ],
   "source": [
    "# Rebind PEB.forward with dtype-harmonization (safe override)\n",
    "import torch\n",
    "\n",
    "def _peb_forward_multi(self, x: torch.Tensor, dnc_state=None, gate_override: float = None):\n",
    "    B, T, D = x.shape\n",
    "    mask = causal_mask(T, device=x.device)\n",
    "\n",
    "    # --- Decide a target dtype for the vanilla path: use ln1.weight.dtype\n",
    "    dtype_v = self.vanilla.ln1.weight.dtype\n",
    "    x_v = x.to(dtype_v) if x.dtype != dtype_v else x\n",
    "\n",
    "    # 1) Vanilla path in its own dtype\n",
    "    vt = self.vanilla(x_v, attn_mask=mask)  # (B,T,D), dtype=dtype_v\n",
    "\n",
    "    # Optional back-compat alias for mem_experts==1\n",
    "    if getattr(self, 'mem_experts', 1) == 1 and not hasattr(self, 'dncblock') and hasattr(self, 'dncblocks'):\n",
    "        try: self.dncblock = self.dncblocks[0]\n",
    "        except Exception: pass\n",
    "\n",
    "    # 2) Memory experts: run in their native dtype, then cast to dtype_v\n",
    "    K = int(getattr(self, 'mem_experts', 1))\n",
    "    states_in = dnc_state if isinstance(dnc_state, (list, tuple)) else [dnc_state] * K\n",
    "    dts, states_out, per_mem_metrics = [], [], []\n",
    "    last_read_feat = None  # (B,T,W) if provided by DNCformerBlock\n",
    "\n",
    "    for m, st in zip(self.dncblocks, states_in):\n",
    "        dt, st2 = m(x, state=st)          # compute with original x (native path)\n",
    "        if dt.dtype != dtype_v:\n",
    "            dt = dt.to(dtype_v)\n",
    "        dts.append(dt); states_out.append(st2)\n",
    "        pm = getattr(m, \"last_metrics\", {}) or {}\n",
    "        per_mem_metrics.append(pm)\n",
    "        # try to harvest a read feature for fusion if the block exposes one\n",
    "        if last_read_feat is None:\n",
    "            lf = getattr(m, \"last_read_feat\", None)\n",
    "            if lf is not None:\n",
    "                last_read_feat = lf\n",
    "\n",
    "    # 3) Optional fusion: use read-hint; align fusion input with fuse_ln dtype\n",
    "    if getattr(self, 'fusion_enable', False):\n",
    "        # pick a feature: provided by block or zeros as no-op\n",
    "        if last_read_feat is None:\n",
    "            W = getattr(self.dncblocks[0], \"W\", self.d_model)  # fallback\n",
    "            last_read_feat = torch.zeros(B, T, W, device=x.device, dtype=dtype_v)\n",
    "        else:\n",
    "            last_read_feat = last_read_feat.to(dtype_v)\n",
    "\n",
    "        # fuse LayerNorm/MLP likely float32; match their parameter dtype\n",
    "        dtype_f = self.fuse_ln.weight.dtype\n",
    "        fuse_in = torch.cat([x_v, last_read_feat], dim=-1)\n",
    "        fuse_in = fuse_in.to(dtype_f) if fuse_in.dtype != dtype_f else fuse_in\n",
    "        delta = self.fuse_mlp(self.fuse_ln(fuse_in))\n",
    "        if delta.dtype != vt.dtype:\n",
    "            delta = delta.to(vt.dtype)\n",
    "        vt = vt + delta\n",
    "        self.last_metrics[\"fusion_delta_norm\"] = float(delta.norm().detach().item() / max(1, delta.numel()))\n",
    "\n",
    "    # 4) Gate over (vanilla + K experts)\n",
    "    paths = [vt] + dts                             # all in dtype_v\n",
    "    z = torch.cat(paths, dim=-1)                   # (B,T,(K+1)*D), dtype=dtype_v\n",
    "\n",
    "    # gate linear likely float32: feed it in its dtype and bring logits back to float32\n",
    "    dtype_g = self.gate.weight.dtype\n",
    "    z_g = z.to(dtype_g) if z.dtype != dtype_g else z\n",
    "    temp = float(getattr(CFG, \"expert_gate_temp\", getattr(self, \"gate_temp\", 1.0)))\n",
    "    logits = self.gate(z_g) / max(1e-6, temp)     # float32\n",
    "\n",
    "    if gate_override is not None:\n",
    "        g_mem = float(gate_override)\n",
    "        pi = torch.zeros_like(logits)             # (B,T,K+1), float32\n",
    "        pi[..., 0] = 1.0 - g_mem\n",
    "        if K > 0:\n",
    "            pi[..., 1:] = g_mem / float(K)\n",
    "    else:\n",
    "        pi = torch.softmax(logits, dim=-1)        # float32\n",
    "\n",
    "    # Synthesize a single memory gate (back-compat); cast pi to dtype_v for the weighted sum\n",
    "    pi_v = pi.to(dtype_v) if pi.dtype != dtype_v else pi\n",
    "    g_mem_synth = pi_v[..., 1:].sum(dim=-1, keepdim=True)  # (B,T,1), dtype_v\n",
    "\n",
    "    out = sum(pi_v[..., i:i+1] * p for i, p in enumerate(paths))  # dtype_v\n",
    "    out = self.dropout(out)\n",
    "\n",
    "    # 5) Metrics\n",
    "    with torch.no_grad():\n",
    "        pi_mean = pi.mean(dim=(0, 1))\n",
    "        H = - (pi * (pi.clamp_min(1e-9).log())).sum(dim=-1).mean().item()\n",
    "        def _safe_mean(vals):\n",
    "            v = [float(x) for x in vals if isinstance(x, (int, float)) and x == x]\n",
    "            return float(sum(v) / max(1, len(v))) if v else float('nan')\n",
    "        wg = _safe_mean([m.get(\"write_gate_mean\", float('nan')) for m in per_mem_metrics])\n",
    "        self.last_metrics.update({\n",
    "            \"experts_pi_mean\": [float(v) for v in pi_mean.detach().cpu()],\n",
    "            \"experts_pi_entropy\": float(H),\n",
    "            \"write_gate_mean\": wg,\n",
    "        })\n",
    "\n",
    "    return out, (states_out if K > 1 else states_out[0]), g_mem_synth\n",
    "\n",
    "ParallelEnrichmentBlock.forward = _peb_forward_multi\n",
    "print(\"Patched ParallelEnrichmentBlock.forward -> multi-expert aware (dncblocks) + dtype-harmonization\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:09.996706200Z",
     "start_time": "2025-08-24T04:59:09.977708600Z"
    }
   },
   "id": "3eabc7991c7e743d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.5 Guardrails warnings (for __debug__ mode)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "409782a954e2cb0"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "import warnings, numbers, torch\n",
    "\n",
    "def _normalize_weights(ws):\n",
    "    ws = list(map(float, ws))\n",
    "    s = sum(ws)\n",
    "    if s <= 0:\n",
    "        warnings.warn(\"[guard] mixture_weights sum<=0; normalizing to uniform.\")\n",
    "        n = max(1, len(ws)); return [1.0/n]*n\n",
    "    return [w/(s+1e-8) for w in ws]\n",
    "\n",
    "def _check_schedule(name, sched):\n",
    "    if sched is None: return True\n",
    "    if not isinstance(sched, (list, tuple)):\n",
    "        warnings.warn(f\"[guard] {name} must be list[(until,value)]\"); return False\n",
    "    ok = True\n",
    "    for i, it in enumerate(sched):\n",
    "        if not (isinstance(it, (list, tuple)) and len(it)==2):\n",
    "            warnings.warn(f\"[guard] {name}[{i}] not (until,value)\"); ok=False; continue\n",
    "        u,v = it\n",
    "        if u is not None and not isinstance(u, numbers.Number):\n",
    "            warnings.warn(f\"[guard] {name}[{i}].until not a number\"); ok=False\n",
    "    return ok\n",
    "\n",
    "# Wrap train_experiment\n",
    "try:\n",
    "    if 'train_experiment' in globals() and not getattr(train_experiment, \"__guarded__\", False):\n",
    "        _orig_train_experiment = train_experiment\n",
    "        def train_experiment(*args, **kwargs):\n",
    "            if __debug__:\n",
    "                if \"mixture_weights\" in kwargs:\n",
    "                    kwargs[\"mixture_weights\"] = _normalize_weights(kwargs[\"mixture_weights\"])\n",
    "                for nm in (\"mixture_schedule\",\"gate_temp_schedule\",\"gate_reg_schedule\"):\n",
    "                    _check_schedule(nm, kwargs.get(nm))\n",
    "                # Optional: enforce positive gate_temp if present in CFG\n",
    "                if hasattr(CFG, \"gate_temp\") and CFG.gate_temp <= 0:\n",
    "                    warnings.warn(\"[guard] CFG.gate_temp <= 0; routing may degenerate.\")\n",
    "            return _orig_train_experiment(*args, **kwargs)\n",
    "        train_experiment.__guarded__ = True\n",
    "except Exception as e:\n",
    "    print(\"[guard] train_experiment wrapper skipped:\", e)\n",
    "\n",
    "# Wrap DNCFormerHead.forward\n",
    "try:\n",
    "    if 'DNCFormerHead' in globals() and not getattr(DNCFormerHead, \"__guarded__\", False):\n",
    "        _orig_forward = DNCFormerHead.forward\n",
    "        def forward(self, input_ids: torch.Tensor, attention_mask=None):\n",
    "            if __debug__:\n",
    "                if not isinstance(input_ids, torch.Tensor) or input_ids.dtype != torch.long:\n",
    "                    warnings.warn(f\"[guard] input_ids dtype should be torch.long (got {getattr(input_ids,'dtype',None)})\")\n",
    "            return _orig_forward(self, input_ids, attention_mask=attention_mask)\n",
    "        DNCFormerHead.forward = forward\n",
    "        DNCFormerHead.__guarded__ = True\n",
    "except Exception as e:\n",
    "    print(\"[guard] DNCFormerHead wrapper skipped:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:10.437183300Z",
     "start_time": "2025-08-24T04:59:10.414187200Z"
    }
   },
   "id": "c3e7e5f06b1681b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Eval harnesses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9ee90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1 copy, reverse, needle-in-haystack"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73218bd54c4f2bc"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad() # WARNING - planned for depreciation\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_haystack(head, steps: int = 50, batch: int = 16, T: int = 256, vocab: int = 1024,\n",
    "                      tb_step: int = None, fast: bool = False):\n",
    "    \"\"\"\n",
    "    Long-span retrieval probe: ... K V ... K ? => predict V at '?'.\n",
    "    fast=True uses inference_mode() and smaller defaults to speed up sweeps.\n",
    "    \"\"\"\n",
    "    from torch.amp import autocast\n",
    "    head.eval()\n",
    "\n",
    "    # Fast path defaults (can still be overridden by explicit args)\n",
    "    if fast:\n",
    "        steps = min(steps, 10)\n",
    "        batch = min(batch, 8)\n",
    "        T = min(T, 128)\n",
    "\n",
    "    device_ = next(head.parameters()).device\n",
    "    use_amp = (amp_dtype in (torch.float16, torch.bfloat16)) and torch.cuda.is_available()\n",
    "\n",
    "    accs, losses = [], []\n",
    "    ctx = torch.inference_mode() if fast else torch.no_grad()\n",
    "    with ctx:\n",
    "        for _ in range(steps):\n",
    "            x, V, qpos = make_haystack_batch(batch, T=T, vocab=vocab)\n",
    "            x = x.to(device_); V = V.to(device_); qpos = qpos.to(device_)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    logits, _g = head(x)\n",
    "            else:\n",
    "                logits, _g = head(x)\n",
    "\n",
    "            idx = torch.arange(x.size(0), device=device_)\n",
    "            logits_q = logits[idx, qpos, :].float()\n",
    "            loss = F.cross_entropy(logits_q, V)\n",
    "            pred = logits_q.argmax(dim=-1)\n",
    "\n",
    "            accs.append((pred == V).float().mean().item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    acc_m = float(np.mean(accs)); loss_m = float(np.mean(losses))\n",
    "\n",
    "    if TB_AVAILABLE and ('tb' in globals()):\n",
    "        tb.writer.add_scalar(\"eval/haystack_acc\", acc_m, tb_step if tb_step is not None else 0)\n",
    "        tb.writer.add_scalar(\"eval/haystack_loss\", loss_m, tb_step if tb_step is not None else 0)\n",
    "        tb.flush()\n",
    "\n",
    "    head.train()\n",
    "    print(f\"[Haystack] acc={acc_m:.3f} | loss={loss_m:.3f} | fast={fast}\")\n",
    "    return acc_m, loss_m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:11.625087100Z",
     "start_time": "2025-08-24T04:59:11.579088200Z"
    }
   },
   "id": "efb2dc08fe1942ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Unit tests, smoke tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3036ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.1 basic unit test, model integrity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba6259d196dd13cc"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def run_basic_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:12.452071Z",
     "start_time": "2025-08-24T04:59:12.415072600Z"
    }
   },
   "id": "2cb70f70"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#run_basic_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:33:54.933838Z",
     "start_time": "2025-08-23T08:33:54.902837800Z"
    }
   },
   "id": "f9f2d4e1ea8a1a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.2 Evaluator unit tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b399df35806fe6f5"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:33:56.447680200Z",
     "start_time": "2025-08-23T08:33:56.406680500Z"
    }
   },
   "id": "db24ba31"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "#run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:41.219895800Z",
     "start_time": "2025-08-23T07:45:41.192897Z"
    }
   },
   "id": "4c45b9e3f62413ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.3 GPU VRAM diagnostics + cuda allocation smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683887a5f3501496"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=9.62 GB | reserved=9.64 GB | free=14.73 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12cd64b9014340c4b613c00d85e3d4b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=17.26 GB | reserved=17.29 GB | free=7.09 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, contextlib\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "        \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:47.390264900Z",
     "start_time": "2025-08-23T07:45:42.244896700Z"
    }
   },
   "id": "2af2ed601f9c7606"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.4 TB logger test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d48becf8a7f50a"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB active: True | logdir: ./runs\\dncformer-20250823-013407\n"
     ]
    }
   ],
   "source": [
    "tb = TBLogger(logdir=\"./runs\")\n",
    "try:\n",
    "    tb\n",
    "except NameError:\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "print(\"TB active:\", TB_AVAILABLE, \"| logdir:\", getattr(tb, \"path\", None))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:34:07.928343400Z",
     "start_time": "2025-08-23T08:34:07.902164800Z"
    }
   },
   "id": "bf494415ac5e8547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.5 Memory tracer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1d5de08007861f"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:47.468264900Z",
     "start_time": "2025-08-23T07:45:47.377266900Z"
    }
   },
   "id": "d842ad22"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:47.501265400Z",
     "start_time": "2025-08-23T07:45:47.392266Z"
    }
   },
   "id": "86a52d1c90f05c57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.6 Data generator smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6097ee7f06d9e87c"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_copy batch shape: torch.Size([3, 128]) | dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:47.562265700Z",
     "start_time": "2025-08-23T07:45:47.409265800Z"
    }
   },
   "id": "be0b78c71554906c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.7 Mixture sampler smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92b9b09dbe07c2b"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "p1: [0.30000001192092896, 0.30000001192092896, 0.25, 0.15000000596046448]\n"
     ]
    }
   ],
   "source": [
    "ms = MixtureSampler(gens=[lambda b: None, lambda b: None, lambda b: None, lambda b: None],\n",
    "                    weights=[0.4,0.2,0.2,0.2],\n",
    "                    names=[\"hf\",\"copy\",\"repeat\",\"nback\"])\n",
    "print(\"p0:\", ms.p.tolist())    # ~[0.4,0.2,0.2,0.2]\n",
    "ms.set_weights([0.3,0.3,0.25,0.15])\n",
    "print(\"p1:\", ms.p.tolist())    # ~[0.3,0.3,0.25,0.15]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:45:47.859267300Z",
     "start_time": "2025-08-23T07:45:47.823268300Z"
    }
   },
   "id": "635b3129c4abe2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.8 Build mixer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc74e56a0b98ce26"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fda2a8722f9477c8831b40df5b96d2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n"
     ]
    }
   ],
   "source": [
    "# 1) With Alpaca (instruction/output)\n",
    "tok, _ = build_model_and_tokenizer()\n",
    "m1 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=500)\n",
    "print(\"names:\", m1.names, \"| p:\", m1.p.tolist())  # expect 'hf' present\n",
    "\n",
    "# 2) With TinyStories (text)\n",
    "m2 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"roneneldan/TinyStories\", hf_max_items=500)\n",
    "print(\"names:\", m2.names, \"| p:\", m2.p.tolist())  # expect 'hf' present (text path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:46:03.056088400Z",
     "start_time": "2025-08-23T07:45:49.139266Z"
    }
   },
   "id": "92366ea6aeb2c846"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.9 haystack smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63121cfeae026f7c"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# tok, head = build_model_and_tokenizer()\n",
    "# acc, loss = evaluate_haystack(head, steps=2, batch=4, T=64, vocab=512, tb_step=0)\n",
    "# print(\"Haystack smoke:\", acc, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:46:03.079088300Z",
     "start_time": "2025-08-23T07:46:03.053088800Z"
    }
   },
   "id": "dca7ccde4a7a19d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.10 Training run Sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867e3f96209f238d"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a9cc2ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:11.383866300Z",
     "start_time": "2025-08-23T07:46:03.068087800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=10.61 GB | reserved=10.63 GB | free=13.74 GB | total=25.77 GB\n",
      "[snapshot: before train_experiment] alloc=10.61 GB | reserved=10.63 GB | free=13.74 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80a16479e3044558b332463092260e7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.9081056118011475, 0.09189442545175552]\n",
      "step 10 | loss 10.4021 | lr 2.00e-05 | gates=[0.09189443290233612, 0.10174798965454102] | mix=copy\n",
      "[snapshot: after train_experiment] alloc=20.25 GB | reserved=33.38 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()\n",
    "cuda_report(\"snapshot: before train_experiment\")\n",
    "head, tok = train_experiment(steps=10, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "cuda_report(\"snapshot: after train_experiment\")\n",
    "#Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.11 Integrity panel\n",
    "- shapes check\n",
    "- dtyptes check\n",
    "- forward check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c77708bd8690c4"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "import torch, contextlib, json\n",
    "\n",
    "def count_params(m: torch.nn.Module, trainable_only=True):\n",
    "    return sum(p.numel() for p in m.parameters() if (p.requires_grad or not trainable_only))\n",
    "\n",
    "def integrity_panel(tok=None, head=None, T: int = 8):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[integrity] device={dev} | amp_dtype={globals().get('amp_dtype', torch.float32)}\")\n",
    "    print(f\"[integrity] base_model_id={getattr(CFG, 'base_model_id', '?')}\")\n",
    "    for k in (\"d_model\", \"N\", \"W\", \"R\", \"num_blocks\"):\n",
    "        if hasattr(CFG, k):\n",
    "            print(f\"[integrity] CFG.{k}={getattr(CFG,k)}\")\n",
    "\n",
    "    if head is not None:\n",
    "        print(f\"[integrity] head params (trainable): {count_params(head):,}\")\n",
    "\n",
    "    # SDPA context visibility\n",
    "    with contextlib.suppress(Exception):\n",
    "        print(f\"[integrity] SDPA_CTX={type(globals().get('SDPA_CTX', None)).__name__}\")\n",
    "\n",
    "    # Tiny forward\n",
    "    try:\n",
    "        if tok is None or head is None:\n",
    "            print(\"[integrity] (skip tiny forward; missing tok/head)\")\n",
    "            return\n",
    "        tok.pad_token = tok.pad_token or tok.eos_token\n",
    "        dummy = tok(\"hello world\", return_tensors=\"pt\")\n",
    "        x = dummy.input_ids.to(dev)\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=globals().get(\"amp_dtype\", torch.float32),\n",
    "                                enabled=(globals().get(\"amp_dtype\", torch.float32) != torch.float32)):\n",
    "                logits, gates = head(x)\n",
    "        print(f\"[integrity] forward ok | logits={tuple(logits.shape)} | gates={[tuple(g.shape) for g in gates]}\")\n",
    "        # Basic gate sanity\n",
    "        for i, g in enumerate(gates):\n",
    "            g_mean = float(torch.sigmoid(g).mean().item()) if g.dtype.is_floating_point else float(g.float().mean().item())\n",
    "            print(f\"[integrity] gate[{i}] mean≈{g_mean:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(\"[integrity] forward failed:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:11.427866Z",
     "start_time": "2025-08-23T07:47:11.386866600Z"
    }
   },
   "id": "f2459475bc37800a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.12 Seed, build integrity, and save/load smoke tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764d7cad6218a8cc"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# 1) Seed determinism invoked\n",
    "set_seed(getattr(CFG, \"seed\", 42))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:11.444877600Z",
     "start_time": "2025-08-23T07:47:11.398866900Z"
    }
   },
   "id": "bc04e917bfc4dc9a"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67bb6ed784784e32b59ee7665ab5602c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[integrity] device=cuda | amp_dtype=torch.bfloat16\n",
      "[integrity] base_model_id=microsoft/Phi-3-mini-4k-instruct\n",
      "[integrity] CFG.d_model=3072\n",
      "[integrity] head params (trainable): 494,574,238\n",
      "[integrity] SDPA_CTX=NoneType\n",
      "[integrity] forward failed: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "# 2) Build & integrity\n",
    "tok, head = build_model_and_tokenizer()\n",
    "integrity_panel(tok, head, T=8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:17.217656100Z",
     "start_time": "2025-08-23T07:47:11.414865900Z"
    }
   },
   "id": "2374567572251938"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save_head] wrote checkpoints\\smoke.pt and checkpoints\\smoke.meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_297088\\2377763709.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(in_path, map_location=map_location or \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_head] restored successfully.\n"
     ]
    }
   ],
   "source": [
    "# 3) Save/load head (no-op training resume test)\n",
    "save_head(head, \"./checkpoints\", cfg=CFG, run_label=\"smoke\")\n",
    "_ = load_head(head, \"./checkpoints/smoke.pt\", strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:34.812739500Z",
     "start_time": "2025-08-23T07:47:17.219656800Z"
    }
   },
   "id": "30c2fe9c40d840ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.13 Parallel / fused memory blocks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4585e0ae262e14c3"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def _assert_close(a, b, eps=1e-5, msg=\"\"):\n",
    "    assert abs(float(a) - float(b)) <= eps, msg or f\"{a} vs {b}\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_parallel_block_basic():\n",
    "    print(\"[smoke] ParallelEnrichmentBlock basic (K=1, fusion=False)\")\n",
    "    B,T,D = 2, 5, 32\n",
    "    R,W,N = 1, 8, 16\n",
    "    x = torch.randn(B,T,D, device=device)\n",
    "    CFG.mem_experts = 1\n",
    "    CFG.fusion_enable = False\n",
    "    CFG.per_block_cfg = None\n",
    "    blk = ParallelEnrichmentBlock(d_model=D, d_in=D, R=R, W=W, N=N, heads=2, dropout=0.0, ffn_mult=2.0).to(device)\n",
    "    y, st, g = blk(x, dnc_state=None)\n",
    "    assert y.shape == (B,T,D)\n",
    "    assert isinstance(st, dict) or st is None\n",
    "    assert g.shape == (B,T,1)\n",
    "    assert (g>=0).all() and (g<=1).all()\n",
    "    print(\"  ok.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_parallel_block_multi_experts():\n",
    "    print(\"[smoke] Multi-experts (K=2), fusion off\")\n",
    "    B,T,D = 2, 5, 32\n",
    "    R,W,N = 1, 8, 16\n",
    "    x = torch.randn(B,T,D, device=device)\n",
    "    CFG.mem_experts = 2\n",
    "    CFG.expert_N = [N, N]\n",
    "    CFG.expert_W = W\n",
    "    CFG.expert_R = R\n",
    "    CFG.fusion_enable = False\n",
    "    CFG.per_block_cfg = None\n",
    "    blk = ParallelEnrichmentBlock(d_model=D, d_in=D, R=R, W=W, N=N, heads=2, dropout=0.0, ffn_mult=2.0).to(device)\n",
    "    y, st, g = blk(x, dnc_state=None)\n",
    "    assert y.shape == (B,T,D)\n",
    "    assert isinstance(st, list) and len(st)==2, \"state should be list(len=K) when mem_experts>1\"\n",
    "    assert g.shape == (B,T,1) and (g>=0).all() and (g<=1).all()\n",
    "    # experts metrics\n",
    "    pm = blk.last_metrics.get(\"experts_pi_mean\", None)\n",
    "    assert isinstance(pm, list) and len(pm)==3, \"experts_pi_mean should have K+1 entries (vanilla+experts)\"\n",
    "    print(\"  ok.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_fusion_path():\n",
    "    print(\"[smoke] Fusion path uses real read features if available\")\n",
    "    B,T,D = 2, 6, 32\n",
    "    R,W,N = 1, 8, 16\n",
    "    x = torch.randn(B,T,D, device=device)\n",
    "    CFG.mem_experts = 1\n",
    "    CFG.fusion_enable = True\n",
    "    CFG.fusion_hidden_mult = 2.0\n",
    "    CFG.fusion_drop = 0.0\n",
    "    blk = ParallelEnrichmentBlock(d_model=D, d_in=D, R=R, W=W, N=N, heads=2, dropout=0.0, ffn_mult=2.0).to(device)\n",
    "    # first pass (collects read feats inside DNCformerBlock)\n",
    "    y1, st, g = blk(x, dnc_state=None)\n",
    "    # second pass to check delta norm presence\n",
    "    y2, st2, g2 = blk(x, dnc_state=st)\n",
    "    dn = blk.last_metrics.get(\"fusion_delta_norm\", None)\n",
    "    assert dn is not None, \"fusion_delta_norm should be logged\"\n",
    "    assert y2.shape == (B,T,D) and g2.shape==(B,T,1)\n",
    "    print(\"  ok.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_per_block_override():\n",
    "    print(\"[smoke] per_block_cfg override (E11)\")\n",
    "    B,T,D = 2, 4, 32\n",
    "    R,W,N = 1, 8, 16\n",
    "    CFG.mem_experts = 1\n",
    "    CFG.fusion_enable = False\n",
    "    # Two different blocks to test overrides\n",
    "    CFG.per_block_cfg = [\n",
    "        {\"N\": 24, \"W\": 8,  \"R\": 1, \"gate_temp\": 1.0, \"free_bias\": +0.2},\n",
    "        {\"N\": 16, \"W\": 16, \"R\": 1, \"gate_temp\": 0.8, \"free_bias\": -0.2},\n",
    "    ]\n",
    "    blk0 = ParallelEnrichmentBlock(d_model=D, d_in=D, R=R, W=W, N=N, heads=2, dropout=0.0, ffn_mult=2.0, block_index=0).to(device)\n",
    "    blk1 = ParallelEnrichmentBlock(d_model=D, d_in=D, R=R, W=W, N=N, heads=2, dropout=0.0, ffn_mult=2.0, block_index=1).to(device)\n",
    "    # check effective N/W on internal DNC\n",
    "    assert blk0.dncblocks[0].N == 24 and blk0.dncblocks[0].W == 8\n",
    "    assert blk1.dncblocks[0].N == 16 and blk1.dncblocks[0].W == 16\n",
    "    # forward for sanity\n",
    "    x = torch.randn(B,T,D, device=device)\n",
    "    y0, st0, g0 = blk0(x, dnc_state=None)\n",
    "    y1, st1, g1 = blk1(x, dnc_state=None)\n",
    "    assert y0.shape == (B,T,D) and y1.shape==(B,T,D)\n",
    "    print(\"  ok.\")\n",
    "\n",
    "def run_patch_smoke_tests():\n",
    "    smoke_parallel_block_basic()\n",
    "    smoke_parallel_block_multi_experts()\n",
    "    smoke_fusion_path()\n",
    "    smoke_per_block_override()\n",
    "    print(\"[smoke] all tests passed.\")\n",
    "\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Fallback sdpa context if not present\n",
    "try:\n",
    "    sdpa_ctx  # noqa: F401\n",
    "except NameError:\n",
    "    import contextlib\n",
    "    def sdpa_ctx():\n",
    "        return contextlib.nullcontext()\n",
    "\n",
    "# Fallback device if not present\n",
    "try:\n",
    "    device  # noqa: F401\n",
    "except NameError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TinyBaseLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal stand-in for AutoModelForCausalLM that DNCFormerHead expects:\n",
    "      - config.hidden_size\n",
    "      - get_input_embeddings().num_embeddings\n",
    "      - forward(..., output_hidden_states=True) -> object with .hidden_states\n",
    "      - lm_head: Linear(d_model -> vocab)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: int = 1024, d_model: int = 32):\n",
    "        super().__init__()\n",
    "        self.config = types.SimpleNamespace(hidden_size=d_model, use_return_dict=True)\n",
    "        self.embed = nn.Embedding(vocab, d_model)\n",
    "        # a tiny 2-layer MLP \"backbone\"\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(d_model, vocab, bias=False)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask=None,\n",
    "                position_ids=None, past_key_values=None, inputs_embeds=None,\n",
    "                use_cache=False, output_attentions=False, output_hidden_states=False,\n",
    "                return_dict=True):\n",
    "        if inputs_embeds is None:\n",
    "            x = self.embed(input_ids)\n",
    "        else:\n",
    "            x = inputs_embeds\n",
    "        h = self.backbone(x)  # (B,T,D)\n",
    "        if output_hidden_states:\n",
    "            out = types.SimpleNamespace(hidden_states=[x, h])\n",
    "        else:\n",
    "            out = types.SimpleNamespace(last_hidden_state=h)\n",
    "        return out\n",
    "\n",
    "def _make_tiny_head(mem_experts: int = 1, n_blocks: int = 2, d_model: int = 32,\n",
    "                    R: int = 1, W: int = 8, N: int = 16, heads: int = 2,\n",
    "                    dropout: float = 0.0, ffn_mult: float = 2.0,\n",
    "                    gate_bias_init: float = -1.0):\n",
    "    \"\"\"\n",
    "    Build a small DNCFormerHead wired to TinyBaseLM; does not touch global training.\n",
    "    \"\"\"\n",
    "    # mirror current CFG but keep local overrides\n",
    "    local_cfg = types.SimpleNamespace(\n",
    "        d_model=None,\n",
    "        dnc_read_heads=R, dnc_cell_size=W, dnc_nr_cells=N,\n",
    "        attn_heads=heads, attn_dropout=dropout, ffn_mult=ffn_mult,\n",
    "        gate_bias_init=gate_bias_init,\n",
    "        n_blocks=n_blocks,\n",
    "    )\n",
    "    # honor mem_experts at block construction time:\n",
    "    setattr(CFG, \"mem_experts\", int(mem_experts))\n",
    "\n",
    "    base = TinyBaseLM(vocab=1024, d_model=d_model).to(device)\n",
    "    head_local = DNCFormerHead(base, local_cfg).to(device)\n",
    "    return head_local\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_head_mem_experts_one_step():\n",
    "    print(\"[smoke] head forward (mem_experts=1)\")\n",
    "    head_local = _make_tiny_head(mem_experts=1, n_blocks=2, d_model=32, R=1, W=8, N=16, heads=2)\n",
    "    B, T = 2, 8\n",
    "    vocab_guess = getattr(head_local.base.get_input_embeddings(), \"num_embeddings\", 1024)\n",
    "    dummy = torch.randint(0, min(1024, vocab_guess), (B, T), device=device)\n",
    "    logits, gates = head_local(dummy)\n",
    "    assert logits.shape[:2] == (B, T)\n",
    "    assert len(gates) == len(head_local.blocks)\n",
    "    for g in gates:\n",
    "        assert g.shape[:2] == (B, T) and (g >= 0).all() and (g <= 1).all()\n",
    "    print(\"  ok.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_head_mem_experts_two_step():\n",
    "    print(\"[smoke] head forward_with_metrics (mem_experts=2)\")\n",
    "    head_local = _make_tiny_head(mem_experts=2, n_blocks=2, d_model=32, R=1, W=8, N=16, heads=2)\n",
    "    B, T = 2, 8\n",
    "    vocab_guess = getattr(head_local.base.get_input_embeddings(), \"num_embeddings\", 1024)\n",
    "    dummy = torch.randint(0, min(1024, vocab_guess), (B, T), device=device)\n",
    "    logits, gates, aux = head_local.forward_with_metrics(dummy, gate_override=None)\n",
    "    assert logits.shape[:2] == (B, T)\n",
    "    assert \"per_block\" in aux and \"blocks\" in aux\n",
    "    for m in aux[\"blocks\"]:\n",
    "        assert isinstance(m, dict)\n",
    "    print(\"  ok.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:34:53.310026500Z",
     "start_time": "2025-08-23T08:34:53.263026300Z"
    }
   },
   "id": "b0210213ee403b08"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smoke] ParallelEnrichmentBlock basic (K=1, fusion=False)\n",
      "  ok.\n",
      "[smoke] Multi-experts (K=2), fusion off\n",
      "  ok.\n",
      "[smoke] Fusion path uses real read features if available\n",
      "  ok.\n",
      "[smoke] per_block_cfg override (E11)\n",
      "  ok.\n",
      "[smoke] all tests passed.\n"
     ]
    }
   ],
   "source": [
    "run_patch_smoke_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:34:54.744267400Z",
     "start_time": "2025-08-23T08:34:54.062132300Z"
    }
   },
   "id": "2161e20bc32dc7a0"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Replace the body of the two smoke tests with this (only the call site changes)\n",
    "@torch.no_grad()\n",
    "def smoke_head_mem_experts_one_step():\n",
    "    print(\"[smoke] head forward_with_metrics (mem_experts=1)\")\n",
    "    head_local = _make_tiny_head(mem_experts=1, n_blocks=2, d_model=32, R=1, W=8, N=16, heads=2)\n",
    "    B, T = 2, 8\n",
    "    vocab_guess = getattr(head_local.base.get_input_embeddings(), \"num_embeddings\", 1024)\n",
    "    dummy = torch.randint(0, min(1024, vocab_guess), (B, T), device=device)\n",
    "    logits, gates, aux = head_local.forward_with_metrics(dummy)\n",
    "    assert logits.shape[:2] == (B, T)\n",
    "    assert len(gates) == len(head_local.blocks)\n",
    "    print(\"  ok.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def smoke_head_mem_experts_two_step():\n",
    "    print(\"[smoke] head forward_with_metrics (mem_experts=2)\")\n",
    "    head_local = _make_tiny_head(mem_experts=2, n_blocks=2, d_model=32, R=1, W=8, N=16, heads=2)\n",
    "    B, T = 2, 8\n",
    "    vocab_guess = getattr(head_local.base.get_input_embeddings(), \"num_embeddings\", 1024)\n",
    "    dummy = torch.randint(0, min(1024, vocab_guess), (B, T), device=device)\n",
    "    logits, gates, aux = head_local.forward_with_metrics(dummy)\n",
    "    assert logits.shape[:2] == (B, T)\n",
    "    assert \"per_block\" in aux and \"blocks\" in aux\n",
    "    print(\"  ok.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:34:56.675877800Z",
     "start_time": "2025-08-23T08:34:56.646878700Z"
    }
   },
   "id": "fd2bf17327a31ee6"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smoke] head forward_with_metrics (mem_experts=1)\n",
      "  ok.\n",
      "[smoke] head forward_with_metrics (mem_experts=2)\n",
      "  ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "smoke_head_mem_experts_one_step()\n",
    "smoke_head_mem_experts_two_step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:34:57.343917900Z",
     "start_time": "2025-08-23T08:34:57.197914300Z"
    }
   },
   "id": "66b8d0ce3dcbd0e0"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla ln1 dtype: torch.float32\n",
      "gate weight dtype: torch.float32\n",
      "fusion ln dtype: n/a\n"
     ]
    }
   ],
   "source": [
    "blk0 = next(iter(head.blocks))\n",
    "print(\"vanilla ln1 dtype:\", blk0.vanilla.ln1.weight.dtype)\n",
    "print(\"gate weight dtype:\", blk0.gate.weight.dtype)\n",
    "print(\"fusion ln dtype:\", getattr(blk0, \"fuse_ln\", nn.LayerNorm(1)).weight.dtype if getattr(blk0,\"fusion_enable\",False) else \"n/a\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:35.094828900Z",
     "start_time": "2025-08-23T07:47:35.061614200Z"
    }
   },
   "id": "5b4b9ec6a3f5e415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Experiments - Stage 1 - basic architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5c8e4aa3034c4a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.1 experiment launch helpers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c75893af2f2aafd2"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# run set parameters\n",
    "EXP_STEPS   = 500       # try 1000–5000 for longer curves\n",
    "EXP_WARMUP  = 10       # keep ~steps/20\n",
    "BASE_MIX    = (0.4, 0.2, 0.2, 0.2)  # (hf, copy, repeat, nback)\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_isolate_test(label,\n",
    "            mixture_weights=BASE_MIX,\n",
    "            gate_reg_lambda=None,\n",
    "            gate_temp=None,\n",
    "            force_g=None,\n",
    "            steps=EXP_STEPS,\n",
    "            warmup=EXP_WARMUP):\n",
    "    \"\"\"Run a single train_experiment experiment with explicit knobs.\"\"\"\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    # set experiment-specific knobs (others come from CFG defaults)\n",
    "    if gate_reg_lambda is not None: set_cfg(gate_reg_lambda=float(gate_reg_lambda))\n",
    "    if gate_temp is not None:       set_cfg(gate_temp=float(gate_temp))\n",
    "    set_cfg(force_g=force_g)  # may be None, 0.0, or 1.0\n",
    "\n",
    "    print(f\"mixture={mixture_weights}, gate_reg_lambda={getattr(CFG,'gate_reg_lambda', 0.0)}, \"\n",
    "          f\"gate_temp={getattr(CFG,'gate_temp', 1.0)}, force_g={getattr(CFG,'force_g', None)}\")\n",
    "\n",
    "    # clean slate for VRAM and allocator fragmentation between runs\n",
    "    free_head_and_cache()\n",
    "    cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # run\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=warmup,\n",
    "        mixture_weights=mixture_weights,\n",
    "        viz_memory_after=False,   # keep quick; use visualize_memory_tb() ad-hoc\n",
    "    )\n",
    "\n",
    "    # post-run snapshot + cleanup\n",
    "    cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "def run_one_labeled(label, steps, mixture_weights, seed=1234,\n",
    "                    mixture_schedule=None, gate_temp_schedule=None, gate_reg_schedule=None,\n",
    "                    post_haystack=False):\n",
    "    print(f\"\\n=== {label} | seed={seed} ===\")\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed) \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_cfg(force_g=None)  # ensure no ablation\n",
    "    # unique TB run for each experiment label\n",
    "    start_tb_run(label)\n",
    "\n",
    "    # echo run metadata\n",
    "    if TB_AVAILABLE and 'tb' in globals():\n",
    "        import json\n",
    "        tb.add_text(\"run/meta\", json.dumps({\n",
    "            \"label\": label,\n",
    "            \"steps\": steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    \n",
    "    # print config stub\n",
    "    print(\"CFG.gate_temp:\", getattr(CFG, \"gate_temp\", 1.0),\n",
    "          \"| CFG.gate_reg_lambda:\", getattr(CFG, \"gate_reg_lambda\", 0.0),\n",
    "          \"| mixture:\", mixture_weights)\n",
    "\n",
    "    free_head_and_cache()\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}\")\n",
    "\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=max(10, steps//20),\n",
    "        mixture_weights=mixture_weights,\n",
    "        mixture_schedule=mixture_schedule,\n",
    "        gate_temp_schedule=gate_temp_schedule,\n",
    "        gate_reg_schedule=gate_reg_schedule,\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    if post_haystack:\n",
    "        evaluate_haystack(head, steps=50, batch=16, T=256, vocab=1024, tb_step=steps, fast=True)\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T04:59:42.315519800Z",
     "start_time": "2025-08-24T04:59:42.292518900Z"
    }
   },
   "id": "52417a96927364f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.2 Experiment Set 1 - Medium training sweep, testing parameter / dataset variants\n",
    " - E0: Baseline\n",
    " - E1: memory/algorithmic slanted data mix\n",
    " - E2: gate regularizer (low)\n",
    " - E3: gate regularizer (high)\n",
    " - E4: sharper routing via lower gate temperature\n",
    " - E5: ablations: disable/force DNC path\n",
    "    - 5a: disable DNC path (DNC path disabled)\n",
    "    - 5b: force DNC path (transformer path disabled) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81cbb7a830e97c75"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E0_baseline ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E0_baseline] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "978f5412cfc4409d8fe23562b6b085d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 8.7638 | lr 2.00e-04 | gates=[0.283203125, 0.28125] | mix=repeat\n",
      "step 20 | loss 8.1121 | lr 2.00e-04 | gates=[0.283203125, 0.28515625] | mix=nback\n",
      "step 30 | loss 4.2501 | lr 1.99e-04 | gates=[0.26171875, 0.26953125] | mix=hf\n",
      "step 40 | loss 4.6715 | lr 1.98e-04 | gates=[0.259765625, 0.26953125] | mix=hf\n",
      "step 50 | loss 4.4469 | lr 1.97e-04 | gates=[0.25390625, 0.263671875] | mix=hf\n",
      "step 60 | loss 7.0320 | lr 1.95e-04 | gates=[0.287109375, 0.310546875] | mix=nback\n",
      "step 70 | loss 5.7877 | lr 1.92e-04 | gates=[0.28515625, 0.314453125] | mix=nback\n",
      "step 80 | loss 1.9082 | lr 1.90e-04 | gates=[0.2353515625, 0.248046875] | mix=hf\n",
      "step 90 | loss 1.8775 | lr 1.87e-04 | gates=[0.2294921875, 0.2412109375] | mix=hf\n",
      "step 100 | loss 6.4503 | lr 1.83e-04 | gates=[0.291015625, 0.3203125] | mix=repeat\n",
      "step 110 | loss 5.8166 | lr 1.80e-04 | gates=[0.291015625, 0.326171875] | mix=copy\n",
      "step 120 | loss 1.3207 | lr 1.76e-04 | gates=[0.2197265625, 0.2412109375] | mix=hf\n",
      "step 130 | loss 5.6160 | lr 1.71e-04 | gates=[0.296875, 0.333984375] | mix=copy\n",
      "step 140 | loss 1.8407 | lr 1.67e-04 | gates=[0.201171875, 0.220703125] | mix=hf\n",
      "step 150 | loss 5.2244 | lr 1.62e-04 | gates=[0.28515625, 0.33203125] | mix=nback\n",
      "step 160 | loss 5.0245 | lr 1.57e-04 | gates=[0.283203125, 0.333984375] | mix=nback\n",
      "step 170 | loss 1.6287 | lr 1.51e-04 | gates=[0.1875, 0.2080078125] | mix=hf\n",
      "step 180 | loss 5.7207 | lr 1.46e-04 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 190 | loss 1.8050 | lr 1.40e-04 | gates=[0.177734375, 0.1982421875] | mix=hf\n",
      "step 200 | loss 1.4193 | lr 1.34e-04 | gates=[0.177734375, 0.2001953125] | mix=hf\n",
      "step 210 | loss 5.5786 | lr 1.28e-04 | gates=[0.294921875, 0.345703125] | mix=copy\n",
      "step 220 | loss 1.3877 | lr 1.22e-04 | gates=[0.1728515625, 0.1962890625] | mix=hf\n",
      "step 230 | loss 1.4895 | lr 1.15e-04 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 240 | loss 1.5816 | lr 1.09e-04 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 250 | loss 5.0480 | lr 1.03e-04 | gates=[0.28515625, 0.34375] | mix=nback\n",
      "step 260 | loss 4.8854 | lr 9.62e-05 | gates=[0.28515625, 0.345703125] | mix=nback\n",
      "step 270 | loss 5.4913 | lr 8.98e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 280 | loss 4.7768 | lr 8.34e-05 | gates=[0.3046875, 0.353515625] | mix=repeat\n",
      "step 290 | loss 5.4612 | lr 7.71e-05 | gates=[0.294921875, 0.3515625] | mix=copy\n",
      "step 300 | loss 2.4750 | lr 7.09e-05 | gates=[0.1708984375, 0.203125] | mix=hf\n",
      "step 310 | loss 5.4431 | lr 6.49e-05 | gates=[0.302734375, 0.353515625] | mix=copy\n",
      "step 320 | loss 1.3132 | lr 5.89e-05 | gates=[0.15234375, 0.1767578125] | mix=hf\n",
      "step 330 | loss 1.4544 | lr 5.32e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 340 | loss 5.4393 | lr 4.76e-05 | gates=[0.3046875, 0.35546875] | mix=copy\n",
      "step 350 | loss 5.4470 | lr 4.23e-05 | gates=[0.30078125, 0.353515625] | mix=copy\n",
      "step 360 | loss 1.3796 | lr 3.72e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 370 | loss 4.8311 | lr 3.23e-05 | gates=[0.306640625, 0.357421875] | mix=repeat\n",
      "step 380 | loss 1.3429 | lr 2.77e-05 | gates=[0.1591796875, 0.1845703125] | mix=hf\n",
      "step 390 | loss 5.4305 | lr 2.34e-05 | gates=[0.306640625, 0.359375] | mix=copy\n",
      "step 400 | loss 4.9858 | lr 2.00e-05 | gates=[0.28515625, 0.349609375] | mix=nback\n",
      "step 410 | loss 4.8875 | lr 2.00e-05 | gates=[0.30078125, 0.3515625] | mix=repeat\n",
      "step 420 | loss 1.2746 | lr 2.00e-05 | gates=[0.154296875, 0.1796875] | mix=hf\n",
      "step 430 | loss 1.5069 | lr 2.00e-05 | gates=[0.1591796875, 0.1884765625] | mix=hf\n",
      "step 440 | loss 4.8447 | lr 2.00e-05 | gates=[0.2890625, 0.353515625] | mix=nback\n",
      "step 450 | loss 1.7236 | lr 2.00e-05 | gates=[0.1552734375, 0.1826171875] | mix=hf\n",
      "step 460 | loss 1.2047 | lr 2.00e-05 | gates=[0.15234375, 0.1748046875] | mix=hf\n",
      "step 470 | loss 1.3864 | lr 2.00e-05 | gates=[0.1533203125, 0.177734375] | mix=hf\n",
      "step 480 | loss 4.8177 | lr 2.00e-05 | gates=[0.2890625, 0.353515625] | mix=nback\n",
      "step 490 | loss 1.3794 | lr 2.00e-05 | gates=[0.1484375, 0.1708984375] | mix=hf\n",
      "step 500 | loss 4.9159 | lr 2.00e-05 | gates=[0.291015625, 0.35546875] | mix=nback\n",
      "[after  E0_baseline] alloc=19.56 GB | reserved=39.42 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E1_memory_lean ===\n",
      "mixture=(0.4, 0.3, 0.2, 0.1), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E1_memory_lean] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f12008c68214fbdafca7ff355e3e93d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.6762 | lr 2.00e-04 | gates=[0.27734375, 0.275390625] | mix=hf\n",
      "step 20 | loss 7.2456 | lr 2.00e-04 | gates=[0.265625, 0.267578125] | mix=hf\n",
      "step 30 | loss 2.8066 | lr 1.99e-04 | gates=[0.259765625, 0.26171875] | mix=hf\n",
      "step 40 | loss 5.9084 | lr 1.98e-04 | gates=[0.28515625, 0.294921875] | mix=copy\n",
      "step 50 | loss 6.2994 | lr 1.97e-04 | gates=[0.283203125, 0.296875] | mix=nback\n",
      "step 60 | loss 2.0780 | lr 1.95e-04 | gates=[0.240234375, 0.251953125] | mix=hf\n",
      "step 70 | loss 5.9646 | lr 1.92e-04 | gates=[0.2890625, 0.302734375] | mix=repeat\n",
      "step 80 | loss 6.1615 | lr 1.90e-04 | gates=[0.279296875, 0.30078125] | mix=nback\n",
      "step 90 | loss 1.6371 | lr 1.87e-04 | gates=[0.2255859375, 0.2412109375] | mix=hf\n",
      "step 100 | loss 5.8504 | lr 1.83e-04 | gates=[0.287109375, 0.30859375] | mix=copy\n",
      "step 110 | loss 6.1592 | lr 1.80e-04 | gates=[0.287109375, 0.310546875] | mix=copy\n",
      "step 120 | loss 1.6537 | lr 1.76e-04 | gates=[0.2109375, 0.228515625] | mix=hf\n",
      "step 130 | loss 1.3438 | lr 1.71e-04 | gates=[0.2060546875, 0.2255859375] | mix=hf\n",
      "step 140 | loss 1.6747 | lr 1.67e-04 | gates=[0.197265625, 0.216796875] | mix=hf\n",
      "step 150 | loss 7.8639 | lr 1.62e-04 | gates=[0.2890625, 0.3125] | mix=repeat\n",
      "step 160 | loss 7.0060 | lr 1.57e-04 | gates=[0.2890625, 0.318359375] | mix=copy\n",
      "step 170 | loss 1.2355 | lr 1.51e-04 | gates=[0.1904296875, 0.212890625] | mix=hf\n",
      "step 180 | loss 1.5434 | lr 1.46e-04 | gates=[0.181640625, 0.2041015625] | mix=hf\n",
      "step 190 | loss 1.3073 | lr 1.40e-04 | gates=[0.17578125, 0.19921875] | mix=hf\n",
      "step 200 | loss 1.5339 | lr 1.34e-04 | gates=[0.1748046875, 0.19921875] | mix=hf\n",
      "step 210 | loss 1.3171 | lr 1.28e-04 | gates=[0.169921875, 0.193359375] | mix=hf\n",
      "step 220 | loss 5.4045 | lr 1.22e-04 | gates=[0.271484375, 0.318359375] | mix=nback\n",
      "step 230 | loss 1.7062 | lr 1.15e-04 | gates=[0.162109375, 0.1875] | mix=hf\n",
      "step 240 | loss 4.8178 | lr 1.09e-04 | gates=[0.29296875, 0.328125] | mix=repeat\n",
      "step 250 | loss 5.0184 | lr 1.03e-04 | gates=[0.306640625, 0.337890625] | mix=repeat\n",
      "step 260 | loss 5.1333 | lr 9.62e-05 | gates=[0.30078125, 0.333984375] | mix=repeat\n",
      "step 270 | loss 1.5841 | lr 8.98e-05 | gates=[0.16015625, 0.1865234375] | mix=hf\n",
      "step 280 | loss 5.4775 | lr 8.34e-05 | gates=[0.29296875, 0.333984375] | mix=copy\n",
      "step 290 | loss 4.8038 | lr 7.71e-05 | gates=[0.29296875, 0.3359375] | mix=repeat\n",
      "step 300 | loss 5.5080 | lr 7.09e-05 | gates=[0.296875, 0.337890625] | mix=copy\n",
      "step 310 | loss 5.8610 | lr 6.49e-05 | gates=[0.2890625, 0.3359375] | mix=copy\n",
      "step 320 | loss 5.3585 | lr 5.89e-05 | gates=[0.294921875, 0.3359375] | mix=repeat\n",
      "step 330 | loss 0.9718 | lr 5.32e-05 | gates=[0.1591796875, 0.1865234375] | mix=hf\n",
      "step 340 | loss 5.4318 | lr 4.76e-05 | gates=[0.298828125, 0.33984375] | mix=copy\n",
      "step 350 | loss 4.9012 | lr 4.23e-05 | gates=[0.275390625, 0.328125] | mix=nback\n",
      "step 360 | loss 1.0814 | lr 3.72e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 370 | loss 5.0328 | lr 3.23e-05 | gates=[0.28125, 0.33203125] | mix=nback\n",
      "step 380 | loss 4.9236 | lr 2.77e-05 | gates=[0.3046875, 0.341796875] | mix=repeat\n",
      "step 390 | loss 4.7060 | lr 2.34e-05 | gates=[0.302734375, 0.33984375] | mix=repeat\n",
      "step 400 | loss 5.4102 | lr 2.00e-05 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 410 | loss 0.8208 | lr 2.00e-05 | gates=[0.1630859375, 0.1943359375] | mix=hf\n",
      "step 420 | loss 1.2078 | lr 2.00e-05 | gates=[0.1640625, 0.193359375] | mix=hf\n",
      "step 430 | loss 5.4661 | lr 2.00e-05 | gates=[0.29296875, 0.337890625] | mix=copy\n",
      "step 440 | loss 5.0707 | lr 2.00e-05 | gates=[0.306640625, 0.341796875] | mix=repeat\n",
      "step 450 | loss 4.6631 | lr 2.00e-05 | gates=[0.296875, 0.3359375] | mix=repeat\n",
      "step 460 | loss 1.4681 | lr 2.00e-05 | gates=[0.150390625, 0.1748046875] | mix=hf\n",
      "step 470 | loss 1.2772 | lr 2.00e-05 | gates=[0.150390625, 0.173828125] | mix=hf\n",
      "step 480 | loss 1.6525 | lr 2.00e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 490 | loss 5.3703 | lr 2.00e-05 | gates=[0.302734375, 0.34375] | mix=copy\n",
      "step 500 | loss 4.6788 | lr 2.00e-05 | gates=[0.310546875, 0.345703125] | mix=repeat\n",
      "[after  E1_memory_lean] alloc=19.56 GB | reserved=39.73 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E2_gate_reg_low ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0002, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E2_gate_reg_low] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4a8fb8f5e0d481b8372171b7a548ef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.5047 | lr 2.00e-04 | gates=[0.275390625, 0.279296875] | mix=hf\n",
      "step 20 | loss 2.1480 | lr 2.00e-04 | gates=[0.263671875, 0.26953125] | mix=hf\n",
      "step 30 | loss 7.5692 | lr 1.99e-04 | gates=[0.279296875, 0.287109375] | mix=nback\n",
      "step 40 | loss 1.5048 | lr 1.98e-04 | gates=[0.244140625, 0.25390625] | mix=hf\n",
      "step 50 | loss 6.6892 | lr 1.97e-04 | gates=[0.279296875, 0.296875] | mix=copy\n",
      "step 60 | loss 5.5064 | lr 1.95e-04 | gates=[0.287109375, 0.3046875] | mix=repeat\n",
      "step 70 | loss 10.6165 | lr 1.92e-04 | gates=[0.28515625, 0.302734375] | mix=repeat\n",
      "step 80 | loss 6.2255 | lr 1.90e-04 | gates=[0.279296875, 0.302734375] | mix=repeat\n",
      "step 90 | loss 1.4385 | lr 1.87e-04 | gates=[0.2080078125, 0.2255859375] | mix=hf\n",
      "step 100 | loss 1.6988 | lr 1.83e-04 | gates=[0.20703125, 0.2265625] | mix=hf\n",
      "step 110 | loss 1.4504 | lr 1.80e-04 | gates=[0.2021484375, 0.2216796875] | mix=hf\n",
      "step 120 | loss 6.7830 | lr 1.76e-04 | gates=[0.271484375, 0.314453125] | mix=nback\n",
      "step 130 | loss 1.6999 | lr 1.71e-04 | gates=[0.1962890625, 0.21875] | mix=hf\n",
      "step 140 | loss 5.7271 | lr 1.67e-04 | gates=[0.271484375, 0.318359375] | mix=nback\n",
      "step 150 | loss 5.8592 | lr 1.62e-04 | gates=[0.28515625, 0.32421875] | mix=repeat\n",
      "step 160 | loss 1.7528 | lr 1.57e-04 | gates=[0.1845703125, 0.208984375] | mix=hf\n",
      "step 170 | loss 5.2232 | lr 1.51e-04 | gates=[0.28515625, 0.328125] | mix=repeat\n",
      "step 180 | loss 1.5497 | lr 1.46e-04 | gates=[0.18359375, 0.2109375] | mix=hf\n",
      "step 190 | loss 5.1735 | lr 1.40e-04 | gates=[0.2890625, 0.33203125] | mix=repeat\n",
      "step 200 | loss 1.2247 | lr 1.34e-04 | gates=[0.181640625, 0.208984375] | mix=hf\n",
      "step 210 | loss 5.1795 | lr 1.28e-04 | gates=[0.279296875, 0.3359375] | mix=nback\n",
      "step 220 | loss 1.7806 | lr 1.22e-04 | gates=[0.177734375, 0.20703125] | mix=hf\n",
      "step 230 | loss 1.4641 | lr 1.15e-04 | gates=[0.1728515625, 0.2001953125] | mix=hf\n",
      "step 240 | loss 5.2401 | lr 1.09e-04 | gates=[0.271484375, 0.333984375] | mix=nback\n",
      "step 250 | loss 4.8972 | lr 1.03e-04 | gates=[0.287109375, 0.33984375] | mix=repeat\n",
      "step 260 | loss 1.4546 | lr 9.62e-05 | gates=[0.171875, 0.19921875] | mix=hf\n",
      "step 270 | loss 5.1950 | lr 8.98e-05 | gates=[0.271484375, 0.3359375] | mix=nback\n",
      "step 280 | loss 1.6311 | lr 8.34e-05 | gates=[0.1650390625, 0.1904296875] | mix=hf\n",
      "step 290 | loss 5.2266 | lr 7.71e-05 | gates=[0.271484375, 0.3359375] | mix=nback\n",
      "step 300 | loss 1.4167 | lr 7.09e-05 | gates=[0.1748046875, 0.2021484375] | mix=hf\n",
      "step 310 | loss 1.2757 | lr 6.49e-05 | gates=[0.1640625, 0.1884765625] | mix=hf\n",
      "step 320 | loss 1.4606 | lr 5.89e-05 | gates=[0.16015625, 0.18359375] | mix=hf\n",
      "step 330 | loss 5.0209 | lr 5.32e-05 | gates=[0.298828125, 0.349609375] | mix=repeat\n",
      "step 340 | loss 1.3760 | lr 4.76e-05 | gates=[0.162109375, 0.1865234375] | mix=hf\n",
      "step 350 | loss 4.9610 | lr 4.23e-05 | gates=[0.2890625, 0.345703125] | mix=repeat\n",
      "step 360 | loss 4.8731 | lr 3.72e-05 | gates=[0.29296875, 0.34765625] | mix=repeat\n",
      "step 370 | loss 4.8434 | lr 3.23e-05 | gates=[0.29296875, 0.34765625] | mix=repeat\n",
      "step 380 | loss 4.8694 | lr 2.77e-05 | gates=[0.271484375, 0.33984375] | mix=nback\n",
      "step 390 | loss 5.4633 | lr 2.34e-05 | gates=[0.2890625, 0.34765625] | mix=copy\n",
      "step 400 | loss 5.4245 | lr 2.00e-05 | gates=[0.294921875, 0.3515625] | mix=copy\n",
      "step 410 | loss 4.8709 | lr 2.00e-05 | gates=[0.27734375, 0.34375] | mix=nback\n",
      "step 420 | loss 2.0610 | lr 2.00e-05 | gates=[0.15625, 0.18359375] | mix=hf\n",
      "step 430 | loss 1.3637 | lr 2.00e-05 | gates=[0.1611328125, 0.185546875] | mix=hf\n",
      "step 440 | loss 5.3997 | lr 2.00e-05 | gates=[0.29296875, 0.3515625] | mix=copy\n",
      "step 450 | loss 5.4484 | lr 2.00e-05 | gates=[0.291015625, 0.349609375] | mix=copy\n",
      "step 460 | loss 1.6328 | lr 2.00e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 470 | loss 4.9579 | lr 2.00e-05 | gates=[0.28125, 0.34765625] | mix=nback\n",
      "step 480 | loss 1.3012 | lr 2.00e-05 | gates=[0.1640625, 0.193359375] | mix=hf\n",
      "step 490 | loss 1.5084 | lr 2.00e-05 | gates=[0.1552734375, 0.181640625] | mix=hf\n",
      "step 500 | loss 5.4518 | lr 2.00e-05 | gates=[0.28515625, 0.34765625] | mix=copy\n",
      "[after  E2_gate_reg_low] alloc=19.56 GB | reserved=42.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E3_gate_reg_high ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0005, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E3_gate_reg_high] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf9fa93631f4eb2ba11472793c1e7a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 8.4827 | lr 2.00e-04 | gates=[0.28515625, 0.283203125] | mix=repeat\n",
      "step 20 | loss 7.1305 | lr 2.00e-04 | gates=[0.287109375, 0.28515625] | mix=repeat\n",
      "step 30 | loss 2.0686 | lr 1.99e-04 | gates=[0.26171875, 0.267578125] | mix=hf\n",
      "step 40 | loss 3.8306 | lr 1.98e-04 | gates=[0.248046875, 0.255859375] | mix=hf\n",
      "step 50 | loss 7.1654 | lr 1.97e-04 | gates=[0.28515625, 0.294921875] | mix=copy\n",
      "step 60 | loss 7.3936 | lr 1.95e-04 | gates=[0.28125, 0.296875] | mix=nback\n",
      "step 70 | loss 6.3211 | lr 1.92e-04 | gates=[0.283203125, 0.302734375] | mix=nback\n",
      "step 80 | loss 5.5104 | lr 1.90e-04 | gates=[0.29296875, 0.310546875] | mix=repeat\n",
      "step 90 | loss 5.6180 | lr 1.87e-04 | gates=[0.28515625, 0.3125] | mix=copy\n",
      "step 100 | loss 5.1885 | lr 1.83e-04 | gates=[0.29296875, 0.31640625] | mix=repeat\n",
      "step 110 | loss 5.5819 | lr 1.80e-04 | gates=[0.283203125, 0.314453125] | mix=nback\n",
      "step 120 | loss 1.4206 | lr 1.76e-04 | gates=[0.216796875, 0.2353515625] | mix=hf\n",
      "step 130 | loss 6.0211 | lr 1.71e-04 | gates=[0.283203125, 0.31640625] | mix=nback\n",
      "step 140 | loss 5.0925 | lr 1.67e-04 | gates=[0.294921875, 0.326171875] | mix=repeat\n",
      "step 150 | loss 5.0160 | lr 1.62e-04 | gates=[0.296875, 0.328125] | mix=repeat\n",
      "step 160 | loss 2.2180 | lr 1.57e-04 | gates=[0.203125, 0.2255859375] | mix=hf\n",
      "step 170 | loss 1.7853 | lr 1.51e-04 | gates=[0.2021484375, 0.2265625] | mix=hf\n",
      "step 180 | loss 1.7626 | lr 1.46e-04 | gates=[0.19921875, 0.22265625] | mix=hf\n",
      "step 190 | loss 5.0773 | lr 1.40e-04 | gates=[0.28515625, 0.326171875] | mix=nback\n",
      "step 200 | loss 5.1497 | lr 1.34e-04 | gates=[0.296875, 0.33203125] | mix=repeat\n",
      "step 210 | loss 5.9953 | lr 1.28e-04 | gates=[0.283203125, 0.326171875] | mix=nback\n",
      "step 220 | loss 1.6030 | lr 1.22e-04 | gates=[0.1884765625, 0.2109375] | mix=hf\n",
      "step 230 | loss 5.5216 | lr 1.15e-04 | gates=[0.2890625, 0.33203125] | mix=copy\n",
      "step 240 | loss 4.9902 | lr 1.09e-04 | gates=[0.30078125, 0.337890625] | mix=repeat\n",
      "step 250 | loss 5.0173 | lr 1.03e-04 | gates=[0.298828125, 0.3359375] | mix=repeat\n",
      "step 260 | loss 5.5651 | lr 9.62e-05 | gates=[0.28125, 0.328125] | mix=nback\n",
      "step 270 | loss 1.4995 | lr 8.98e-05 | gates=[0.1748046875, 0.197265625] | mix=hf\n",
      "step 280 | loss 4.9527 | lr 8.34e-05 | gates=[0.3046875, 0.337890625] | mix=repeat\n",
      "step 290 | loss 5.5485 | lr 7.71e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 300 | loss 5.0064 | lr 7.09e-05 | gates=[0.30859375, 0.34375] | mix=repeat\n",
      "step 310 | loss 5.5153 | lr 6.49e-05 | gates=[0.28125, 0.33203125] | mix=nback\n",
      "step 320 | loss 6.0751 | lr 5.89e-05 | gates=[0.296875, 0.337890625] | mix=copy\n",
      "step 330 | loss 1.1022 | lr 5.32e-05 | gates=[0.169921875, 0.1953125] | mix=hf\n",
      "step 340 | loss 5.5156 | lr 4.76e-05 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 350 | loss 4.7582 | lr 4.23e-05 | gates=[0.294921875, 0.3359375] | mix=repeat\n",
      "step 360 | loss 1.3964 | lr 3.72e-05 | gates=[0.1728515625, 0.1982421875] | mix=hf\n",
      "step 370 | loss 1.3397 | lr 3.23e-05 | gates=[0.169921875, 0.1962890625] | mix=hf\n",
      "step 380 | loss 0.8502 | lr 2.77e-05 | gates=[0.1767578125, 0.20703125] | mix=hf\n",
      "step 390 | loss 4.9189 | lr 2.34e-05 | gates=[0.28515625, 0.3359375] | mix=nback\n",
      "step 400 | loss 1.3257 | lr 2.00e-05 | gates=[0.173828125, 0.2001953125] | mix=hf\n",
      "step 410 | loss 1.2752 | lr 2.00e-05 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 420 | loss 4.6597 | lr 2.00e-05 | gates=[0.30078125, 0.341796875] | mix=repeat\n",
      "step 430 | loss 4.8766 | lr 2.00e-05 | gates=[0.30078125, 0.341796875] | mix=repeat\n",
      "step 440 | loss 4.8770 | lr 2.00e-05 | gates=[0.283203125, 0.3359375] | mix=nback\n",
      "step 450 | loss 5.4217 | lr 2.00e-05 | gates=[0.2890625, 0.337890625] | mix=copy\n",
      "step 460 | loss 5.4015 | lr 2.00e-05 | gates=[0.298828125, 0.34375] | mix=copy\n",
      "step 470 | loss 1.5290 | lr 2.00e-05 | gates=[0.166015625, 0.19140625] | mix=hf\n",
      "step 480 | loss 4.8769 | lr 2.00e-05 | gates=[0.283203125, 0.3359375] | mix=nback\n",
      "step 490 | loss 5.3786 | lr 2.00e-05 | gates=[0.298828125, 0.345703125] | mix=copy\n",
      "step 500 | loss 5.4438 | lr 2.00e-05 | gates=[0.298828125, 0.34375] | mix=copy\n",
      "[after  E3_gate_reg_high] alloc=19.56 GB | reserved=42.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E4_gate_temp_0p7 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=0.7, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E4_gate_temp_0p7] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68333fd92e164c9198ccb40f42d54687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.7747 | lr 2.00e-04 | gates=[0.224609375, 0.224609375] | mix=repeat\n",
      "step 20 | loss 10.0938 | lr 2.00e-04 | gates=[0.2294921875, 0.232421875] | mix=copy\n",
      "step 30 | loss 9.4271 | lr 1.99e-04 | gates=[0.23046875, 0.23828125] | mix=nback\n",
      "step 40 | loss 7.5605 | lr 1.98e-04 | gates=[0.2294921875, 0.2412109375] | mix=nback\n",
      "step 50 | loss 7.0946 | lr 1.97e-04 | gates=[0.23046875, 0.2451171875] | mix=nback\n",
      "step 60 | loss 5.3907 | lr 1.95e-04 | gates=[0.236328125, 0.25390625] | mix=repeat\n",
      "step 70 | loss 8.6001 | lr 1.92e-04 | gates=[0.232421875, 0.255859375] | mix=copy\n",
      "step 80 | loss 6.2352 | lr 1.90e-04 | gates=[0.2275390625, 0.259765625] | mix=nback\n",
      "step 90 | loss 2.6570 | lr 1.87e-04 | gates=[0.166015625, 0.19140625] | mix=hf\n",
      "step 100 | loss 2.9589 | lr 1.83e-04 | gates=[0.1591796875, 0.185546875] | mix=hf\n",
      "step 110 | loss 5.4593 | lr 1.80e-04 | gates=[0.2353515625, 0.2734375] | mix=repeat\n",
      "step 120 | loss 5.5796 | lr 1.76e-04 | gates=[0.2275390625, 0.2734375] | mix=nback\n",
      "step 130 | loss 6.1719 | lr 1.71e-04 | gates=[0.236328125, 0.279296875] | mix=copy\n",
      "step 140 | loss 9.6652 | lr 1.67e-04 | gates=[0.2314453125, 0.27734375] | mix=copy\n",
      "step 150 | loss 5.2064 | lr 1.62e-04 | gates=[0.236328125, 0.27734375] | mix=repeat\n",
      "step 160 | loss 5.9392 | lr 1.57e-04 | gates=[0.232421875, 0.27734375] | mix=repeat\n",
      "step 170 | loss 5.7170 | lr 1.51e-04 | gates=[0.236328125, 0.28515625] | mix=copy\n",
      "step 180 | loss 1.5507 | lr 1.46e-04 | gates=[0.1337890625, 0.162109375] | mix=hf\n",
      "step 190 | loss 5.3480 | lr 1.40e-04 | gates=[0.2255859375, 0.28515625] | mix=nback\n",
      "step 200 | loss 5.6519 | lr 1.34e-04 | gates=[0.2333984375, 0.291015625] | mix=copy\n",
      "step 210 | loss 5.5102 | lr 1.28e-04 | gates=[0.234375, 0.291015625] | mix=copy\n",
      "step 220 | loss 5.6864 | lr 1.22e-04 | gates=[0.2451171875, 0.30078125] | mix=copy\n",
      "step 230 | loss 5.9367 | lr 1.15e-04 | gates=[0.244140625, 0.30078125] | mix=repeat\n",
      "step 240 | loss 5.1008 | lr 1.09e-04 | gates=[0.240234375, 0.298828125] | mix=repeat\n",
      "step 250 | loss 5.5583 | lr 1.03e-04 | gates=[0.23828125, 0.302734375] | mix=copy\n",
      "step 260 | loss 1.7395 | lr 9.62e-05 | gates=[0.12060546875, 0.15234375] | mix=hf\n",
      "step 270 | loss 1.3323 | lr 8.98e-05 | gates=[0.12109375, 0.1513671875] | mix=hf\n",
      "step 280 | loss 5.6282 | lr 8.34e-05 | gates=[0.23046875, 0.302734375] | mix=nback\n",
      "step 290 | loss 1.5458 | lr 7.71e-05 | gates=[0.12353515625, 0.1572265625] | mix=hf\n",
      "step 300 | loss 5.5118 | lr 7.09e-05 | gates=[0.2421875, 0.30859375] | mix=copy\n",
      "step 310 | loss 5.4353 | lr 6.49e-05 | gates=[0.2451171875, 0.310546875] | mix=copy\n",
      "step 320 | loss 1.3244 | lr 5.89e-05 | gates=[0.119140625, 0.1513671875] | mix=hf\n",
      "step 330 | loss 5.5029 | lr 5.32e-05 | gates=[0.251953125, 0.31640625] | mix=copy\n",
      "step 340 | loss 4.8675 | lr 4.76e-05 | gates=[0.251953125, 0.314453125] | mix=repeat\n",
      "step 350 | loss 5.4279 | lr 4.23e-05 | gates=[0.2392578125, 0.3125] | mix=copy\n",
      "step 360 | loss 1.3162 | lr 3.72e-05 | gates=[0.11328125, 0.14453125] | mix=hf\n",
      "step 370 | loss 4.7927 | lr 3.23e-05 | gates=[0.259765625, 0.3203125] | mix=repeat\n",
      "step 380 | loss 1.2108 | lr 2.77e-05 | gates=[0.11865234375, 0.1494140625] | mix=hf\n",
      "step 390 | loss 1.2477 | lr 2.34e-05 | gates=[0.1142578125, 0.1455078125] | mix=hf\n",
      "step 400 | loss 1.3056 | lr 2.00e-05 | gates=[0.1162109375, 0.1474609375] | mix=hf\n",
      "step 410 | loss 1.4811 | lr 2.00e-05 | gates=[0.12353515625, 0.158203125] | mix=hf\n",
      "step 420 | loss 5.3813 | lr 2.00e-05 | gates=[0.255859375, 0.3203125] | mix=copy\n",
      "step 430 | loss 5.4253 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=copy\n",
      "step 440 | loss 4.8336 | lr 2.00e-05 | gates=[0.234375, 0.3125] | mix=nback\n",
      "step 450 | loss 1.3101 | lr 2.00e-05 | gates=[0.11181640625, 0.1416015625] | mix=hf\n",
      "step 460 | loss 1.1295 | lr 2.00e-05 | gates=[0.1201171875, 0.1533203125] | mix=hf\n",
      "step 470 | loss 1.1692 | lr 2.00e-05 | gates=[0.1171875, 0.1474609375] | mix=hf\n",
      "step 480 | loss 1.3702 | lr 2.00e-05 | gates=[0.1083984375, 0.1376953125] | mix=hf\n",
      "step 490 | loss 4.6268 | lr 2.00e-05 | gates=[0.255859375, 0.3203125] | mix=repeat\n",
      "step 500 | loss 4.7667 | lr 2.00e-05 | gates=[0.2578125, 0.3203125] | mix=repeat\n",
      "[after  E4_gate_temp_0p7] alloc=19.56 GB | reserved=37.59 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E5a_force_g_0 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=0.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E5a_force_g_0] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "345a21a3d79c486e9a79cffc1ce3664b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.7708 | lr 2.00e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 20 | loss 8.1129 | lr 2.00e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 30 | loss 9.4176 | lr 1.99e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 40 | loss 2.0564 | lr 1.98e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 50 | loss 1.4602 | lr 1.97e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 60 | loss 6.0042 | lr 1.95e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 70 | loss 6.5107 | lr 1.92e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 80 | loss 1.6592 | lr 1.90e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 90 | loss 6.2884 | lr 1.87e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 100 | loss 6.1378 | lr 1.83e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 110 | loss 1.2196 | lr 1.80e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 120 | loss 5.9388 | lr 1.76e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 130 | loss 6.2737 | lr 1.71e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 140 | loss 1.4693 | lr 1.67e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 150 | loss 1.4359 | lr 1.62e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 160 | loss 1.6469 | lr 1.57e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 170 | loss 5.7269 | lr 1.51e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 180 | loss 1.5394 | lr 1.46e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 190 | loss 5.8034 | lr 1.40e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 200 | loss 6.5025 | lr 1.34e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 210 | loss 5.5158 | lr 1.28e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 220 | loss 1.1690 | lr 1.22e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 230 | loss 4.9982 | lr 1.15e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 240 | loss 5.2309 | lr 1.09e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 250 | loss 5.7406 | lr 1.03e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 260 | loss 1.3949 | lr 9.62e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 270 | loss 5.7502 | lr 8.98e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 280 | loss 5.8369 | lr 8.34e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 290 | loss 1.4449 | lr 7.71e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 300 | loss 1.4356 | lr 7.09e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 310 | loss 1.5654 | lr 6.49e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 320 | loss 5.0112 | lr 5.89e-05 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 330 | loss 1.4810 | lr 5.32e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 340 | loss 5.0976 | lr 4.76e-05 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 350 | loss 5.2713 | lr 4.23e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 360 | loss 1.2658 | lr 3.72e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 370 | loss 5.7405 | lr 3.23e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 380 | loss 5.0271 | lr 2.77e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 390 | loss 5.5519 | lr 2.34e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 400 | loss 1.0491 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 410 | loss 1.1208 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 420 | loss 5.5193 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 430 | loss 1.0712 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 440 | loss 5.1604 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 450 | loss 1.4760 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 460 | loss 1.4285 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 470 | loss 0.9634 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 480 | loss 1.4824 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 490 | loss 4.9023 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 500 | loss 1.1232 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "[after  E5a_force_g_0] alloc=19.56 GB | reserved=39.52 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E5b_force_g_1 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=1.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E5b_force_g_1] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d16d204e0dd5456ba7cc6bebe60afcd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.4470 | lr 2.00e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 20 | loss 8.4882 | lr 2.00e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 30 | loss 6.6057 | lr 1.99e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 40 | loss 6.8276 | lr 1.98e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 50 | loss 6.6664 | lr 1.97e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 60 | loss 6.3100 | lr 1.95e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 70 | loss 6.7212 | lr 1.92e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 80 | loss 9.0949 | lr 1.90e-04 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 90 | loss 5.5494 | lr 1.87e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 100 | loss 5.9994 | lr 1.83e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 110 | loss 5.6693 | lr 1.80e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 120 | loss 5.1639 | lr 1.76e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 130 | loss 5.7746 | lr 1.71e-04 | gates=[1.0, 1.0] | mix=copy\n",
      "step 140 | loss 4.8143 | lr 1.67e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 150 | loss 5.4356 | lr 1.62e-04 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 160 | loss 4.6245 | lr 1.57e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 170 | loss 4.8690 | lr 1.51e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 180 | loss 5.4418 | lr 1.46e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 190 | loss 4.0093 | lr 1.40e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 200 | loss 7.7539 | lr 1.34e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 210 | loss 4.1556 | lr 1.28e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 220 | loss 4.5632 | lr 1.22e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 230 | loss 4.9013 | lr 1.15e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 240 | loss 4.9961 | lr 1.09e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 250 | loss 3.8238 | lr 1.03e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 260 | loss 5.6437 | lr 9.62e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 270 | loss 3.8135 | lr 8.98e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 280 | loss 4.0806 | lr 8.34e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 290 | loss 5.2600 | lr 7.71e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 300 | loss 3.6085 | lr 7.09e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 310 | loss 3.5248 | lr 6.49e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 320 | loss 5.3987 | lr 5.89e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 330 | loss 3.3324 | lr 5.32e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 340 | loss 3.0953 | lr 4.76e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 350 | loss 3.5784 | lr 4.23e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 360 | loss 4.8400 | lr 3.72e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 370 | loss 3.5400 | lr 3.23e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 380 | loss 5.4159 | lr 2.77e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 390 | loss 5.1525 | lr 2.34e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 400 | loss 5.1219 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 410 | loss 4.8144 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 420 | loss 2.9476 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 430 | loss 3.6580 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 440 | loss 5.3956 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 450 | loss 5.1512 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 460 | loss 4.7878 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 470 | loss 5.0965 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 480 | loss 4.8268 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 490 | loss 3.2123 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 500 | loss 4.7564 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "[after  E5b_force_g_1] alloc=19.56 GB | reserved=38.97 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, torch, gc\n",
    "\n",
    "# === E0 baseline: identical to your sanity run ===\n",
    "run_isolate_test(\"E0_baseline\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E1 memory-leaning mix: more algorithmic exposure ===\n",
    "run_isolate_test(\"E1_memory_lean\",\n",
    "        mixture_weights=(0.4, 0.3, 0.2, 0.1),  # HF, copy, repeat, nback\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E2 gate regularizer (low) ===\n",
    "run_isolate_test(\"E2_gate_reg_low\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=2e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E3 gate regularizer (high) ===\n",
    "run_isolate_test(\"E3_gate_reg_high\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=5e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E4 sharper routing via lower gate temperature ===\n",
    "run_isolate_test(\"E4_gate_temp_0p7\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=0.7,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E5 ablations: disable/force DNC path ===\n",
    "run_isolate_test(\"E5a_force_g_0\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=0.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "run_isolate_test(\"E5b_force_g_1\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=1.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T21:32:12.458599800Z",
     "start_time": "2025-08-20T16:33:53.323296700Z"
    }
   },
   "id": "771c919035840bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T21:32:12.613598600Z",
     "start_time": "2025-08-20T21:32:12.457599100Z"
    }
   },
   "id": "f07fc074b58caaa0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.3 Experiment Set 2 - parameter sweeps, based on E2 params from set 1 above\n",
    " - E6: higher gate temp\n",
    " - E7: memory-leaning warm-start\n",
    " - E8: capacity sweep\n",
    " - E9: baseline training, haystack eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a269fb24f3b9075"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E6_temp0p8_seed1337 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250820-143212-E6_temp0p8_seed1337\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed1337] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ef4156bede44a0485abeae825940206"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.8139 | lr 8.80e-05 | gates=[0.2392578125, 0.2431640625] | mix=hf\n",
      "step 20 | loss 5.6580 | lr 1.68e-04 | gates=[0.2451171875, 0.25390625] | mix=nback\n",
      "step 30 | loss 6.5675 | lr 2.00e-04 | gates=[0.25, 0.2578125] | mix=copy\n",
      "step 40 | loss 3.2836 | lr 1.99e-04 | gates=[0.2158203125, 0.224609375] | mix=hf\n",
      "step 50 | loss 1.9842 | lr 1.99e-04 | gates=[0.2080078125, 0.21875] | mix=hf\n",
      "step 60 | loss 2.1618 | lr 1.97e-04 | gates=[0.201171875, 0.21484375] | mix=hf\n",
      "step 70 | loss 2.7611 | lr 1.95e-04 | gates=[0.1923828125, 0.2080078125] | mix=hf\n",
      "step 80 | loss 6.0227 | lr 1.93e-04 | gates=[0.25390625, 0.275390625] | mix=copy\n",
      "step 90 | loss 1.4240 | lr 1.91e-04 | gates=[0.1884765625, 0.2060546875] | mix=hf\n",
      "step 100 | loss 1.8305 | lr 1.88e-04 | gates=[0.177734375, 0.197265625] | mix=hf\n",
      "step 110 | loss 5.7472 | lr 1.84e-04 | gates=[0.2578125, 0.283203125] | mix=copy\n",
      "step 120 | loss 1.5439 | lr 1.81e-04 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 130 | loss 5.7418 | lr 1.76e-04 | gates=[0.2470703125, 0.283203125] | mix=nback\n",
      "step 140 | loss 5.3791 | lr 1.72e-04 | gates=[0.267578125, 0.294921875] | mix=repeat\n",
      "step 150 | loss 5.8621 | lr 1.67e-04 | gates=[0.25390625, 0.291015625] | mix=copy\n",
      "step 160 | loss 1.5878 | lr 1.62e-04 | gates=[0.1533203125, 0.1796875] | mix=hf\n",
      "step 170 | loss 5.5724 | lr 1.57e-04 | gates=[0.263671875, 0.294921875] | mix=repeat\n",
      "step 180 | loss 5.0408 | lr 1.51e-04 | gates=[0.26171875, 0.296875] | mix=repeat\n",
      "step 190 | loss 5.2424 | lr 1.46e-04 | gates=[0.24609375, 0.294921875] | mix=nback\n",
      "step 200 | loss 5.5308 | lr 1.40e-04 | gates=[0.2490234375, 0.296875] | mix=nback\n",
      "step 210 | loss 5.4577 | lr 1.33e-04 | gates=[0.26171875, 0.302734375] | mix=repeat\n",
      "step 220 | loss 5.9007 | lr 1.27e-04 | gates=[0.251953125, 0.3046875] | mix=nback\n",
      "step 230 | loss 5.6615 | lr 1.21e-04 | gates=[0.255859375, 0.306640625] | mix=copy\n",
      "step 240 | loss 1.4753 | lr 1.14e-04 | gates=[0.1396484375, 0.169921875] | mix=hf\n",
      "step 250 | loss 5.1542 | lr 1.08e-04 | gates=[0.2470703125, 0.3046875] | mix=nback\n",
      "step 260 | loss 5.3103 | lr 1.01e-04 | gates=[0.255859375, 0.310546875] | mix=nback\n",
      "step 270 | loss 5.5097 | lr 9.44e-05 | gates=[0.26953125, 0.31640625] | mix=copy\n",
      "step 280 | loss 5.7380 | lr 8.78e-05 | gates=[0.25, 0.310546875] | mix=nback\n",
      "step 290 | loss 5.4838 | lr 8.13e-05 | gates=[0.263671875, 0.3125] | mix=repeat\n",
      "step 300 | loss 7.8310 | lr 7.48e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 310 | loss 5.2254 | lr 6.85e-05 | gates=[0.263671875, 0.310546875] | mix=repeat\n",
      "step 320 | loss 5.4695 | lr 6.23e-05 | gates=[0.26953125, 0.318359375] | mix=copy\n",
      "step 330 | loss 5.5201 | lr 5.62e-05 | gates=[0.2734375, 0.3203125] | mix=copy\n",
      "step 340 | loss 5.3968 | lr 5.04e-05 | gates=[0.26171875, 0.318359375] | mix=copy\n",
      "step 350 | loss 5.1314 | lr 4.48e-05 | gates=[0.279296875, 0.322265625] | mix=repeat\n",
      "step 360 | loss 1.7178 | lr 3.94e-05 | gates=[0.12451171875, 0.1533203125] | mix=hf\n",
      "step 370 | loss 5.4563 | lr 3.42e-05 | gates=[0.263671875, 0.3203125] | mix=copy\n",
      "step 380 | loss 1.0962 | lr 2.94e-05 | gates=[0.12890625, 0.158203125] | mix=hf\n",
      "step 390 | loss 5.4683 | lr 2.49e-05 | gates=[0.27734375, 0.326171875] | mix=copy\n",
      "step 400 | loss 1.3760 | lr 2.07e-05 | gates=[0.1259765625, 0.1552734375] | mix=hf\n",
      "step 410 | loss 1.2889 | lr 2.00e-05 | gates=[0.125, 0.15234375] | mix=hf\n",
      "step 420 | loss 4.6192 | lr 2.00e-05 | gates=[0.2734375, 0.32421875] | mix=repeat\n",
      "step 430 | loss 1.4892 | lr 2.00e-05 | gates=[0.12451171875, 0.154296875] | mix=hf\n",
      "step 440 | loss 1.3414 | lr 2.00e-05 | gates=[0.12353515625, 0.1513671875] | mix=hf\n",
      "step 450 | loss 1.2564 | lr 2.00e-05 | gates=[0.126953125, 0.1552734375] | mix=hf\n",
      "step 460 | loss 1.5645 | lr 2.00e-05 | gates=[0.12353515625, 0.15234375] | mix=hf\n",
      "step 470 | loss 5.4328 | lr 2.00e-05 | gates=[0.275390625, 0.326171875] | mix=copy\n",
      "step 480 | loss 1.1220 | lr 2.00e-05 | gates=[0.1259765625, 0.1552734375] | mix=hf\n",
      "step 490 | loss 4.7706 | lr 2.00e-05 | gates=[0.2734375, 0.32421875] | mix=repeat\n",
      "step 500 | loss 5.4319 | lr 2.00e-05 | gates=[0.265625, 0.322265625] | mix=copy\n",
      "[after  E6_temp0p8_seed1337] alloc=29.34 GB | reserved=50.91 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed2027 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250820-151404-E6_temp0p8_seed2027\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed2027] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2104813c18934d6084925ca25a90acfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.7216 | lr 8.80e-05 | gates=[0.240234375, 0.2451171875] | mix=nback\n",
      "step 20 | loss 1.7473 | lr 1.68e-04 | gates=[0.2373046875, 0.2392578125] | mix=hf\n",
      "step 30 | loss 7.0766 | lr 2.00e-04 | gates=[0.2470703125, 0.255859375] | mix=repeat\n",
      "step 40 | loss 7.0362 | lr 1.99e-04 | gates=[0.25, 0.26171875] | mix=repeat\n",
      "step 50 | loss 1.9754 | lr 1.99e-04 | gates=[0.2119140625, 0.21875] | mix=hf\n",
      "step 60 | loss 6.4103 | lr 1.97e-04 | gates=[0.2470703125, 0.263671875] | mix=copy\n",
      "step 70 | loss 6.1247 | lr 1.95e-04 | gates=[0.25, 0.26953125] | mix=copy\n",
      "step 80 | loss 1.6452 | lr 1.93e-04 | gates=[0.1875, 0.2021484375] | mix=hf\n",
      "step 90 | loss 5.8153 | lr 1.91e-04 | gates=[0.2451171875, 0.26953125] | mix=repeat\n",
      "step 100 | loss 1.5125 | lr 1.88e-04 | gates=[0.1708984375, 0.189453125] | mix=hf\n",
      "step 110 | loss 5.8301 | lr 1.84e-04 | gates=[0.25390625, 0.279296875] | mix=repeat\n",
      "step 120 | loss 6.0745 | lr 1.81e-04 | gates=[0.2490234375, 0.279296875] | mix=repeat\n",
      "step 130 | loss 6.5550 | lr 1.76e-04 | gates=[0.2392578125, 0.283203125] | mix=nback\n",
      "step 140 | loss 6.5625 | lr 1.72e-04 | gates=[0.2490234375, 0.291015625] | mix=copy\n",
      "step 150 | loss 5.1010 | lr 1.67e-04 | gates=[0.25390625, 0.29296875] | mix=repeat\n",
      "step 160 | loss 6.3703 | lr 1.62e-04 | gates=[0.2470703125, 0.294921875] | mix=copy\n",
      "step 170 | loss 5.6672 | lr 1.57e-04 | gates=[0.2490234375, 0.294921875] | mix=copy\n",
      "step 180 | loss 6.0536 | lr 1.51e-04 | gates=[0.2578125, 0.3046875] | mix=copy\n",
      "step 190 | loss 6.1479 | lr 1.46e-04 | gates=[0.2578125, 0.3046875] | mix=repeat\n",
      "step 200 | loss 1.3962 | lr 1.40e-04 | gates=[0.1494140625, 0.1787109375] | mix=hf\n",
      "step 210 | loss 5.6449 | lr 1.33e-04 | gates=[0.259765625, 0.306640625] | mix=copy\n",
      "step 220 | loss 5.3002 | lr 1.27e-04 | gates=[0.2431640625, 0.30078125] | mix=nback\n",
      "step 230 | loss 1.3832 | lr 1.21e-04 | gates=[0.138671875, 0.166015625] | mix=hf\n",
      "step 240 | loss 5.1007 | lr 1.14e-04 | gates=[0.267578125, 0.3125] | mix=repeat\n",
      "step 250 | loss 1.7392 | lr 1.08e-04 | gates=[0.1337890625, 0.162109375] | mix=hf\n",
      "step 260 | loss 1.9869 | lr 1.01e-04 | gates=[0.1328125, 0.162109375] | mix=hf\n",
      "step 270 | loss 5.5443 | lr 9.44e-05 | gates=[0.267578125, 0.318359375] | mix=copy\n",
      "step 280 | loss 1.2283 | lr 8.78e-05 | gates=[0.13671875, 0.1650390625] | mix=hf\n",
      "step 290 | loss 1.4291 | lr 8.13e-05 | gates=[0.1328125, 0.1630859375] | mix=hf\n",
      "step 300 | loss 5.5580 | lr 7.48e-05 | gates=[0.2470703125, 0.3125] | mix=nback\n",
      "step 310 | loss 1.5050 | lr 6.85e-05 | gates=[0.130859375, 0.158203125] | mix=hf\n",
      "step 320 | loss 5.1237 | lr 6.23e-05 | gates=[0.2431640625, 0.310546875] | mix=nback\n",
      "step 330 | loss 5.1306 | lr 5.62e-05 | gates=[0.2470703125, 0.3125] | mix=nback\n",
      "step 340 | loss 4.8957 | lr 5.04e-05 | gates=[0.2578125, 0.3125] | mix=repeat\n",
      "step 350 | loss 1.4053 | lr 4.48e-05 | gates=[0.1259765625, 0.15234375] | mix=hf\n",
      "step 360 | loss 5.0109 | lr 3.94e-05 | gates=[0.25, 0.314453125] | mix=nback\n",
      "step 370 | loss 1.1974 | lr 3.42e-05 | gates=[0.1337890625, 0.16015625] | mix=hf\n",
      "step 380 | loss 1.4149 | lr 2.94e-05 | gates=[0.130859375, 0.1611328125] | mix=hf\n",
      "step 390 | loss 4.8075 | lr 2.49e-05 | gates=[0.25, 0.31640625] | mix=nback\n",
      "step 400 | loss 5.4349 | lr 2.07e-05 | gates=[0.25390625, 0.318359375] | mix=copy\n",
      "step 410 | loss 4.7952 | lr 2.00e-05 | gates=[0.2734375, 0.326171875] | mix=repeat\n",
      "step 420 | loss 1.1273 | lr 2.00e-05 | gates=[0.134765625, 0.162109375] | mix=hf\n",
      "step 430 | loss 4.9395 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=nback\n",
      "step 440 | loss 1.2522 | lr 2.00e-05 | gates=[0.126953125, 0.1552734375] | mix=hf\n",
      "step 450 | loss 5.4420 | lr 2.00e-05 | gates=[0.263671875, 0.322265625] | mix=copy\n",
      "step 460 | loss 5.4315 | lr 2.00e-05 | gates=[0.265625, 0.32421875] | mix=copy\n",
      "step 470 | loss 1.1288 | lr 2.00e-05 | gates=[0.1298828125, 0.1572265625] | mix=hf\n",
      "step 480 | loss 4.8761 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=nback\n",
      "step 490 | loss 5.4577 | lr 2.00e-05 | gates=[0.265625, 0.32421875] | mix=copy\n",
      "step 500 | loss 0.9094 | lr 2.00e-05 | gates=[0.13671875, 0.1689453125] | mix=hf\n",
      "[after  E6_temp0p8_seed2027] alloc=29.34 GB | reserved=49.33 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed4242 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250820-160508-E6_temp0p8_seed4242\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed4242] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f2117d39ac142059d34b3c28a1b7296"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.5768 | lr 8.80e-05 | gates=[0.248046875, 0.2470703125] | mix=repeat\n",
      "step 20 | loss 5.6260 | lr 1.68e-04 | gates=[0.251953125, 0.255859375] | mix=nback\n",
      "step 30 | loss 2.4026 | lr 2.00e-04 | gates=[0.2373046875, 0.240234375] | mix=hf\n",
      "step 40 | loss 7.2257 | lr 1.99e-04 | gates=[0.2578125, 0.263671875] | mix=repeat\n",
      "step 50 | loss 2.8536 | lr 1.99e-04 | gates=[0.2236328125, 0.228515625] | mix=hf\n",
      "step 60 | loss 6.7658 | lr 1.97e-04 | gates=[0.25390625, 0.271484375] | mix=nback\n",
      "step 70 | loss 6.8637 | lr 1.95e-04 | gates=[0.255859375, 0.275390625] | mix=copy\n",
      "step 80 | loss 6.1135 | lr 1.93e-04 | gates=[0.259765625, 0.279296875] | mix=copy\n",
      "step 90 | loss 1.7781 | lr 1.91e-04 | gates=[0.19921875, 0.2119140625] | mix=hf\n",
      "step 100 | loss 5.3427 | lr 1.88e-04 | gates=[0.259765625, 0.28125] | mix=repeat\n",
      "step 110 | loss 1.6011 | lr 1.84e-04 | gates=[0.185546875, 0.2041015625] | mix=hf\n",
      "step 120 | loss 1.5415 | lr 1.81e-04 | gates=[0.1845703125, 0.2060546875] | mix=hf\n",
      "step 130 | loss 1.5450 | lr 1.76e-04 | gates=[0.1845703125, 0.208984375] | mix=hf\n",
      "step 140 | loss 9.2277 | lr 1.72e-04 | gates=[0.259765625, 0.291015625] | mix=copy\n",
      "step 150 | loss 6.0319 | lr 1.67e-04 | gates=[0.26171875, 0.29296875] | mix=copy\n",
      "step 160 | loss 1.7149 | lr 1.62e-04 | gates=[0.1669921875, 0.189453125] | mix=hf\n",
      "step 170 | loss 5.7346 | lr 1.57e-04 | gates=[0.2578125, 0.29296875] | mix=copy\n",
      "step 180 | loss 1.4303 | lr 1.51e-04 | gates=[0.1650390625, 0.1875] | mix=hf\n",
      "step 190 | loss 5.6727 | lr 1.46e-04 | gates=[0.2451171875, 0.29296875] | mix=nback\n",
      "step 200 | loss 5.4288 | lr 1.40e-04 | gates=[0.2451171875, 0.29296875] | mix=nback\n",
      "step 210 | loss 5.1370 | lr 1.33e-04 | gates=[0.2451171875, 0.296875] | mix=nback\n",
      "step 220 | loss 1.1836 | lr 1.27e-04 | gates=[0.162109375, 0.19140625] | mix=hf\n",
      "step 230 | loss 1.4738 | lr 1.21e-04 | gates=[0.146484375, 0.1748046875] | mix=hf\n",
      "step 240 | loss 4.8534 | lr 1.14e-04 | gates=[0.267578125, 0.306640625] | mix=repeat\n",
      "step 250 | loss 5.1738 | lr 1.08e-04 | gates=[0.25, 0.30078125] | mix=nback\n",
      "step 260 | loss 5.5302 | lr 1.01e-04 | gates=[0.26171875, 0.306640625] | mix=copy\n",
      "step 270 | loss 5.4974 | lr 9.44e-05 | gates=[0.259765625, 0.30859375] | mix=copy\n",
      "step 280 | loss 5.2618 | lr 8.78e-05 | gates=[0.251953125, 0.306640625] | mix=nback\n",
      "step 290 | loss 4.9981 | lr 8.13e-05 | gates=[0.263671875, 0.30859375] | mix=repeat\n",
      "step 300 | loss 1.3768 | lr 7.48e-05 | gates=[0.1357421875, 0.162109375] | mix=hf\n",
      "step 310 | loss 1.3271 | lr 6.85e-05 | gates=[0.1357421875, 0.1630859375] | mix=hf\n",
      "step 320 | loss 4.7898 | lr 6.23e-05 | gates=[0.26171875, 0.30859375] | mix=repeat\n",
      "step 330 | loss 1.4903 | lr 5.62e-05 | gates=[0.1396484375, 0.166015625] | mix=hf\n",
      "step 340 | loss 4.6092 | lr 5.04e-05 | gates=[0.2734375, 0.31640625] | mix=repeat\n",
      "step 350 | loss 1.3502 | lr 4.48e-05 | gates=[0.134765625, 0.1611328125] | mix=hf\n",
      "step 360 | loss 5.6208 | lr 3.94e-05 | gates=[0.26171875, 0.3125] | mix=copy\n",
      "step 370 | loss 4.9553 | lr 3.42e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 380 | loss 1.1219 | lr 2.94e-05 | gates=[0.138671875, 0.1669921875] | mix=hf\n",
      "step 390 | loss 4.9555 | lr 2.49e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 400 | loss 4.9078 | lr 2.07e-05 | gates=[0.271484375, 0.31640625] | mix=repeat\n",
      "step 410 | loss 4.8760 | lr 2.00e-05 | gates=[0.25390625, 0.3125] | mix=nback\n",
      "step 420 | loss 1.3089 | lr 2.00e-05 | gates=[0.134765625, 0.1630859375] | mix=hf\n",
      "step 430 | loss 5.4493 | lr 2.00e-05 | gates=[0.26953125, 0.3203125] | mix=copy\n",
      "step 440 | loss 5.4301 | lr 2.00e-05 | gates=[0.275390625, 0.322265625] | mix=copy\n",
      "step 450 | loss 1.2898 | lr 2.00e-05 | gates=[0.1328125, 0.16015625] | mix=hf\n",
      "step 460 | loss 1.4554 | lr 2.00e-05 | gates=[0.13671875, 0.1630859375] | mix=hf\n",
      "step 470 | loss 1.5372 | lr 2.00e-05 | gates=[0.1328125, 0.1591796875] | mix=hf\n",
      "step 480 | loss 5.4075 | lr 2.00e-05 | gates=[0.2734375, 0.322265625] | mix=copy\n",
      "step 490 | loss 4.7853 | lr 2.00e-05 | gates=[0.27734375, 0.322265625] | mix=repeat\n",
      "step 500 | loss 1.4105 | lr 2.00e-05 | gates=[0.130859375, 0.15625] | mix=hf\n",
      "[after  E6_temp0p8_seed4242] alloc=29.34 GB | reserved=50.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed1337 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250820-164415-E7_warmstart_seed1337\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed1337] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b8481efdf704907a3646928fa44b7e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 7.1638 | lr 8.80e-05 | gates=[0.2431640625, 0.2490234375] | mix=repeat\n",
      "step 20 | loss 1.8687 | lr 1.68e-04 | gates=[0.2373046875, 0.2431640625] | mix=hf\n",
      "step 30 | loss 6.0424 | lr 2.00e-04 | gates=[0.25390625, 0.259765625] | mix=nback\n",
      "step 40 | loss 2.2922 | lr 1.99e-04 | gates=[0.2236328125, 0.232421875] | mix=hf\n",
      "step 50 | loss 1.4852 | lr 1.99e-04 | gates=[0.21484375, 0.2255859375] | mix=hf\n",
      "step 60 | loss 6.5158 | lr 1.97e-04 | gates=[0.291015625, 0.30078125] | mix=copy\n",
      "step 70 | loss 5.9229 | lr 1.95e-04 | gates=[0.2890625, 0.298828125] | mix=repeat\n",
      "step 80 | loss 5.7869 | lr 1.93e-04 | gates=[0.291015625, 0.3046875] | mix=copy\n",
      "step 90 | loss 1.7243 | lr 1.91e-04 | gates=[0.2294921875, 0.2431640625] | mix=hf\n",
      "step 100 | loss 5.8018 | lr 1.88e-04 | gates=[0.28515625, 0.306640625] | mix=nback\n",
      "step 110 | loss 5.0903 | lr 1.84e-04 | gates=[0.296875, 0.3125] | mix=repeat\n",
      "step 120 | loss 5.2232 | lr 1.81e-04 | gates=[0.283203125, 0.30859375] | mix=nback\n",
      "step 130 | loss 5.4257 | lr 1.76e-04 | gates=[0.28515625, 0.310546875] | mix=nback\n",
      "step 140 | loss 6.6510 | lr 1.72e-04 | gates=[0.283203125, 0.3125] | mix=nback\n",
      "step 150 | loss 1.5875 | lr 1.67e-04 | gates=[0.1982421875, 0.2177734375] | mix=hf\n",
      "step 160 | loss 1.4418 | lr 1.62e-04 | gates=[0.19140625, 0.2109375] | mix=hf\n",
      "step 170 | loss 1.5913 | lr 1.57e-04 | gates=[0.1845703125, 0.205078125] | mix=hf\n",
      "step 180 | loss 5.6345 | lr 1.51e-04 | gates=[0.28515625, 0.31640625] | mix=nback\n",
      "step 190 | loss 5.8077 | lr 1.46e-04 | gates=[0.291015625, 0.322265625] | mix=copy\n",
      "step 200 | loss 6.2038 | lr 1.40e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 210 | loss 5.4467 | lr 1.33e-04 | gates=[0.30078125, 0.33203125] | mix=copy\n",
      "step 220 | loss 5.2100 | lr 1.27e-04 | gates=[0.287109375, 0.326171875] | mix=nback\n",
      "step 230 | loss 5.2107 | lr 1.21e-04 | gates=[0.302734375, 0.33203125] | mix=repeat\n",
      "step 240 | loss 5.2596 | lr 1.14e-04 | gates=[0.28515625, 0.328125] | mix=nback\n",
      "step 250 | loss 1.4311 | lr 1.08e-04 | gates=[0.166015625, 0.1904296875] | mix=hf\n",
      "step 260 | loss 5.0308 | lr 1.01e-04 | gates=[0.310546875, 0.337890625] | mix=repeat\n",
      "step 270 | loss 1.5158 | lr 9.44e-05 | gates=[0.1611328125, 0.1845703125] | mix=hf\n",
      "step 280 | loss 5.7483 | lr 8.78e-05 | gates=[0.28125, 0.328125] | mix=nback\n",
      "step 290 | loss 5.5913 | lr 8.13e-05 | gates=[0.294921875, 0.33203125] | mix=repeat\n",
      "step 300 | loss 7.1839 | lr 7.48e-05 | gates=[0.287109375, 0.330078125] | mix=nback\n",
      "step 310 | loss 5.3411 | lr 6.85e-05 | gates=[0.296875, 0.330078125] | mix=repeat\n",
      "step 320 | loss 5.5067 | lr 6.23e-05 | gates=[0.30078125, 0.337890625] | mix=copy\n",
      "step 330 | loss 5.5551 | lr 5.62e-05 | gates=[0.3046875, 0.33984375] | mix=copy\n",
      "step 340 | loss 5.5801 | lr 5.04e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 350 | loss 5.1894 | lr 4.48e-05 | gates=[0.3125, 0.341796875] | mix=repeat\n",
      "step 360 | loss 1.7356 | lr 3.94e-05 | gates=[0.150390625, 0.1728515625] | mix=hf\n",
      "step 370 | loss 5.5183 | lr 3.42e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 380 | loss 1.1121 | lr 2.94e-05 | gates=[0.154296875, 0.177734375] | mix=hf\n",
      "step 390 | loss 5.4260 | lr 2.49e-05 | gates=[0.30859375, 0.34375] | mix=copy\n",
      "step 400 | loss 1.3752 | lr 2.07e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 410 | loss 1.3046 | lr 2.00e-05 | gates=[0.150390625, 0.1728515625] | mix=hf\n",
      "step 420 | loss 4.6520 | lr 2.00e-05 | gates=[0.3046875, 0.341796875] | mix=repeat\n",
      "step 430 | loss 1.4948 | lr 2.00e-05 | gates=[0.1494140625, 0.173828125] | mix=hf\n",
      "step 440 | loss 1.3455 | lr 2.00e-05 | gates=[0.1484375, 0.171875] | mix=hf\n",
      "step 450 | loss 1.2662 | lr 2.00e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 460 | loss 1.5739 | lr 2.00e-05 | gates=[0.1484375, 0.1728515625] | mix=hf\n",
      "step 470 | loss 5.4359 | lr 2.00e-05 | gates=[0.3046875, 0.341796875] | mix=copy\n",
      "step 480 | loss 1.1348 | lr 2.00e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 490 | loss 4.8031 | lr 2.00e-05 | gates=[0.302734375, 0.33984375] | mix=repeat\n",
      "step 500 | loss 5.4466 | lr 2.00e-05 | gates=[0.294921875, 0.337890625] | mix=copy\n",
      "[after  E7_warmstart_seed1337] alloc=29.34 GB | reserved=50.91 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed2027 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250820-174016-E7_warmstart_seed2027\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed2027] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b569bcf4e7d47c68114ab593acb7201"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.4950 | lr 8.80e-05 | gates=[0.244140625, 0.24609375] | mix=hf\n",
      "step 20 | loss 5.4397 | lr 1.68e-04 | gates=[0.240234375, 0.2421875] | mix=hf\n",
      "step 30 | loss 5.8121 | lr 2.00e-04 | gates=[0.248046875, 0.255859375] | mix=nback\n",
      "step 40 | loss 8.2393 | lr 1.99e-04 | gates=[0.251953125, 0.263671875] | mix=copy\n",
      "step 50 | loss 6.9483 | lr 1.99e-04 | gates=[0.251953125, 0.265625] | mix=copy\n",
      "step 60 | loss 2.1542 | lr 1.97e-04 | gates=[0.25390625, 0.26171875] | mix=hf\n",
      "step 70 | loss 1.6823 | lr 1.95e-04 | gates=[0.248046875, 0.259765625] | mix=hf\n",
      "step 80 | loss 6.2565 | lr 1.93e-04 | gates=[0.28515625, 0.306640625] | mix=copy\n",
      "step 90 | loss 8.1904 | lr 1.91e-04 | gates=[0.28125, 0.306640625] | mix=nback\n",
      "step 100 | loss 6.0802 | lr 1.88e-04 | gates=[0.291015625, 0.3125] | mix=repeat\n",
      "step 110 | loss 6.4359 | lr 1.84e-04 | gates=[0.283203125, 0.310546875] | mix=copy\n",
      "step 120 | loss 1.4914 | lr 1.81e-04 | gates=[0.2236328125, 0.2451171875] | mix=hf\n",
      "step 130 | loss 5.9994 | lr 1.76e-04 | gates=[0.287109375, 0.318359375] | mix=copy\n",
      "step 140 | loss 5.9462 | lr 1.72e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 150 | loss 6.0262 | lr 1.67e-04 | gates=[0.279296875, 0.322265625] | mix=nback\n",
      "step 160 | loss 1.4679 | lr 1.62e-04 | gates=[0.2041015625, 0.23046875] | mix=hf\n",
      "step 170 | loss 5.2189 | lr 1.57e-04 | gates=[0.29296875, 0.330078125] | mix=repeat\n",
      "step 180 | loss 7.6009 | lr 1.51e-04 | gates=[0.275390625, 0.32421875] | mix=nback\n",
      "step 190 | loss 1.4652 | lr 1.46e-04 | gates=[0.1875, 0.21484375] | mix=hf\n",
      "step 200 | loss 1.3550 | lr 1.40e-04 | gates=[0.1884765625, 0.216796875] | mix=hf\n",
      "step 210 | loss 1.0956 | lr 1.33e-04 | gates=[0.193359375, 0.2216796875] | mix=hf\n",
      "step 220 | loss 5.3350 | lr 1.27e-04 | gates=[0.271484375, 0.32421875] | mix=nback\n",
      "step 230 | loss 1.4243 | lr 1.21e-04 | gates=[0.1728515625, 0.1982421875] | mix=hf\n",
      "step 240 | loss 5.6708 | lr 1.14e-04 | gates=[0.296875, 0.337890625] | mix=repeat\n",
      "step 250 | loss 1.6800 | lr 1.08e-04 | gates=[0.166015625, 0.193359375] | mix=hf\n",
      "step 260 | loss 1.8363 | lr 1.01e-04 | gates=[0.1650390625, 0.193359375] | mix=hf\n",
      "step 270 | loss 5.4899 | lr 9.44e-05 | gates=[0.294921875, 0.341796875] | mix=copy\n",
      "step 280 | loss 1.2525 | lr 8.78e-05 | gates=[0.1689453125, 0.197265625] | mix=hf\n",
      "step 290 | loss 1.4398 | lr 8.13e-05 | gates=[0.1650390625, 0.193359375] | mix=hf\n",
      "step 300 | loss 5.2471 | lr 7.48e-05 | gates=[0.275390625, 0.333984375] | mix=nback\n",
      "step 310 | loss 1.5230 | lr 6.85e-05 | gates=[0.1611328125, 0.1875] | mix=hf\n",
      "step 320 | loss 5.0540 | lr 6.23e-05 | gates=[0.271484375, 0.33203125] | mix=nback\n",
      "step 330 | loss 5.0565 | lr 5.62e-05 | gates=[0.275390625, 0.3359375] | mix=nback\n",
      "step 340 | loss 4.9982 | lr 5.04e-05 | gates=[0.287109375, 0.337890625] | mix=repeat\n",
      "step 350 | loss 1.4297 | lr 4.48e-05 | gates=[0.15625, 0.181640625] | mix=hf\n",
      "step 360 | loss 4.8631 | lr 3.94e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 370 | loss 1.2115 | lr 3.42e-05 | gates=[0.1640625, 0.189453125] | mix=hf\n",
      "step 380 | loss 1.4938 | lr 2.94e-05 | gates=[0.16015625, 0.189453125] | mix=hf\n",
      "step 390 | loss 4.8127 | lr 2.49e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 400 | loss 5.4351 | lr 2.07e-05 | gates=[0.28125, 0.33984375] | mix=copy\n",
      "step 410 | loss 4.8386 | lr 2.00e-05 | gates=[0.30078125, 0.34765625] | mix=repeat\n",
      "step 420 | loss 1.1400 | lr 2.00e-05 | gates=[0.1640625, 0.19140625] | mix=hf\n",
      "step 430 | loss 4.9045 | lr 2.00e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 440 | loss 1.2898 | lr 2.00e-05 | gates=[0.15625, 0.1826171875] | mix=hf\n",
      "step 450 | loss 5.4476 | lr 2.00e-05 | gates=[0.2890625, 0.34375] | mix=copy\n",
      "step 460 | loss 5.4332 | lr 2.00e-05 | gates=[0.291015625, 0.34375] | mix=copy\n",
      "step 470 | loss 1.1479 | lr 2.00e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 480 | loss 4.8677 | lr 2.00e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 490 | loss 5.4633 | lr 2.00e-05 | gates=[0.291015625, 0.345703125] | mix=copy\n",
      "step 500 | loss 0.9382 | lr 2.00e-05 | gates=[0.166015625, 0.1982421875] | mix=hf\n",
      "[after  E7_warmstart_seed2027] alloc=29.34 GB | reserved=49.33 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed4242 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250820-181956-E7_warmstart_seed4242\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed4242] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d003beeac4048cba6ddf625019b320a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.8860 | lr 8.80e-05 | gates=[0.248046875, 0.2470703125] | mix=copy\n",
      "step 20 | loss 6.7967 | lr 1.68e-04 | gates=[0.25390625, 0.255859375] | mix=nback\n",
      "step 30 | loss 5.8866 | lr 2.00e-04 | gates=[0.2578125, 0.2578125] | mix=copy\n",
      "step 40 | loss 5.5299 | lr 1.99e-04 | gates=[0.259765625, 0.26171875] | mix=repeat\n",
      "step 50 | loss 1.4707 | lr 1.99e-04 | gates=[0.220703125, 0.228515625] | mix=hf\n",
      "step 60 | loss 5.3748 | lr 1.97e-04 | gates=[0.291015625, 0.30078125] | mix=nback\n",
      "step 70 | loss 8.4891 | lr 1.95e-04 | gates=[0.29296875, 0.30078125] | mix=copy\n",
      "step 80 | loss 6.0569 | lr 1.93e-04 | gates=[0.296875, 0.302734375] | mix=copy\n",
      "step 90 | loss 1.6448 | lr 1.91e-04 | gates=[0.228515625, 0.236328125] | mix=hf\n",
      "step 100 | loss 5.4202 | lr 1.88e-04 | gates=[0.294921875, 0.3046875] | mix=repeat\n",
      "step 110 | loss 1.7415 | lr 1.84e-04 | gates=[0.2138671875, 0.224609375] | mix=hf\n",
      "step 120 | loss 3.8637 | lr 1.81e-04 | gates=[0.212890625, 0.2255859375] | mix=hf\n",
      "step 130 | loss 1.7198 | lr 1.76e-04 | gates=[0.2119140625, 0.224609375] | mix=hf\n",
      "step 140 | loss 10.8270 | lr 1.72e-04 | gates=[0.291015625, 0.310546875] | mix=copy\n",
      "step 150 | loss 6.0492 | lr 1.67e-04 | gates=[0.29296875, 0.3125] | mix=copy\n",
      "step 160 | loss 2.5608 | lr 1.62e-04 | gates=[0.1982421875, 0.2158203125] | mix=hf\n",
      "step 170 | loss 5.8301 | lr 1.57e-04 | gates=[0.2890625, 0.31640625] | mix=copy\n",
      "step 180 | loss 1.5077 | lr 1.51e-04 | gates=[0.1982421875, 0.21484375] | mix=hf\n",
      "step 190 | loss 5.8626 | lr 1.46e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 200 | loss 5.3078 | lr 1.40e-04 | gates=[0.275390625, 0.314453125] | mix=nback\n",
      "step 210 | loss 5.4471 | lr 1.33e-04 | gates=[0.27734375, 0.31640625] | mix=nback\n",
      "step 220 | loss 1.2853 | lr 1.27e-04 | gates=[0.1923828125, 0.2158203125] | mix=hf\n",
      "step 230 | loss 1.5224 | lr 1.21e-04 | gates=[0.177734375, 0.2001953125] | mix=hf\n",
      "step 240 | loss 4.9142 | lr 1.14e-04 | gates=[0.296875, 0.32421875] | mix=repeat\n",
      "step 250 | loss 5.1006 | lr 1.08e-04 | gates=[0.27734375, 0.318359375] | mix=nback\n",
      "step 260 | loss 5.5793 | lr 1.01e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 270 | loss 5.5309 | lr 9.44e-05 | gates=[0.2890625, 0.32421875] | mix=copy\n",
      "step 280 | loss 5.3767 | lr 8.78e-05 | gates=[0.279296875, 0.322265625] | mix=nback\n",
      "step 290 | loss 5.5246 | lr 8.13e-05 | gates=[0.29296875, 0.32421875] | mix=repeat\n",
      "step 300 | loss 1.3964 | lr 7.48e-05 | gates=[0.1650390625, 0.185546875] | mix=hf\n",
      "step 310 | loss 1.3540 | lr 6.85e-05 | gates=[0.1650390625, 0.1865234375] | mix=hf\n",
      "step 320 | loss 4.9202 | lr 6.23e-05 | gates=[0.2890625, 0.322265625] | mix=repeat\n",
      "step 330 | loss 1.5136 | lr 5.62e-05 | gates=[0.1669921875, 0.1884765625] | mix=hf\n",
      "step 340 | loss 4.7757 | lr 5.04e-05 | gates=[0.30078125, 0.33203125] | mix=repeat\n",
      "step 350 | loss 1.3729 | lr 4.48e-05 | gates=[0.162109375, 0.18359375] | mix=hf\n",
      "step 360 | loss 5.5856 | lr 3.94e-05 | gates=[0.287109375, 0.32421875] | mix=copy\n",
      "step 370 | loss 4.9658 | lr 3.42e-05 | gates=[0.279296875, 0.32421875] | mix=nback\n",
      "step 380 | loss 1.1322 | lr 2.94e-05 | gates=[0.166015625, 0.1884765625] | mix=hf\n",
      "step 390 | loss 4.9446 | lr 2.49e-05 | gates=[0.28125, 0.32421875] | mix=nback\n",
      "step 400 | loss 4.9412 | lr 2.07e-05 | gates=[0.296875, 0.330078125] | mix=repeat\n",
      "step 410 | loss 4.8818 | lr 2.00e-05 | gates=[0.27734375, 0.322265625] | mix=nback\n",
      "step 420 | loss 1.3139 | lr 2.00e-05 | gates=[0.162109375, 0.1845703125] | mix=hf\n",
      "step 430 | loss 5.4501 | lr 2.00e-05 | gates=[0.294921875, 0.33203125] | mix=copy\n",
      "step 440 | loss 5.4297 | lr 2.00e-05 | gates=[0.298828125, 0.333984375] | mix=copy\n",
      "step 450 | loss 1.3043 | lr 2.00e-05 | gates=[0.1591796875, 0.181640625] | mix=hf\n",
      "step 460 | loss 1.4742 | lr 2.00e-05 | gates=[0.1630859375, 0.1845703125] | mix=hf\n",
      "step 470 | loss 1.5606 | lr 2.00e-05 | gates=[0.1591796875, 0.1806640625] | mix=hf\n",
      "step 480 | loss 5.4087 | lr 2.00e-05 | gates=[0.296875, 0.333984375] | mix=copy\n",
      "step 490 | loss 4.7991 | lr 2.00e-05 | gates=[0.302734375, 0.3359375] | mix=repeat\n",
      "step 500 | loss 1.4304 | lr 2.00e-05 | gates=[0.1572265625, 0.177734375] | mix=hf\n",
      "[after  E7_warmstart_seed4242] alloc=29.34 GB | reserved=50.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N64 | seed=777 ===\n",
      "TB run started: ./runs\\dncformer-20250820-191327-E8_capacity_N64\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E8_capacity_N64] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5470c79e6ba412fabd39ac3c8ba54f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.6379 | lr 8.80e-05 | gates=[0.28125, 0.279296875] | mix=hf\n",
      "step 20 | loss 6.7689 | lr 1.68e-04 | gates=[0.279296875, 0.287109375] | mix=copy\n",
      "step 30 | loss 6.5771 | lr 2.00e-04 | gates=[0.28125, 0.291015625] | mix=nback\n",
      "step 40 | loss 6.0427 | lr 1.99e-04 | gates=[0.28515625, 0.294921875] | mix=repeat\n",
      "step 50 | loss 7.1813 | lr 1.99e-04 | gates=[0.28125, 0.294921875] | mix=nback\n",
      "step 60 | loss 2.1033 | lr 1.97e-04 | gates=[0.240234375, 0.24609375] | mix=hf\n",
      "step 70 | loss 1.5742 | lr 1.95e-04 | gates=[0.232421875, 0.240234375] | mix=hf\n",
      "step 80 | loss 8.6676 | lr 1.93e-04 | gates=[0.28125, 0.302734375] | mix=nback\n",
      "step 90 | loss 1.8938 | lr 1.91e-04 | gates=[0.2099609375, 0.22265625] | mix=hf\n",
      "step 100 | loss 5.6929 | lr 1.88e-04 | gates=[0.275390625, 0.306640625] | mix=nback\n",
      "step 110 | loss 1.7913 | lr 1.84e-04 | gates=[0.1982421875, 0.2138671875] | mix=hf\n",
      "step 120 | loss 7.8458 | lr 1.81e-04 | gates=[0.287109375, 0.31640625] | mix=copy\n",
      "step 130 | loss 9.1497 | lr 1.76e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 140 | loss 5.9620 | lr 1.72e-04 | gates=[0.279296875, 0.31640625] | mix=copy\n",
      "step 150 | loss 1.0452 | lr 1.67e-04 | gates=[0.1923828125, 0.21484375] | mix=hf\n",
      "step 160 | loss 5.2955 | lr 1.62e-04 | gates=[0.28125, 0.32421875] | mix=nback\n",
      "step 170 | loss 5.7708 | lr 1.57e-04 | gates=[0.287109375, 0.326171875] | mix=copy\n",
      "step 180 | loss 5.1257 | lr 1.51e-04 | gates=[0.296875, 0.33203125] | mix=repeat\n",
      "step 190 | loss 1.9651 | lr 1.46e-04 | gates=[0.17578125, 0.2021484375] | mix=hf\n",
      "step 200 | loss 5.2592 | lr 1.40e-04 | gates=[0.28125, 0.330078125] | mix=nback\n",
      "step 210 | loss 4.9860 | lr 1.33e-04 | gates=[0.28125, 0.333984375] | mix=nback\n",
      "step 220 | loss 5.6707 | lr 1.27e-04 | gates=[0.30078125, 0.341796875] | mix=copy\n",
      "step 230 | loss 5.6167 | lr 1.21e-04 | gates=[0.291015625, 0.33984375] | mix=copy\n",
      "step 240 | loss 5.2426 | lr 1.14e-04 | gates=[0.283203125, 0.337890625] | mix=nback\n",
      "step 250 | loss 5.2357 | lr 1.08e-04 | gates=[0.302734375, 0.345703125] | mix=repeat\n",
      "step 260 | loss 1.2203 | lr 1.01e-04 | gates=[0.16796875, 0.193359375] | mix=hf\n",
      "step 270 | loss 1.2332 | lr 9.44e-05 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 280 | loss 5.6227 | lr 8.78e-05 | gates=[0.29296875, 0.34375] | mix=copy\n",
      "step 290 | loss 5.0960 | lr 8.13e-05 | gates=[0.279296875, 0.33984375] | mix=nback\n",
      "step 300 | loss 4.8855 | lr 7.48e-05 | gates=[0.28125, 0.341796875] | mix=nback\n",
      "step 310 | loss 1.3677 | lr 6.85e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 320 | loss 1.3331 | lr 6.23e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 330 | loss 5.4267 | lr 5.62e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 340 | loss 4.9618 | lr 5.04e-05 | gates=[0.28515625, 0.34375] | mix=nback\n",
      "step 350 | loss 1.0984 | lr 4.48e-05 | gates=[0.16015625, 0.1865234375] | mix=hf\n",
      "step 360 | loss 4.8881 | lr 3.94e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 370 | loss 1.4181 | lr 3.42e-05 | gates=[0.1513671875, 0.17578125] | mix=hf\n",
      "step 380 | loss 1.2786 | lr 2.94e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 390 | loss 1.4918 | lr 2.49e-05 | gates=[0.15234375, 0.1767578125] | mix=hf\n",
      "step 400 | loss 1.4856 | lr 2.07e-05 | gates=[0.15625, 0.1826171875] | mix=hf\n",
      "step 410 | loss 5.4803 | lr 2.00e-05 | gates=[0.3046875, 0.353515625] | mix=copy\n",
      "step 420 | loss 5.4546 | lr 2.00e-05 | gates=[0.294921875, 0.349609375] | mix=copy\n",
      "step 430 | loss 1.3307 | lr 2.00e-05 | gates=[0.16015625, 0.18359375] | mix=hf\n",
      "step 440 | loss 4.8551 | lr 2.00e-05 | gates=[0.28125, 0.34375] | mix=nback\n",
      "step 450 | loss 1.5435 | lr 2.00e-05 | gates=[0.1513671875, 0.173828125] | mix=hf\n",
      "step 460 | loss 1.4390 | lr 2.00e-05 | gates=[0.150390625, 0.173828125] | mix=hf\n",
      "step 470 | loss 4.8235 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 480 | loss 4.8741 | lr 2.00e-05 | gates=[0.283203125, 0.345703125] | mix=nback\n",
      "step 490 | loss 4.8593 | lr 2.00e-05 | gates=[0.298828125, 0.349609375] | mix=repeat\n",
      "step 500 | loss 1.0074 | lr 2.00e-05 | gates=[0.1572265625, 0.181640625] | mix=hf\n",
      "[after  E8_capacity_N64] alloc=29.34 GB | reserved=54.00 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.38 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N128 | seed=777 ===\n",
      "TB run started: ./runs\\dncformer-20250820-195242-E8_capacity_N128\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E8_capacity_N128] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cbdec81063d4f7786b23319a21bf5c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.6384 | lr 8.80e-05 | gates=[0.28125, 0.279296875] | mix=hf\n",
      "step 20 | loss 6.7687 | lr 1.68e-04 | gates=[0.279296875, 0.287109375] | mix=copy\n",
      "step 30 | loss 6.5741 | lr 2.00e-04 | gates=[0.28125, 0.291015625] | mix=nback\n",
      "step 40 | loss 6.0421 | lr 1.99e-04 | gates=[0.28515625, 0.294921875] | mix=repeat\n",
      "step 50 | loss 6.5700 | lr 1.99e-04 | gates=[0.28125, 0.294921875] | mix=nback\n",
      "step 60 | loss 2.1646 | lr 1.97e-04 | gates=[0.240234375, 0.24609375] | mix=hf\n",
      "step 70 | loss 1.6074 | lr 1.95e-04 | gates=[0.232421875, 0.240234375] | mix=hf\n",
      "step 80 | loss 5.7022 | lr 1.93e-04 | gates=[0.28125, 0.302734375] | mix=nback\n",
      "step 90 | loss 1.8592 | lr 1.91e-04 | gates=[0.2099609375, 0.22265625] | mix=hf\n",
      "step 100 | loss 6.3857 | lr 1.88e-04 | gates=[0.275390625, 0.3046875] | mix=nback\n",
      "step 110 | loss 1.9222 | lr 1.84e-04 | gates=[0.19921875, 0.21484375] | mix=hf\n",
      "step 120 | loss 5.9504 | lr 1.81e-04 | gates=[0.2890625, 0.314453125] | mix=copy\n",
      "step 130 | loss 7.1080 | lr 1.76e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 140 | loss 5.8286 | lr 1.72e-04 | gates=[0.283203125, 0.322265625] | mix=copy\n",
      "step 150 | loss 1.2142 | lr 1.67e-04 | gates=[0.1962890625, 0.2197265625] | mix=hf\n",
      "step 160 | loss 5.6951 | lr 1.62e-04 | gates=[0.283203125, 0.328125] | mix=nback\n",
      "step 170 | loss 6.0265 | lr 1.57e-04 | gates=[0.287109375, 0.328125] | mix=copy\n",
      "step 180 | loss 5.3573 | lr 1.51e-04 | gates=[0.294921875, 0.333984375] | mix=repeat\n",
      "step 190 | loss 2.1462 | lr 1.46e-04 | gates=[0.1806640625, 0.20703125] | mix=hf\n",
      "step 200 | loss 5.3493 | lr 1.40e-04 | gates=[0.279296875, 0.330078125] | mix=nback\n",
      "step 210 | loss 5.3006 | lr 1.33e-04 | gates=[0.279296875, 0.33203125] | mix=nback\n",
      "step 220 | loss 5.8976 | lr 1.27e-04 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 230 | loss 5.7644 | lr 1.21e-04 | gates=[0.2890625, 0.337890625] | mix=copy\n",
      "step 240 | loss 5.5860 | lr 1.14e-04 | gates=[0.279296875, 0.333984375] | mix=nback\n",
      "step 250 | loss 5.0976 | lr 1.08e-04 | gates=[0.298828125, 0.341796875] | mix=repeat\n",
      "step 260 | loss 1.2219 | lr 1.01e-04 | gates=[0.171875, 0.1962890625] | mix=hf\n",
      "step 270 | loss 1.2188 | lr 9.44e-05 | gates=[0.169921875, 0.1953125] | mix=hf\n",
      "step 280 | loss 5.5387 | lr 8.78e-05 | gates=[0.291015625, 0.341796875] | mix=copy\n",
      "step 290 | loss 4.9039 | lr 8.13e-05 | gates=[0.27734375, 0.33984375] | mix=nback\n",
      "step 300 | loss 4.8545 | lr 7.48e-05 | gates=[0.28125, 0.341796875] | mix=nback\n",
      "step 310 | loss 1.3759 | lr 6.85e-05 | gates=[0.1611328125, 0.1875] | mix=hf\n",
      "step 320 | loss 1.3384 | lr 6.23e-05 | gates=[0.1572265625, 0.181640625] | mix=hf\n",
      "step 330 | loss 5.5661 | lr 5.62e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 340 | loss 5.1252 | lr 5.04e-05 | gates=[0.283203125, 0.34375] | mix=nback\n",
      "step 350 | loss 1.0931 | lr 4.48e-05 | gates=[0.162109375, 0.1884765625] | mix=hf\n",
      "step 360 | loss 4.9137 | lr 3.94e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 370 | loss 1.4183 | lr 3.42e-05 | gates=[0.154296875, 0.177734375] | mix=hf\n",
      "step 380 | loss 1.2778 | lr 2.94e-05 | gates=[0.16015625, 0.1875] | mix=hf\n",
      "step 390 | loss 1.4921 | lr 2.49e-05 | gates=[0.1552734375, 0.1787109375] | mix=hf\n",
      "step 400 | loss 1.4885 | lr 2.07e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 410 | loss 5.5278 | lr 2.00e-05 | gates=[0.302734375, 0.353515625] | mix=copy\n",
      "step 420 | loss 5.4768 | lr 2.00e-05 | gates=[0.29296875, 0.349609375] | mix=copy\n",
      "step 430 | loss 1.3314 | lr 2.00e-05 | gates=[0.162109375, 0.185546875] | mix=hf\n",
      "step 440 | loss 4.8825 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 450 | loss 1.5504 | lr 2.00e-05 | gates=[0.1533203125, 0.17578125] | mix=hf\n",
      "step 460 | loss 1.4431 | lr 2.00e-05 | gates=[0.15234375, 0.17578125] | mix=hf\n",
      "step 470 | loss 4.8245 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 480 | loss 4.8751 | lr 2.00e-05 | gates=[0.28125, 0.345703125] | mix=nback\n",
      "step 490 | loss 4.8706 | lr 2.00e-05 | gates=[0.296875, 0.34765625] | mix=repeat\n",
      "step 500 | loss 1.0085 | lr 2.00e-05 | gates=[0.1591796875, 0.18359375] | mix=hf\n",
      "[after  E8_capacity_N128] alloc=29.34 GB | reserved=54.00 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.38 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E9_baseline_haystack | seed=31415 ===\n",
      "TB run started: ./runs\\dncformer-20250820-210448-E9_baseline_haystack\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E9_baseline_haystack] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba751c396da24aa28042857a8f84a616"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.5027 | lr 8.80e-05 | gates=[0.27734375, 0.279296875] | mix=hf\n",
      "step 20 | loss 2.2774 | lr 1.68e-04 | gates=[0.26953125, 0.271484375] | mix=hf\n",
      "step 30 | loss 6.3625 | lr 2.00e-04 | gates=[0.28125, 0.2890625] | mix=nback\n",
      "step 40 | loss 6.3541 | lr 1.99e-04 | gates=[0.28515625, 0.29296875] | mix=copy\n",
      "step 50 | loss 1.6552 | lr 1.99e-04 | gates=[0.240234375, 0.2470703125] | mix=hf\n",
      "step 60 | loss 1.6690 | lr 1.97e-04 | gates=[0.23828125, 0.2451171875] | mix=hf\n",
      "step 70 | loss 2.0622 | lr 1.95e-04 | gates=[0.2275390625, 0.2373046875] | mix=hf\n",
      "step 80 | loss 1.6111 | lr 1.93e-04 | gates=[0.220703125, 0.232421875] | mix=hf\n",
      "step 90 | loss 1.6116 | lr 1.91e-04 | gates=[0.2158203125, 0.2294921875] | mix=hf\n",
      "step 100 | loss 5.7697 | lr 1.88e-04 | gates=[0.28125, 0.30859375] | mix=copy\n",
      "step 110 | loss 3.2152 | lr 1.84e-04 | gates=[0.2021484375, 0.2216796875] | mix=hf\n",
      "step 120 | loss 2.9168 | lr 1.81e-04 | gates=[0.20703125, 0.2294921875] | mix=hf\n",
      "step 130 | loss 5.2972 | lr 1.76e-04 | gates=[0.2890625, 0.318359375] | mix=repeat\n",
      "step 140 | loss 6.2627 | lr 1.72e-04 | gates=[0.2890625, 0.32421875] | mix=copy\n",
      "step 150 | loss 3.5212 | lr 1.67e-04 | gates=[0.201171875, 0.2265625] | mix=hf\n",
      "step 160 | loss 5.0936 | lr 1.62e-04 | gates=[0.275390625, 0.3203125] | mix=nback\n",
      "step 170 | loss 5.3806 | lr 1.57e-04 | gates=[0.27734375, 0.32421875] | mix=nback\n",
      "step 180 | loss 1.3807 | lr 1.51e-04 | gates=[0.1904296875, 0.21484375] | mix=hf\n",
      "step 190 | loss 5.8375 | lr 1.46e-04 | gates=[0.275390625, 0.326171875] | mix=nback\n",
      "step 200 | loss 4.9613 | lr 1.40e-04 | gates=[0.29296875, 0.33203125] | mix=repeat\n",
      "step 210 | loss 5.1688 | lr 1.33e-04 | gates=[0.296875, 0.333984375] | mix=repeat\n",
      "step 220 | loss 5.1036 | lr 1.27e-04 | gates=[0.291015625, 0.33203125] | mix=repeat\n",
      "step 230 | loss 5.0059 | lr 1.21e-04 | gates=[0.28515625, 0.33203125] | mix=repeat\n",
      "step 240 | loss 1.7511 | lr 1.14e-04 | gates=[0.1806640625, 0.2060546875] | mix=hf\n",
      "step 250 | loss 1.6581 | lr 1.08e-04 | gates=[0.181640625, 0.20703125] | mix=hf\n",
      "step 260 | loss 5.3927 | lr 1.01e-04 | gates=[0.294921875, 0.337890625] | mix=repeat\n",
      "step 270 | loss 1.3165 | lr 9.44e-05 | gates=[0.17578125, 0.201171875] | mix=hf\n",
      "step 280 | loss 5.2877 | lr 8.78e-05 | gates=[0.298828125, 0.33984375] | mix=repeat\n",
      "step 290 | loss 5.0323 | lr 8.13e-05 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 300 | loss 1.3564 | lr 7.48e-05 | gates=[0.169921875, 0.1943359375] | mix=hf\n",
      "step 310 | loss 4.8378 | lr 6.85e-05 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 320 | loss 1.4902 | lr 6.23e-05 | gates=[0.1728515625, 0.19921875] | mix=hf\n",
      "step 330 | loss 4.8779 | lr 5.62e-05 | gates=[0.298828125, 0.34375] | mix=repeat\n",
      "step 340 | loss 1.4196 | lr 5.04e-05 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 350 | loss 1.3047 | lr 4.48e-05 | gates=[0.16796875, 0.193359375] | mix=hf\n",
      "step 360 | loss 1.2860 | lr 3.94e-05 | gates=[0.171875, 0.1962890625] | mix=hf\n",
      "step 370 | loss 5.0669 | lr 3.42e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 380 | loss 1.4352 | lr 2.94e-05 | gates=[0.1591796875, 0.1826171875] | mix=hf\n",
      "step 390 | loss 4.8232 | lr 2.49e-05 | gates=[0.28125, 0.33984375] | mix=nback\n",
      "step 400 | loss 1.1356 | lr 2.07e-05 | gates=[0.1630859375, 0.1865234375] | mix=hf\n",
      "step 410 | loss 4.8544 | lr 2.00e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 420 | loss 5.4727 | lr 2.00e-05 | gates=[0.296875, 0.345703125] | mix=copy\n",
      "step 430 | loss 1.4443 | lr 2.00e-05 | gates=[0.15625, 0.1787109375] | mix=hf\n",
      "step 440 | loss 1.3751 | lr 2.00e-05 | gates=[0.1669921875, 0.19140625] | mix=hf\n",
      "step 450 | loss 1.3825 | lr 2.00e-05 | gates=[0.1591796875, 0.1826171875] | mix=hf\n",
      "step 460 | loss 5.4045 | lr 2.00e-05 | gates=[0.291015625, 0.34375] | mix=copy\n",
      "step 470 | loss 4.7521 | lr 2.00e-05 | gates=[0.30078125, 0.34765625] | mix=repeat\n",
      "step 480 | loss 5.4408 | lr 2.00e-05 | gates=[0.29296875, 0.345703125] | mix=copy\n",
      "step 490 | loss 1.0640 | lr 2.00e-05 | gates=[0.16015625, 0.1845703125] | mix=hf\n",
      "step 500 | loss 1.2654 | lr 2.00e-05 | gates=[0.15234375, 0.1748046875] | mix=hf\n",
      "[Haystack] acc=0.000 | loss=15.411 | fast=True\n",
      "[after  E9_baseline_haystack] alloc=29.34 GB | reserved=50.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.36 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, time\n",
    "\n",
    "# Common settings\n",
    "EXP_STEPS = 500\n",
    "BASE_MIX  = (0.4, 0.2, 0.2, 0.2)\n",
    "SEEDS     = [1337, 2027, 4242]\n",
    "\n",
    "# === E6: gate_temp=0.8 (3 seeds), otherwise baseline ===\n",
    "set_cfg(gate_reg_lambda=getattr(CFG, \"gate_reg_lambda\", 2e-4))  # low-λ default\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E6_temp0p8_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    gate_temp_schedule=[(None, 0.8)])\n",
    "\n",
    "# === E7: memory-leaning warm-start (first 10% steps), then baseline; 3 seeds ===\n",
    "warm_steps = max(50, EXP_STEPS // 10)\n",
    "mix_warm   = (0.3, 0.3, 0.25, 0.15)  # a bit more memory-heavy than baseline\n",
    "mix_main   = BASE_MIX\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E7_warmstart_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    mixture_schedule=[(warm_steps, mix_warm), (None, mix_main)],\n",
    "                    gate_temp_schedule=[(warm_steps, 0.8), (None, 1.0)],\n",
    "                    gate_reg_schedule=[(warm_steps, max(2e-4, getattr(CFG,'gate_reg_lambda', 2e-4))), (None, getattr(CFG,'gate_reg_lambda', 2e-4))])\n",
    "\n",
    "# === E8: capacity sweep N=64 vs N=128 (1 seed each) ===\n",
    "for N_val in (64, 128):\n",
    "    set_cfg(N=N_val)  # assumes your DNC block reads CFG.N at construction\n",
    "    run_one_labeled(f\"E8_capacity_N{N_val}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=777,\n",
    "                    gate_temp_schedule=[(None, getattr(CFG, 'gate_temp', 1.0))])\n",
    "\n",
    "# Reset N if changed\n",
    "set_cfg(N=getattr(CFG, 'N', 128))\n",
    "\n",
    "# === E9: baseline with haystack eval ===\n",
    "run_one_labeled(\"E9_baseline_haystack\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                seed=31415, post_haystack=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:19.682094200Z",
     "start_time": "2025-08-20T21:32:12.614600900Z"
    }
   },
   "id": "ff510cbc0c8eb5c5"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=9.64 GB | reserved=10.39 GB | free=13.98 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T07:47:35.616525600Z",
     "start_time": "2025-08-23T07:47:35.093828200Z"
    }
   },
   "id": "823dd4035914de8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Experiments - Stage 2 - tiered/parallel memory systems"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e4b05cfd4e94265"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.1 E10-14"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "729141a8e27ccd12"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def set_e10_multi_experts(K=2, N_each=64, temp=0.9, lambda_div=1e-3):\n",
    "    CFG.mem_experts = int(K)\n",
    "    CFG.expert_N = [int(N_each)]*K\n",
    "    CFG.expert_W = getattr(CFG, \"W\", 64)\n",
    "    CFG.expert_R = getattr(CFG, \"R\", 1)\n",
    "    CFG.expert_gate_temp = float(temp)\n",
    "    CFG.expert_diversity_lambda = float(lambda_div)\n",
    "    # Keep fusion off for a pure E10 test\n",
    "    CFG.fusion_enable = False\n",
    "\n",
    "def set_e11_tiered(which=\"a\"):\n",
    "    # E11a: many-narrow early, few-wide late\n",
    "    if which == \"a\":\n",
    "        CFG.per_block_cfg = [\n",
    "            {\"N\":128, \"W\":32, \"R\":1, \"gate_temp\":1.0, \"free_bias\": +0.3},\n",
    "            {\"N\": 64, \"W\":64, \"R\":1, \"gate_temp\":0.8, \"free_bias\": -0.2},\n",
    "        ]\n",
    "    # E11b: classic - longer horizon late\n",
    "    else:\n",
    "        CFG.per_block_cfg = [\n",
    "            {\"N\": 64, \"W\":32, \"R\":1, \"gate_temp\":1.0, \"free_bias\": +0.3},\n",
    "            {\"N\":128, \"W\":64, \"R\":2, \"gate_temp\":0.8, \"free_bias\": -0.2},\n",
    "        ]\n",
    "    CFG.mem_experts = 1   # keep single memory per block for E11\n",
    "    CFG.fusion_enable = False\n",
    "\n",
    "def set_e12_fusion(enable=True, hidden_mult=2.0, drop=0.0):\n",
    "    CFG.fusion_enable = bool(enable)\n",
    "    CFG.fusion_hidden_mult = float(hidden_mult)\n",
    "    CFG.fusion_drop = float(drop)\n",
    "\n",
    "def set_e13_regs(write_lambda=1e-4, overlap_lambda=0.0, on_mem_only=True):\n",
    "    CFG.write_reg_lambda = float(write_lambda)\n",
    "    CFG.key_overlap_lambda = float(overlap_lambda)  # currently inert\n",
    "    CFG.key_overlap_window = int(getattr(CFG, \"key_overlap_window\", 1))\n",
    "    CFG.reg_only_on_memory_batches = bool(on_mem_only)\n",
    "\n",
    "def set_e14_curriculum(S=200):\n",
    "    # extreme warm-start: 0 HF early\n",
    "    CFG.mixture_schedule = [(S, (0.0, 0.34, 0.33, 0.33)), (None, (0.4, 0.2, 0.2, 0.2))]\n",
    "    CFG.gate_temp_schedule = [(S, 0.8), (None, 1.0)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:35:22.009564100Z",
     "start_time": "2025-08-23T08:35:21.977537200Z"
    }
   },
   "id": "da253ebbc27abf4a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# --- E10–E14 Sweep Driver (uses run_one_labeled + setters) ---\n",
    "\n",
    "# Keys we may touch in per-experiment setters; snapshot/restore to avoid bleed-over\n",
    "_EXP_KEYS = [\n",
    "    \"mem_experts\", \"expert_N\", \"expert_W\", \"expert_R\", \"expert_gate_temp\", \"expert_diversity_lambda\",\n",
    "    \"per_block_cfg\", \"fusion_enable\", \"fusion_hidden_mult\", \"fusion_drop\",\n",
    "    \"mixture_schedule\", \"gate_temp_schedule\", \"gate_reg_schedule\",\n",
    "    \"write_reg_lambda\", \"key_overlap_lambda\", \"key_overlap_window\", \"reg_only_on_memory_batches\",\n",
    "    \"force_g\", \"gate_temp\", \"gate_reg_lambda\"\n",
    "]\n",
    "\n",
    "def _snap_exp_cfg():\n",
    "    return {k: getattr(CFG, k, None) for k in _EXP_KEYS}\n",
    "\n",
    "def _restore_exp_cfg(snap):\n",
    "    for k, v in snap.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_e10_14_sweep(\n",
    "    steps: int = None,\n",
    "    seeds = (1337,),\n",
    "    base_mix = (0.4, 0.2, 0.2, 0.2),\n",
    "    warmup: int = None,\n",
    "    post_haystack: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    E10: multi-memory experts (K=2) with modest diversity penalty\n",
    "    E11a/b: tiered memories (many/narrow early vs few/deep late)\n",
    "    E12: read-to-attention fusion enabled\n",
    "    E13: write-sparsity regs (if your training loop consumes them)\n",
    "    E14: curriculum warm-start (HF=0 early, then baseline)\n",
    "    \"\"\"\n",
    "    steps  = int(steps  if steps  is not None else getattr(CFG, \"train_steps\", 500))\n",
    "    warmup = int(warmup if warmup is not None else max(10, steps // 20))\n",
    "\n",
    "    for seed in seeds:\n",
    "        # E10 ─ Multi-memory experts (K=2)\n",
    "        snap = _snap_exp_cfg()\n",
    "        set_e10_multi_experts(K=2, N_each=64, temp=0.9, lambda_div=1e-3)\n",
    "        run_one_labeled(\n",
    "            label=f\"E10_multi_experts_k2_s{seed}_{int(time.time())}\",\n",
    "            steps=steps,\n",
    "            mixture_weights=base_mix,\n",
    "            seed=seed,\n",
    "            mixture_schedule=None, gate_temp_schedule=None, gate_reg_schedule=None,\n",
    "            post_haystack=post_haystack\n",
    "        )\n",
    "        _restore_exp_cfg(snap)\n",
    "\n",
    "        # E11a ─ Tiered memories (many/narrow early, few/deep late)\n",
    "        snap = _snap_exp_cfg()\n",
    "        set_e11_tiered(\"a\")\n",
    "        run_one_labeled(\n",
    "            label=f\"E11a_tiered_many_narrow_s{seed}_{int(time.time())}\",\n",
    "            steps=steps,\n",
    "            mixture_weights=base_mix,\n",
    "            seed=seed,\n",
    "            post_haystack=post_haystack\n",
    "        )\n",
    "        _restore_exp_cfg(snap)\n",
    "\n",
    "        # E11b ─ Tiered memories (classic: longer horizon late, deeper W/R)\n",
    "        snap = _snap_exp_cfg()\n",
    "        set_e11_tiered(\"b\")\n",
    "        run_one_labeled(\n",
    "            label=f\"E11b_tiered_deeper_late_s{seed}_{int(time.time())}\",\n",
    "            steps=steps,\n",
    "            mixture_weights=base_mix,\n",
    "            seed=seed,\n",
    "            post_haystack=post_haystack\n",
    "        )\n",
    "        _restore_exp_cfg(snap)\n",
    "\n",
    "        # # E12 ─ Fusion (read‑hint → residual MLP on vanilla path)\n",
    "        # snap = _snap_exp_cfg()\n",
    "        # set_e12_fusion(enable=True, hidden_mult=2.0, drop=0.0)\n",
    "        # run_one_labeled(\n",
    "        #     label=f\"E12_fusion_on_s{seed}_{int(time.time())}\",\n",
    "        #     steps=steps,\n",
    "        #     mixture_weights=base_mix,\n",
    "        #     seed=seed,\n",
    "        #     post_haystack=post_haystack\n",
    "        # )\n",
    "        # _restore_exp_cfg(snap)\n",
    "\n",
    "        # E13 ─ Write/overlap regs (NOTE: only effective if your train loop uses them)\n",
    "        snap = _snap_exp_cfg()\n",
    "        set_e13_regs(write_lambda=1e-4, overlap_lambda=0.0, on_mem_only=True)\n",
    "        run_one_labeled(\n",
    "            label=f\"E13_write_sparsity_s{seed}_{int(time.time())}\",\n",
    "            steps=steps,\n",
    "            mixture_weights=base_mix,\n",
    "            seed=seed,\n",
    "            post_haystack=post_haystack\n",
    "        )\n",
    "        _restore_exp_cfg(snap)\n",
    "\n",
    "        # E14 ─ Curriculum (0 HF early, then back to baseline)\n",
    "        snap = _snap_exp_cfg()\n",
    "        set_e14_curriculum(S=max(50, steps // 5))   # early phase ~20% of steps\n",
    "        run_one_labeled(\n",
    "            label=f\"E14_curriculum_warmstart_s{seed}_{int(time.time())}\",\n",
    "            steps=steps,\n",
    "            mixture_weights=base_mix,\n",
    "            seed=seed,\n",
    "            mixture_schedule=getattr(CFG, \"mixture_schedule\", None),\n",
    "            gate_temp_schedule=getattr(CFG, \"gate_temp_schedule\", None),\n",
    "            gate_reg_schedule=getattr(CFG, \"gate_reg_schedule\", None),\n",
    "            post_haystack=post_haystack\n",
    "        )\n",
    "        _restore_exp_cfg(snap)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:39:19.660995700Z",
     "start_time": "2025-08-23T08:39:19.626995100Z"
    }
   },
   "id": "b57ec5ace0b31a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Quick launch presets (adjust STEPS/SEEDS as needed)\n",
    "\n",
    "# Baseline knobs (kept in sync with your earlier experiments)\n",
    "EXP_STEPS  = 1000\n",
    "BASE_MIX   = (0.4, 0.2, 0.2, 0.2)\n",
    "SEEDS      = (1337,)            # use (1337, 2027, 4242) for 3× repeats\n",
    "POST_HAY   = False              # set True to run short haystack after each run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:39:20.724952100Z",
     "start_time": "2025-08-23T08:39:20.692952Z"
    }
   },
   "id": "c29e1f5d411ff69d"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E10_multi_experts_k2_s1337_1755938361 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-013921-E10_multi_experts_k2_s1337_1755938361\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=0.01 GB | reserved=0.02 GB | free=24.36 GB | total=25.77 GB\n",
      "[before E10_multi_experts_k2_s1337_1755938361] alloc=0.01 GB | reserved=0.02 GB | free=24.36 GB | total=25.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d35894297e24082853478643c08aeb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8813047409057617, 0.059696048498153687, 0.05899922549724579]\n",
      "step 10 | loss 9.9201 | lr 4.40e-05 | gates=[0.11869527399539948, 0.29810744524002075] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8175254464149475, 0.08705279231071472, 0.09542176127433777]\n",
      "step 20 | loss 9.8345 | lr 8.40e-05 | gates=[0.1824745535850525, 0.30169153213500977] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9532410502433777, 0.01264512725174427, 0.0341138020157814]\n",
      "step 30 | loss 1.9990 | lr 1.24e-04 | gates=[0.046758927404880524, 0.3054656982421875] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8043654561042786, 0.10710994899272919, 0.08852458745241165]\n",
      "step 40 | loss 7.5039 | lr 1.64e-04 | gates=[0.19563452899456024, 0.3201567530632019] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9803692698478699, 0.01403859630227089, 0.0055921003222465515]\n",
      "step 50 | loss 6.8181 | lr 2.00e-04 | gates=[0.01963069662451744, 0.9948534965515137] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995763301849365, 0.0003516958386171609, 7.191076292656362e-05]\n",
      "step 60 | loss 7.1852 | lr 2.00e-04 | gates=[0.00042360660154372454, 0.9995826482772827] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9971269965171814, 0.002220018533989787, 0.0006529768579639494]\n",
      "step 70 | loss 2.0181 | lr 2.00e-04 | gates=[0.0028729955665767193, 0.07380615174770355] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997627139091492, 0.0002268218668177724, 1.0464411388966255e-05]\n",
      "step 80 | loss 7.8888 | lr 1.99e-04 | gates=[0.00023728628002572805, 0.9897497296333313] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9978407621383667, 0.0016075229505077004, 0.0005517402896657586]\n",
      "step 90 | loss 7.0188 | lr 1.99e-04 | gates=[0.0021592630073428154, 0.16789910197257996] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9993606805801392, 0.0005392094608396292, 0.0001001782511593774]\n",
      "step 100 | loss 6.3741 | lr 1.99e-04 | gates=[0.0006393876392394304, 0.9928337335586548] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9977126121520996, 0.0019456695299595594, 0.00034167582634836435]\n",
      "step 110 | loss 5.8299 | lr 1.98e-04 | gates=[0.0022873454727232456, 0.9931504726409912] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9996491074562073, 0.0003330261097289622, 1.7876658603199758e-05]\n",
      "step 120 | loss 7.2007 | lr 1.97e-04 | gates=[0.00035090273013338447, 0.9980580806732178] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8847100734710693, 0.11508876085281372, 0.00020122560090385377]\n",
      "step 130 | loss 2.6536 | lr 1.96e-04 | gates=[0.11528998613357544, 0.00019872630946338177] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9974671602249146, 0.0021795309148728848, 0.0003532344417180866]\n",
      "step 140 | loss 2.0534 | lr 1.96e-04 | gates=[0.0025327655021101236, 0.0011960340198129416] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991311430931091, 0.00048803078243508935, 0.0003808398323599249]\n",
      "step 150 | loss 9.3527 | lr 1.94e-04 | gates=[0.0008688705856911838, 0.9999251365661621] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999805212020874, 0.0001528956345282495, 4.1844992665573955e-05]\n",
      "step 160 | loss 1.9430 | lr 1.93e-04 | gates=[0.00019474064174573869, 0.00287044420838356] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998724460601807, 8.235056884586811e-05, 4.5198543375590816e-05]\n",
      "step 170 | loss 5.8928 | lr 1.92e-04 | gates=[0.00012754912313539535, 0.9999966025352478] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.997994065284729, 0.0012665148824453354, 0.0007394164567813277]\n",
      "step 180 | loss 5.6767 | lr 1.91e-04 | gates=[0.0020059312228113413, 0.9999935030937195] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997360706329346, 0.0002043841959675774, 5.960041016805917e-05]\n",
      "step 190 | loss 1.9398 | lr 1.89e-04 | gates=[0.00026398460613563657, 0.024981971830129623] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999563455581665, 0.0003608488477766514, 7.56341905798763e-05]\n",
      "step 200 | loss 1.9973 | lr 1.88e-04 | gates=[0.0004364830383565277, 0.026941655203700066] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996020197868347, 0.0003177440376020968, 8.016432548174635e-05]\n",
      "step 210 | loss 1.9174 | lr 1.86e-04 | gates=[0.00039790835580788553, 0.0365692637860775] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9967566132545471, 0.001887264777906239, 0.0013561340747401118]\n",
      "step 220 | loss 6.2559 | lr 1.84e-04 | gates=[0.003243398852646351, 0.9998495578765869] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996169805526733, 0.0003068055084440857, 7.626823207829148e-05]\n",
      "step 230 | loss 1.8475 | lr 1.83e-04 | gates=[0.00038307372597046196, 0.0013933974551036954] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9984358549118042, 0.0009731598547659814, 0.0005909780156798661]\n",
      "step 240 | loss 5.7127 | lr 1.81e-04 | gates=[0.0015641377540305257, 0.9977409243583679] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997696876525879, 0.00016090180724859238, 6.943012704141438e-05]\n",
      "step 250 | loss 6.3623 | lr 1.79e-04 | gates=[0.0002303319051861763, 0.9878573417663574] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9988095760345459, 0.0007598367519676685, 0.00043061975156888366]\n",
      "step 260 | loss 5.6469 | lr 1.77e-04 | gates=[0.001190456561744213, 0.9999938011169434] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9992703199386597, 0.0004907334223389626, 0.00023896939819678664]\n",
      "step 270 | loss 5.4979 | lr 1.74e-04 | gates=[0.0007297028787434101, 0.9999932050704956] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9986366033554077, 0.0009679848444648087, 0.00039543502498418093]\n",
      "step 280 | loss 5.3478 | lr 1.72e-04 | gates=[0.0013634199276566505, 0.9999827146530151] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9987468719482422, 0.0008493323111906648, 0.0004037597682327032]\n",
      "step 290 | loss 5.3151 | lr 1.70e-04 | gates=[0.001253092079423368, 0.9999978542327881] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9984236359596252, 0.0012097737053409219, 0.0003665538097266108]\n",
      "step 300 | loss 1.4855 | lr 1.67e-04 | gates=[0.0015763273695483804, 0.01227947324514389] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9984985589981079, 0.0010981587693095207, 0.0004032450378872454]\n",
      "step 310 | loss 1.6048 | lr 1.65e-04 | gates=[0.0015014037489891052, 0.0017338987672701478] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989646077156067, 0.0008442839025519788, 0.00019115058239549398]\n",
      "step 320 | loss 1.6954 | lr 1.62e-04 | gates=[0.001035434426739812, 0.0037967569660395384] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987536668777466, 0.0010586350690573454, 0.00018778016965370625]\n",
      "step 330 | loss 1.7183 | lr 1.60e-04 | gates=[0.001246415195055306, 0.010948529466986656] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998031854629517, 0.00015530770178884268, 4.156702198088169e-05]\n",
      "step 340 | loss 5.8413 | lr 1.57e-04 | gates=[0.0001968747383216396, 0.991152822971344] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997719526290894, 0.00016477383906021714, 6.328117160592228e-05]\n",
      "step 350 | loss 5.1392 | lr 1.54e-04 | gates=[0.00022805501066613942, 0.9999997615814209] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9987542629241943, 0.0007510261493735015, 0.0004946975386701524]\n",
      "step 360 | loss 5.3885 | lr 1.52e-04 | gates=[0.0012457238044589758, 0.9999985694885254] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9884254932403564, 0.011132271029055119, 0.00044222045107744634]\n",
      "step 370 | loss 1.7236 | lr 1.49e-04 | gates=[0.01157449185848236, 0.014391428790986538] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9613440036773682, 0.03775737062096596, 0.0008986152242869139]\n",
      "step 380 | loss 1.7863 | lr 1.46e-04 | gates=[0.03865598514676094, 0.07489864528179169] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992147088050842, 0.00063707260414958, 0.0001482398365624249]\n",
      "step 390 | loss 5.2680 | lr 1.43e-04 | gates=[0.0007853124989196658, 0.9987399578094482] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9932809472084045, 0.0051751164719462395, 0.0015439067501574755]\n",
      "step 400 | loss 5.3681 | lr 1.40e-04 | gates=[0.006719023454934359, 0.9999982714653015] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992066621780396, 0.000713260262273252, 8.007451833691448e-05]\n",
      "step 410 | loss 5.7455 | lr 1.37e-04 | gates=[0.0007933348533697426, 0.9999993443489075] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8169893026351929, 0.18070757389068604, 0.002303014975041151]\n",
      "step 420 | loss 1.2306 | lr 1.34e-04 | gates=[0.18301057815551758, 0.04875144734978676] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9993630647659302, 0.0004580907116178423, 0.00017880205996334553]\n",
      "step 430 | loss 6.0219 | lr 1.31e-04 | gates=[0.0006368928006850183, 0.999997615814209] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9886476993560791, 0.010580235160887241, 0.0007720168796367943]\n",
      "step 440 | loss 1.6490 | lr 1.27e-04 | gates=[0.011352252215147018, 0.0015046258922666311] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.996178925037384, 0.0033444289583712816, 0.00047663922305218875]\n",
      "step 450 | loss 5.6170 | lr 1.24e-04 | gates=[0.003821068210527301, 0.9987608194351196] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9428240060806274, 0.055044110864400864, 0.0021319265943020582]\n",
      "step 460 | loss 1.6094 | lr 1.21e-04 | gates=[0.05717603489756584, 0.03063318319618702] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9827972650527954, 0.014965355396270752, 0.00223741726949811]\n",
      "step 470 | loss 5.2038 | lr 1.18e-04 | gates=[0.017202772200107574, 0.9998391270637512] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9977760314941406, 0.00180193898268044, 0.00042200484313070774]\n",
      "step 480 | loss 5.1681 | lr 1.14e-04 | gates=[0.0022239438258111477, 0.9999745488166809] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9976076483726501, 0.0022244155406951904, 0.0001679273264016956]\n",
      "step 490 | loss 5.0356 | lr 1.11e-04 | gates=[0.002392343245446682, 0.999911904335022] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9977865219116211, 0.001848109532147646, 0.00036540915607474744]\n",
      "step 500 | loss 5.5438 | lr 1.08e-04 | gates=[0.002213518600910902, 0.9999987483024597] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9983403086662292, 0.0012169787660241127, 0.00044274626998230815]\n",
      "step 510 | loss 5.4857 | lr 1.05e-04 | gates=[0.00165972497779876, 0.9999884366989136] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9986757040023804, 0.0012212784495204687, 0.00010305044270353392]\n",
      "step 520 | loss 5.1504 | lr 1.01e-04 | gates=[0.001324328826740384, 0.9999934434890747] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9986487627029419, 0.0007824008935131133, 0.0005687588127329946]\n",
      "step 530 | loss 1.6009 | lr 9.80e-05 | gates=[0.0013511597644537687, 0.04550345987081528] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986985921859741, 0.0004562319372780621, 0.0008451315807178617]\n",
      "step 540 | loss 1.3961 | lr 9.47e-05 | gates=[0.0013013634597882628, 0.28919461369514465] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995328187942505, 0.0004157337243668735, 5.1426228310447186e-05]\n",
      "step 550 | loss 5.5051 | lr 9.14e-05 | gates=[0.00046715993084944785, 0.9995201826095581] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999402642250061, 0.0002411989844404161, 0.0003561781195458025]\n",
      "step 560 | loss 5.5155 | lr 8.81e-05 | gates=[0.0005973770748823881, 0.9999995231628418] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9990171194076538, 0.0005549836787395179, 0.0004279180720914155]\n",
      "step 570 | loss 4.9039 | lr 8.48e-05 | gates=[0.000982901779934764, 0.9999988675117493] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9989140033721924, 0.0004306777846068144, 0.0006553005077876151]\n",
      "step 580 | loss 1.2168 | lr 8.16e-05 | gates=[0.0010859783506020904, 0.41745123267173767] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989078640937805, 0.00043786870082840323, 0.0006542432820424438]\n",
      "step 590 | loss 1.4794 | lr 7.83e-05 | gates=[0.001092111924663186, 0.22523103654384613] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989397525787354, 0.00042046711314469576, 0.0006398751283995807]\n",
      "step 600 | loss 1.5688 | lr 7.51e-05 | gates=[0.0010603421833366156, 0.20508533716201782] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985698461532593, 0.0005606379127129912, 0.0008695466676726937]\n",
      "step 610 | loss 5.0429 | lr 7.19e-05 | gates=[0.0014301846968010068, 0.9999710321426392] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9991228580474854, 0.00029930000891909003, 0.0005778676131740212]\n",
      "step 620 | loss 1.4988 | lr 6.88e-05 | gates=[0.0008771676803007722, 0.2963302433490753] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997100830078125, 0.00018354148778598756, 0.00010631374607328326]\n",
      "step 630 | loss 5.0661 | lr 6.57e-05 | gates=[0.0002898552338592708, 0.9999872446060181] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9989948272705078, 0.0002241267211502418, 0.0007810810348019004]\n",
      "step 640 | loss 5.4812 | lr 6.26e-05 | gates=[0.0010052077705040574, 0.9999862313270569] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993629455566406, 0.00015623224317096174, 0.0004807961522601545]\n",
      "step 650 | loss 5.5149 | lr 5.95e-05 | gates=[0.0006370283663272858, 0.9999738931655884] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999521017074585, 0.00024199479958042502, 0.00023702489852439612]\n",
      "step 660 | loss 5.5221 | lr 5.65e-05 | gates=[0.0004790196835529059, 0.9999206066131592] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9983274340629578, 0.00037103384966030717, 0.0013014768483117223]\n",
      "step 670 | loss 1.6604 | lr 5.36e-05 | gates=[0.0016725106397643685, 0.6354338526725769] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999136924743652, 4.19122188759502e-05, 4.4411928683985025e-05]\n",
      "step 680 | loss 5.0315 | lr 5.07e-05 | gates=[8.632415119791403e-05, 0.9998964071273804] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999947726726532, 4.719573917100206e-05, 5.061700903752353e-06]\n",
      "step 690 | loss 5.0358 | lr 4.78e-05 | gates=[5.2257440984249115e-05, 0.9998061060905457] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997390508651733, 0.00014189048670232296, 0.00011903568520210683]\n",
      "step 700 | loss 4.9859 | lr 4.50e-05 | gates=[0.0002609261719044298, 0.9998679161071777] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9985396862030029, 0.00040231202729046345, 0.001057907473295927]\n",
      "step 710 | loss 1.1748 | lr 4.23e-05 | gates=[0.0014602196170017123, 0.11666151881217957] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994906783103943, 0.00023705491912551224, 0.0002722418576013297]\n",
      "step 720 | loss 5.5647 | lr 3.96e-05 | gates=[0.0005092968349345028, 0.9998385906219482] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995441436767578, 0.00027166929794475436, 0.0001841956691350788]\n",
      "step 730 | loss 4.9806 | lr 3.70e-05 | gates=[0.00045586490887217224, 0.9998193979263306] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9988376498222351, 0.0008152109221555293, 0.00034712982596829534]\n",
      "step 740 | loss 4.8676 | lr 3.45e-05 | gates=[0.0011623407481238246, 0.9999120831489563] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9977411031723022, 0.001471212599426508, 0.0007876820745877922]\n",
      "step 750 | loss 4.9092 | lr 3.20e-05 | gates=[0.0022588944993913174, 0.999284565448761] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998452663421631, 7.69645776017569e-05, 7.776182610541582e-05]\n",
      "step 760 | loss 5.0714 | lr 2.96e-05 | gates=[0.0001547263964312151, 0.9997543096542358] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997125864028931, 0.00021668843692168593, 7.074698805809021e-05]\n",
      "step 770 | loss 4.9075 | lr 2.73e-05 | gates=[0.00028743542497977614, 0.999824047088623] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994031190872192, 0.00030909659108147025, 0.00028780868160538375]\n",
      "step 780 | loss 5.4979 | lr 2.51e-05 | gates=[0.0005969053017906845, 0.9998767375946045] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.998645007610321, 0.00025876975269056857, 0.001096271676942706]\n",
      "step 790 | loss 1.3526 | lr 2.29e-05 | gates=[0.001355041516944766, 0.23035454750061035] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9966493248939514, 0.002097941702231765, 0.0012527571525424719]\n",
      "step 800 | loss 4.6586 | lr 2.09e-05 | gates=[0.0033506990876048803, 0.9998606443405151] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998500347137451, 0.00012892046652268618, 2.1010027921875007e-05]\n",
      "step 810 | loss 4.9330 | lr 2.00e-05 | gates=[0.0001499304926255718, 0.9997256994247437] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9971016645431519, 0.0007793395780026913, 0.0021190231200307608]\n",
      "step 820 | loss 1.2232 | lr 2.00e-05 | gates=[0.002898362698033452, 0.27013149857521057] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.996387779712677, 0.0008522570133209229, 0.002759984228760004]\n",
      "step 830 | loss 1.3467 | lr 2.00e-05 | gates=[0.003612241242080927, 0.42317208647727966] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9972516298294067, 0.000688726722728461, 0.002059692982584238]\n",
      "step 840 | loss 1.4083 | lr 2.00e-05 | gates=[0.0027484195306897163, 0.26593759655952454] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996960163116455, 0.00020826241234317422, 9.566330118104815e-05]\n",
      "step 850 | loss 4.9712 | lr 2.00e-05 | gates=[0.00030392574262805283, 0.9998269081115723] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994674921035767, 0.00023320227046497166, 0.00029925809940323234]\n",
      "step 860 | loss 5.4547 | lr 2.00e-05 | gates=[0.0005324603407643735, 0.9999110102653503] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9992983341217041, 0.0001923144154716283, 0.000509369361680001]\n",
      "step 870 | loss 5.4846 | lr 2.00e-05 | gates=[0.0007016837480477989, 0.9998626112937927] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9976761341094971, 0.0003419761487748474, 0.001981843262910843]\n",
      "step 880 | loss 1.5057 | lr 2.00e-05 | gates=[0.002323819324374199, 0.4761941432952881] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987369775772095, 0.0009817607933655381, 0.0002813067985698581]\n",
      "step 890 | loss 4.9606 | lr 2.00e-05 | gates=[0.0012630678247660398, 0.9994409084320068] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997978210449219, 8.730970876058564e-05, 0.0001148564915638417]\n",
      "step 900 | loss 4.9655 | lr 2.00e-05 | gates=[0.00020216619304846972, 0.9997025728225708] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997149109840393, 0.00016967383271548897, 0.0001154110359493643]\n",
      "step 910 | loss 5.6946 | lr 2.00e-05 | gates=[0.0002850848832167685, 0.9997856616973877] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996639490127563, 0.00010154952178709209, 0.0002345173415960744]\n",
      "step 920 | loss 5.0780 | lr 2.00e-05 | gates=[0.00033606684883125126, 0.9996854662895203] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9956414103507996, 0.000882737513165921, 0.003475929144769907]\n",
      "step 930 | loss 1.3682 | lr 2.00e-05 | gates=[0.004358666483312845, 0.3808656334877014] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998617768287659, 6.56841802992858e-05, 7.250021735671908e-05]\n",
      "step 940 | loss 5.4823 | lr 2.00e-05 | gates=[0.00013818439038004726, 0.9997177124023438] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995143413543701, 8.828881982481107e-05, 0.000397346680983901]\n",
      "step 950 | loss 4.9760 | lr 2.00e-05 | gates=[0.00048563547898083925, 0.9996611475944519] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9975548386573792, 0.0003347495512571186, 0.0021104062907397747]\n",
      "step 960 | loss 1.4635 | lr 2.00e-05 | gates=[0.002445155754685402, 0.19699859619140625] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985601902008057, 0.0007873042486608028, 0.0006525323842652142]\n",
      "step 970 | loss 4.7289 | lr 2.00e-05 | gates=[0.0014398365747183561, 0.9993585348129272] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9963937997817993, 0.00048820525989867747, 0.003117927582934499]\n",
      "step 980 | loss 1.6662 | lr 2.00e-05 | gates=[0.0036061324644833803, 0.17402128875255585] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996638894081116, 0.0001306281192228198, 0.00020546047016978264]\n",
      "step 990 | loss 4.9842 | lr 2.00e-05 | gates=[0.00033608858939260244, 0.9997652769088745] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9959757328033447, 0.0026617764960974455, 0.001362512120977044]\n",
      "step 1000 | loss 4.8202 | lr 2.00e-05 | gates=[0.00402428861707449, 0.9996744394302368] | mix=repeat\n",
      "[after  E10_multi_experts_k2_s1337_1755938361] alloc=10.70 GB | reserved=32.21 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.70 GB | reserved=10.74 GB | free=13.62 GB | total=25.77 GB\n",
      "\n",
      "=== E11a_tiered_many_narrow_s1337_1755941870 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-023750-E11a_tiered_many_narrow_s1337_1755941870\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E11a_tiered_many_narrow_s1337_1755941870] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9794a9b09a6640eab659629fbb21dabd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.827828586101532, 0.17217141389846802]\n",
      "step 10 | loss 9.2175 | lr 4.40e-05 | gates=[0.17217141389846802, 0.41098305583000183] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9850215315818787, 0.014978446997702122]\n",
      "step 20 | loss 1.8815 | lr 8.40e-05 | gates=[0.014978446066379547, 0.11323383450508118] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.12222029268741608, 0.8777797222137451]\n",
      "step 30 | loss 5.9493 | lr 1.24e-04 | gates=[0.8777797222137451, 0.2587236762046814] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8507078886032104, 0.14929214119911194]\n",
      "step 40 | loss 6.1915 | lr 1.64e-04 | gates=[0.14929214119911194, 0.9159359931945801] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8856296539306641, 0.11437037587165833]\n",
      "step 50 | loss 6.2415 | lr 2.00e-04 | gates=[0.11437037587165833, 0.5090433955192566] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9974364638328552, 0.0025634909979999065]\n",
      "step 60 | loss 2.5038 | lr 2.00e-04 | gates=[0.00256349123083055, 0.5222287774085999] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998427629470825, 0.00015720041119493544]\n",
      "step 70 | loss 2.1537 | lr 2.00e-04 | gates=[0.00015720041119493544, 0.05039095878601074] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9534137845039368, 0.046586208045482635]\n",
      "step 80 | loss 6.1835 | lr 1.99e-04 | gates=[0.046586208045482635, 0.999866783618927] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9949443340301514, 0.005055644549429417]\n",
      "step 90 | loss 6.6931 | lr 1.99e-04 | gates=[0.005055644549429417, 0.9944722652435303] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9854469299316406, 0.014553110115230083]\n",
      "step 100 | loss 6.0016 | lr 1.99e-04 | gates=[0.014553111046552658, 0.9794572591781616] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9965078234672546, 0.003492217743769288]\n",
      "step 110 | loss 8.0599 | lr 1.98e-04 | gates=[0.003492217743769288, 0.00010647660383256152] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9938181042671204, 0.0061818696558475494]\n",
      "step 120 | loss 5.5179 | lr 1.97e-04 | gates=[0.006181870587170124, 0.9970830678939819] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9992659687995911, 0.000734082655981183]\n",
      "step 130 | loss 2.3362 | lr 1.96e-04 | gates=[0.000734082655981183, 0.00030126763158477843] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.991660475730896, 0.00833949539810419]\n",
      "step 140 | loss 7.5368 | lr 1.96e-04 | gates=[0.00833949539810419, 0.9998482465744019] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9772225022315979, 0.022777436301112175]\n",
      "step 150 | loss 1.8314 | lr 1.94e-04 | gates=[0.022777438163757324, 0.13120155036449432] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9829845428466797, 0.01701539196074009]\n",
      "step 160 | loss 2.6384 | lr 1.93e-04 | gates=[0.01701539196074009, 0.2474268525838852] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9921913146972656, 0.00780858239158988]\n",
      "step 170 | loss 2.4585 | lr 1.92e-04 | gates=[0.007808581925928593, 0.025641417130827904] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9956002235412598, 0.004399773199111223]\n",
      "step 180 | loss 3.3654 | lr 1.91e-04 | gates=[0.004399773199111223, 0.001691134530119598] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9827308058738708, 0.017269151285290718]\n",
      "step 190 | loss 2.6330 | lr 1.89e-04 | gates=[0.017269151285290718, 0.005471042823046446] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9724266529083252, 0.027573391795158386]\n",
      "step 200 | loss 5.8428 | lr 1.88e-04 | gates=[0.027573395520448685, 0.8600056171417236] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9927550554275513, 0.007244903594255447]\n",
      "step 210 | loss 5.7636 | lr 1.86e-04 | gates=[0.007244904525578022, 0.9985334873199463] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9826531410217285, 0.017346832901239395]\n",
      "step 220 | loss 5.7030 | lr 1.84e-04 | gates=[0.017346832901239395, 0.9194017648696899] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9825669527053833, 0.017433006316423416]\n",
      "step 230 | loss 6.3411 | lr 1.83e-04 | gates=[0.017433006316423416, 0.29368072748184204] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9784086346626282, 0.021591348573565483]\n",
      "step 240 | loss 6.4744 | lr 1.81e-04 | gates=[0.021591350436210632, 0.3260747790336609] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9814525246620178, 0.018547512590885162]\n",
      "step 250 | loss 1.7035 | lr 1.79e-04 | gates=[0.018547512590885162, 0.14262814819812775] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9942268133163452, 0.00577318761497736]\n",
      "step 260 | loss 5.0795 | lr 1.77e-04 | gates=[0.00577318761497736, 0.9851711988449097] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.990700364112854, 0.009299619123339653]\n",
      "step 270 | loss 5.8552 | lr 1.74e-04 | gates=[0.009299618192017078, 0.9183329939842224] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9913305640220642, 0.008669490925967693]\n",
      "step 280 | loss 1.7663 | lr 1.72e-04 | gates=[0.008669491857290268, 0.0014887594152241945] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.49241843819618225, 0.5075816512107849]\n",
      "step 290 | loss 2.6194 | lr 1.70e-04 | gates=[0.5075816512107849, 0.01845584064722061] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9888337254524231, 0.011166292242705822]\n",
      "step 300 | loss 5.1337 | lr 1.67e-04 | gates=[0.011166293174028397, 0.934594988822937] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9985131621360779, 0.0014867725549265742]\n",
      "step 310 | loss 1.5541 | lr 1.65e-04 | gates=[0.0014867725549265742, 0.18275700509548187] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9964500069618225, 0.003549956250935793]\n",
      "step 320 | loss 1.1199 | lr 1.62e-04 | gates=[0.003549956250935793, 0.558883547782898] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9932901263237, 0.006709866225719452]\n",
      "step 330 | loss 5.6862 | lr 1.60e-04 | gates=[0.006709866691380739, 0.9175140857696533] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9938369989395142, 0.006162955891340971]\n",
      "step 340 | loss 6.0465 | lr 1.57e-04 | gates=[0.006162955425679684, 0.9993455410003662] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9797824621200562, 0.02021756023168564]\n",
      "step 350 | loss 6.6739 | lr 1.54e-04 | gates=[0.02021756023168564, 0.9989571571350098] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996728897094727, 0.00032703904435038567]\n",
      "step 360 | loss 2.0204 | lr 1.52e-04 | gates=[0.00032703904435038567, 0.4506423771381378] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995562434196472, 0.00044371176045387983]\n",
      "step 370 | loss 1.5329 | lr 1.49e-04 | gates=[0.00044371176045387983, 0.02129586972296238] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9934991598129272, 0.006500807590782642]\n",
      "step 380 | loss 5.7025 | lr 1.46e-04 | gates=[0.006500808987766504, 0.9993288516998291] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9889997243881226, 0.011000283062458038]\n",
      "step 390 | loss 5.6874 | lr 1.43e-04 | gates=[0.011000282131135464, 0.9999426603317261] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9913848042488098, 0.00861523486673832]\n",
      "step 400 | loss 5.5040 | lr 1.40e-04 | gates=[0.00861523486673832, 0.9993524551391602] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9907779693603516, 0.009222050197422504]\n",
      "step 410 | loss 4.9603 | lr 1.37e-04 | gates=[0.00922204926609993, 0.9999346733093262] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9991223812103271, 0.0008776048198342323]\n",
      "step 420 | loss 1.6382 | lr 1.34e-04 | gates=[0.0008776048198342323, 0.3273252844810486] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.985443115234375, 0.014556878246366978]\n",
      "step 430 | loss 5.5663 | lr 1.31e-04 | gates=[0.014556878246366978, 0.9993125200271606] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9929213523864746, 0.0070786187425255775]\n",
      "step 440 | loss 5.1717 | lr 1.27e-04 | gates=[0.00707861827686429, 0.9993784427642822] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9755969047546387, 0.024403143674135208]\n",
      "step 450 | loss 5.5589 | lr 1.24e-04 | gates=[0.02440314181149006, 0.9951980113983154] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998024702072144, 0.00019755534594878554]\n",
      "step 460 | loss 1.8230 | lr 1.21e-04 | gates=[0.00019755534594878554, 0.07388125360012054] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9774883985519409, 0.02251157909631729]\n",
      "step 470 | loss 5.0981 | lr 1.18e-04 | gates=[0.02251158095896244, 0.9733895063400269] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9920667409896851, 0.007933307439088821]\n",
      "step 480 | loss 5.1854 | lr 1.14e-04 | gates=[0.007933307439088821, 0.9929274320602417] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9981863498687744, 0.0018136287108063698]\n",
      "step 490 | loss 1.5358 | lr 1.11e-04 | gates=[0.001813628594391048, 0.26449665427207947] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9917488098144531, 0.008251175284385681]\n",
      "step 500 | loss 5.5426 | lr 1.08e-04 | gates=[0.008251175284385681, 0.9971250295639038] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9800237417221069, 0.019976241514086723]\n",
      "step 510 | loss 5.4101 | lr 1.05e-04 | gates=[0.019976241514086723, 0.9920103549957275] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9950472116470337, 0.004952688701450825]\n",
      "step 520 | loss 1.6749 | lr 1.01e-04 | gates=[0.00495268777012825, 0.24553285539150238] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9817468523979187, 0.018253136426210403]\n",
      "step 530 | loss 5.5328 | lr 9.80e-05 | gates=[0.018253136426210403, 0.8010483384132385] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9806020259857178, 0.019397947937250137]\n",
      "step 540 | loss 5.4802 | lr 9.47e-05 | gates=[0.019397947937250137, 0.9785671234130859] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9902904033660889, 0.009709572419524193]\n",
      "step 550 | loss 5.0317 | lr 9.14e-05 | gates=[0.009709573350846767, 0.8562394976615906] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.975830078125, 0.024169955402612686]\n",
      "step 560 | loss 5.5062 | lr 8.81e-05 | gates=[0.024169957265257835, 0.9875128269195557] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9915074706077576, 0.008492509834468365]\n",
      "step 570 | loss 5.0141 | lr 8.48e-05 | gates=[0.00849250890314579, 0.9952518343925476] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9841895699501038, 0.015810389071702957]\n",
      "step 580 | loss 5.5356 | lr 8.16e-05 | gates=[0.015810389071702957, 0.9783718585968018] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9983442425727844, 0.0016557242488488555]\n",
      "step 590 | loss 1.5377 | lr 7.83e-05 | gates=[0.0016557242488488555, 0.03876293823122978] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9908385276794434, 0.009161490015685558]\n",
      "step 600 | loss 5.4776 | lr 7.51e-05 | gates=[0.009161490015685558, 0.8968846797943115] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9919754862785339, 0.00802448857575655]\n",
      "step 610 | loss 1.4223 | lr 7.19e-05 | gates=[0.00802448857575655, 0.23919080197811127] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9889905452728271, 0.011009427718818188]\n",
      "step 620 | loss 5.5458 | lr 6.88e-05 | gates=[0.011009427718818188, 0.9198204874992371] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9941971898078918, 0.005802782252430916]\n",
      "step 630 | loss 5.5520 | lr 6.57e-05 | gates=[0.005802782252430916, 0.9621096253395081] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.998643696308136, 0.0013562894891947508]\n",
      "step 640 | loss 1.5466 | lr 6.26e-05 | gates=[0.0013562894891947508, 0.2190120667219162] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9946417808532715, 0.00535823218524456]\n",
      "step 650 | loss 5.0035 | lr 5.95e-05 | gates=[0.005358231719583273, 0.8888365030288696] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9943236112594604, 0.00567637337371707]\n",
      "step 660 | loss 4.9689 | lr 5.65e-05 | gates=[0.00567637337371707, 0.9472494125366211] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.991193413734436, 0.008806639350950718]\n",
      "step 670 | loss 4.9188 | lr 5.36e-05 | gates=[0.008806640282273293, 0.9450193643569946] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9934457540512085, 0.006554215215146542]\n",
      "step 680 | loss 4.9153 | lr 5.07e-05 | gates=[0.006554215215146542, 0.9004313945770264] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996408820152283, 0.000359041994670406]\n",
      "step 690 | loss 1.5195 | lr 4.78e-05 | gates=[0.000359041994670406, 0.12354672700166702] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9969125390052795, 0.003087378339841962]\n",
      "step 700 | loss 1.6755 | lr 4.50e-05 | gates=[0.003087378339841962, 0.23477703332901] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990716576576233, 0.0009283961844630539]\n",
      "step 710 | loss 1.1746 | lr 4.23e-05 | gates=[0.0009283961844630539, 0.24655118584632874] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9977712631225586, 0.0022287664469331503]\n",
      "step 720 | loss 1.5109 | lr 3.96e-05 | gates=[0.002228766679763794, 0.23952876031398773] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9257359504699707, 0.0742640495300293]\n",
      "step 730 | loss 5.0715 | lr 3.70e-05 | gates=[0.0742640420794487, 0.8722848296165466] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9940168857574463, 0.005983153358101845]\n",
      "step 740 | loss 4.8459 | lr 3.45e-05 | gates=[0.0059831528924405575, 0.9592143297195435] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9982732534408569, 0.0017266797367483377]\n",
      "step 750 | loss 1.3806 | lr 3.20e-05 | gates=[0.0017266797367483377, 0.2871016263961792] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.995036780834198, 0.004963234066963196]\n",
      "step 760 | loss 5.4708 | lr 2.96e-05 | gates=[0.004963234066963196, 0.9057646989822388] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9910321235656738, 0.008967891335487366]\n",
      "step 770 | loss 5.5053 | lr 2.73e-05 | gates=[0.008967890404164791, 0.9616842269897461] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9931005239486694, 0.006899453699588776]\n",
      "step 780 | loss 4.8562 | lr 2.51e-05 | gates=[0.006899453699588776, 0.9502398371696472] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9775265455245972, 0.02247345820069313]\n",
      "step 790 | loss 4.9514 | lr 2.29e-05 | gates=[0.02247345820069313, 0.8745126724243164] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9637871980667114, 0.03621279075741768]\n",
      "step 800 | loss 4.9147 | lr 2.09e-05 | gates=[0.03621279448270798, 0.8710569143295288] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9979646801948547, 0.002035180339589715]\n",
      "step 810 | loss 1.3772 | lr 2.00e-05 | gates=[0.002035180339589715, 0.23173542320728302] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9936678409576416, 0.0063321515917778015]\n",
      "step 820 | loss 5.4493 | lr 2.00e-05 | gates=[0.0063321515917778015, 0.9572527408599854] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.962933361530304, 0.03706663101911545]\n",
      "step 830 | loss 4.9736 | lr 2.00e-05 | gates=[0.03706663101911545, 0.928687334060669] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9774978756904602, 0.022502150386571884]\n",
      "step 840 | loss 5.4683 | lr 2.00e-05 | gates=[0.022502150386571884, 0.9104430675506592] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9982531666755676, 0.0017468807054683566]\n",
      "step 850 | loss 1.4320 | lr 2.00e-05 | gates=[0.0017468807054683566, 0.31408557295799255] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9901946187019348, 0.009805392473936081]\n",
      "step 860 | loss 4.8786 | lr 2.00e-05 | gates=[0.009805393405258656, 0.9436765313148499] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9958520531654358, 0.004147984087467194]\n",
      "step 870 | loss 5.4485 | lr 2.00e-05 | gates=[0.004147984087467194, 0.9684610366821289] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9921600222587585, 0.007839984260499477]\n",
      "step 880 | loss 4.8409 | lr 2.00e-05 | gates=[0.007839984260499477, 0.9903494119644165] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9520593285560608, 0.04794067144393921]\n",
      "step 890 | loss 5.1329 | lr 2.00e-05 | gates=[0.04794067516922951, 0.9717919826507568] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9358728528022766, 0.06412713974714279]\n",
      "step 900 | loss 4.8100 | lr 2.00e-05 | gates=[0.06412714719772339, 0.8883997797966003] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9522002935409546, 0.04779965057969093]\n",
      "step 910 | loss 4.9192 | lr 2.00e-05 | gates=[0.04779965057969093, 0.8133330941200256] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9969273805618286, 0.0030725179240107536]\n",
      "step 920 | loss 1.1439 | lr 2.00e-05 | gates=[0.0030725179240107536, 0.2190271019935608] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985674619674683, 0.0014325100928544998]\n",
      "step 930 | loss 1.4092 | lr 2.00e-05 | gates=[0.0014325100928544998, 0.19123545289039612] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9908287525177002, 0.009171191602945328]\n",
      "step 940 | loss 1.3458 | lr 2.00e-05 | gates=[0.009171191602945328, 0.3061133623123169] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.989189863204956, 0.010810159146785736]\n",
      "step 950 | loss 5.0425 | lr 2.00e-05 | gates=[0.010810159146785736, 0.919174075126648] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9899640679359436, 0.010035930201411247]\n",
      "step 960 | loss 4.8689 | lr 2.00e-05 | gates=[0.010035932064056396, 0.8925696611404419] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9973596930503845, 0.0026403155643492937]\n",
      "step 970 | loss 4.8762 | lr 2.00e-05 | gates=[0.00264031533151865, 0.9512474536895752] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9926281571388245, 0.007371803745627403]\n",
      "step 980 | loss 5.4032 | lr 2.00e-05 | gates=[0.007371804211288691, 0.9528213739395142] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9968447685241699, 0.003155194688588381]\n",
      "step 990 | loss 1.4265 | lr 2.00e-05 | gates=[0.003155194688588381, 0.27709490060806274] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9957416653633118, 0.004258348606526852]\n",
      "step 1000 | loss 4.9262 | lr 2.00e-05 | gates=[0.004258347675204277, 0.9169564843177795] | mix=nback\n",
      "[after  E11a_tiered_many_narrow_s1337_1755941870] alloc=9.63 GB | reserved=26.57 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.63 GB | reserved=9.65 GB | free=14.71 GB | total=25.77 GB\n",
      "\n",
      "=== E11b_tiered_deeper_late_s1337_1755943893 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-031133-E11b_tiered_deeper_late_s1337_1755943893\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E11b_tiered_deeper_late_s1337_1755943893] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d7b2fe7f06e4f65a25ff87d38c71f3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8276891708374023, 0.17231079936027527]\n",
      "step 10 | loss 9.2188 | lr 4.40e-05 | gates=[0.17231079936027527, 0.410535991191864] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9848330020904541, 0.015167055651545525]\n",
      "step 20 | loss 1.8858 | lr 8.40e-05 | gates=[0.015167055651545525, 0.11637142300605774] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.1882839798927307, 0.8117159605026245]\n",
      "step 30 | loss 5.8030 | lr 1.24e-04 | gates=[0.8117160201072693, 0.3568115234375] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.700255274772644, 0.29974478483200073]\n",
      "step 40 | loss 6.1163 | lr 1.64e-04 | gates=[0.29974478483200073, 0.8269035816192627] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.883404016494751, 0.11659602075815201]\n",
      "step 50 | loss 5.7706 | lr 2.00e-04 | gates=[0.11659602075815201, 0.9920541048049927] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9125295281410217, 0.08747048676013947]\n",
      "step 60 | loss 4.3830 | lr 2.00e-04 | gates=[0.08747047930955887, 0.13603438436985016] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9892066717147827, 0.010793237015604973]\n",
      "step 70 | loss 2.4304 | lr 2.00e-04 | gates=[0.010793237015604973, 0.3122113347053528] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.98465895652771, 0.015341034159064293]\n",
      "step 80 | loss 7.3375 | lr 1.99e-04 | gates=[0.015341034159064293, 0.9928675293922424] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9991172552108765, 0.0008827671408653259]\n",
      "step 90 | loss 6.4375 | lr 1.99e-04 | gates=[0.000882767082657665, 0.9985933303833008] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9953614473342896, 0.0046386076137423515]\n",
      "step 100 | loss 5.9428 | lr 1.99e-04 | gates=[0.0046386076137423515, 0.998466968536377] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9983695149421692, 0.0016304535092785954]\n",
      "step 110 | loss 7.0377 | lr 1.98e-04 | gates=[0.0016304535092785954, 0.9130457639694214] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997984170913696, 0.00020160747226327658]\n",
      "step 120 | loss 5.8380 | lr 1.97e-04 | gates=[0.00020160747226327658, 0.7379480600357056] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9991430044174194, 0.0008570572244934738]\n",
      "step 130 | loss 2.4956 | lr 1.96e-04 | gates=[0.0008570571662858129, 3.211806188119226e-06] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9970929622650146, 0.0029071250464767218]\n",
      "step 140 | loss 7.0159 | lr 1.96e-04 | gates=[0.0029071250464767218, 0.9981038570404053] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9778580665588379, 0.022141996771097183]\n",
      "step 150 | loss 1.8334 | lr 1.94e-04 | gates=[0.022141998633742332, 0.014046574011445045] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7387767434120178, 0.26122331619262695]\n",
      "step 160 | loss 2.1189 | lr 1.93e-04 | gates=[0.26122334599494934, 0.09961070865392685] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999297261238098, 7.028158142929897e-05]\n",
      "step 170 | loss 1.9515 | lr 1.92e-04 | gates=[7.028158142929897e-05, 0.007702523376792669] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999890923500061, 1.081287791748764e-05]\n",
      "step 180 | loss 1.9210 | lr 1.91e-04 | gates=[1.0812878826982342e-05, 0.005118752364069223] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999933838844299, 6.5784047365013976e-06]\n",
      "step 190 | loss 2.2728 | lr 1.89e-04 | gates=[6.578404281754047e-06, 0.21392647922039032] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996907114982605, 0.0003093039558734745]\n",
      "step 200 | loss 5.8587 | lr 1.88e-04 | gates=[0.00030930398497730494, 0.9927034378051758] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997247457504272, 0.000275296886684373]\n",
      "step 210 | loss 5.7452 | lr 1.86e-04 | gates=[0.000275296886684373, 0.9999955892562866] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995782971382141, 0.0004217068781144917]\n",
      "step 220 | loss 5.8145 | lr 1.84e-04 | gates=[0.0004217068781144917, 0.9998972415924072] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997342228889465, 0.00026582018472254276]\n",
      "step 230 | loss 6.5882 | lr 1.83e-04 | gates=[0.00026582018472254276, 0.999756395816803] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9986412525177002, 0.0013587502762675285]\n",
      "step 240 | loss 5.5436 | lr 1.81e-04 | gates=[0.0013587501598522067, 0.9929633140563965] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999808073043823, 1.925603464769665e-05]\n",
      "step 250 | loss 1.7154 | lr 1.79e-04 | gates=[1.9256036466686055e-05, 0.07679203897714615] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999136924743652, 8.630474621895701e-05]\n",
      "step 260 | loss 5.3107 | lr 1.77e-04 | gates=[8.630473894299939e-05, 0.9984246492385864] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999589741230011, 0.000410267646657303]\n",
      "step 270 | loss 5.5959 | lr 1.74e-04 | gates=[0.00041026758844964206, 0.9991483092308044] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999978244304657, 2.1859652406419627e-05]\n",
      "step 280 | loss 1.6959 | lr 1.72e-04 | gates=[2.185964876844082e-05, 0.22522537410259247] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999967217445374, 3.3920407531695673e-06]\n",
      "step 290 | loss 2.4569 | lr 1.70e-04 | gates=[3.3920407531695673e-06, 0.6276565194129944] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997954368591309, 0.00020451621094252914]\n",
      "step 300 | loss 5.0932 | lr 1.67e-04 | gates=[0.00020451622549444437, 0.9936643838882446] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998817443847656, 0.00011824355169665068]\n",
      "step 310 | loss 1.5415 | lr 1.65e-04 | gates=[0.00011824355169665068, 0.037505969405174255] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998489022254944, 0.0001511717273388058]\n",
      "step 320 | loss 1.1188 | lr 1.62e-04 | gates=[0.0001511717273388058, 0.10853628814220428] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998701810836792, 0.0001298768474953249]\n",
      "step 330 | loss 5.9595 | lr 1.60e-04 | gates=[0.00012987687659915537, 0.3609291613101959] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999260902404785, 7.396362343570217e-05]\n",
      "step 340 | loss 6.9123 | lr 1.57e-04 | gates=[7.396362343570217e-05, 0.9989453554153442] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990338683128357, 0.0009661566582508385]\n",
      "step 350 | loss 5.7512 | lr 1.54e-04 | gates=[0.0009661567164584994, 0.9989779591560364] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999616146087646, 3.8366910303011537e-05]\n",
      "step 360 | loss 1.6679 | lr 1.52e-04 | gates=[3.836691757896915e-05, 0.010928765870630741] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999476075172424, 5.240442987997085e-05]\n",
      "step 370 | loss 1.4856 | lr 1.49e-04 | gates=[5.240442987997085e-05, 0.026923585683107376] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999468326568604, 5.320884520187974e-05]\n",
      "step 380 | loss 5.4760 | lr 1.46e-04 | gates=[5.3208852477837354e-05, 0.9995802044868469] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9972183704376221, 0.0027816598303616047]\n",
      "step 390 | loss 5.5836 | lr 1.43e-04 | gates=[0.0027816598303616047, 0.9997028112411499] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999916553497314, 8.341199645656161e-06]\n",
      "step 400 | loss 5.5698 | lr 1.40e-04 | gates=[8.341201464645565e-06, 0.9994040727615356] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996650815010071, 0.0003349179751239717]\n",
      "step 410 | loss 5.1687 | lr 1.37e-04 | gates=[0.0003349180333316326, 0.9993950724601746] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999790787696838, 2.0889152438030578e-05]\n",
      "step 420 | loss 1.6297 | lr 1.34e-04 | gates=[2.0889152438030578e-05, 0.3527657985687256] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9976522922515869, 0.0023477422073483467]\n",
      "step 430 | loss 5.5927 | lr 1.31e-04 | gates=[0.002347741974517703, 0.9961928129196167] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9990095496177673, 0.0009904707549139857]\n",
      "step 440 | loss 5.0679 | lr 1.27e-04 | gates=[0.0009904707549139857, 0.9580761790275574] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9868344068527222, 0.013165654614567757]\n",
      "step 450 | loss 5.5407 | lr 1.24e-04 | gates=[0.013165654614567757, 0.9908517003059387] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999922513961792, 7.851864211261272e-06]\n",
      "step 460 | loss 1.7971 | lr 1.21e-04 | gates=[7.851864211261272e-06, 0.1704675853252411] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996075630187988, 0.0003923990298062563]\n",
      "step 470 | loss 5.0774 | lr 1.18e-04 | gates=[0.0003923990298062563, 0.9980217218399048] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9973264932632446, 0.0026735016144812107]\n",
      "step 480 | loss 4.9749 | lr 1.14e-04 | gates=[0.0026735018473118544, 0.9795330166816711] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996725916862488, 0.00032738992013037205]\n",
      "step 490 | loss 1.8552 | lr 1.11e-04 | gates=[0.00032738992013037205, 0.6084074378013611] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997184872627258, 0.00028153671883046627]\n",
      "step 500 | loss 5.5489 | lr 1.08e-04 | gates=[0.00028153671883046627, 0.9373906850814819] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9984196424484253, 0.001580353477038443]\n",
      "step 510 | loss 5.4185 | lr 1.05e-04 | gates=[0.001580353477038443, 0.9651354551315308] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998935461044312, 0.00010640869732014835]\n",
      "step 520 | loss 1.6823 | lr 1.01e-04 | gates=[0.00010640868276823312, 0.36746296286582947] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989398717880249, 0.0010601613903418183]\n",
      "step 530 | loss 5.5392 | lr 9.80e-05 | gates=[0.0010601615067571402, 0.8715717792510986] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.997254490852356, 0.0027455678209662437]\n",
      "step 540 | loss 5.4508 | lr 9.47e-05 | gates=[0.0027455680537968874, 0.9516192078590393] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9989663362503052, 0.0010337368585169315]\n",
      "step 550 | loss 4.9650 | lr 9.14e-05 | gates=[0.0010337368585169315, 0.9821469783782959] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.998478889465332, 0.001521096215583384]\n",
      "step 560 | loss 5.5100 | lr 8.81e-05 | gates=[0.001521096215583384, 0.9978583455085754] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9991660118103027, 0.0008340007625520229]\n",
      "step 570 | loss 4.8672 | lr 8.48e-05 | gates=[0.0008340008789673448, 0.9973529577255249] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9978957176208496, 0.002104356186464429]\n",
      "step 580 | loss 5.4628 | lr 8.16e-05 | gates=[0.0021043564192950726, 0.9963295459747314] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997061491012573, 0.00029383201035670936]\n",
      "step 590 | loss 1.4920 | lr 7.83e-05 | gates=[0.0002938320685643703, 0.1660376638174057] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996832609176636, 0.00031677575316280127]\n",
      "step 600 | loss 5.5113 | lr 7.51e-05 | gates=[0.0003167757240589708, 0.8998081684112549] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9952576160430908, 0.0047423457726836205]\n",
      "step 610 | loss 1.4186 | lr 7.19e-05 | gates=[0.0047423457726836205, 0.12642152607440948] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990122318267822, 0.0009877902921289206]\n",
      "step 620 | loss 5.5495 | lr 6.88e-05 | gates=[0.0009877901757135987, 0.9621265530586243] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998569488525391, 0.0001430785923730582]\n",
      "step 630 | loss 5.5372 | lr 6.57e-05 | gates=[0.00014307857782114297, 0.938446044921875] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999434351921082, 5.6625751312822104e-05]\n",
      "step 640 | loss 1.5129 | lr 6.26e-05 | gates=[5.662574403686449e-05, 0.2930299639701843] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996198415756226, 0.00038014710298739374]\n",
      "step 650 | loss 5.0308 | lr 5.95e-05 | gates=[0.00038014716119505465, 0.838420033454895] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999720454216003, 2.7966614652541466e-05]\n",
      "step 660 | loss 4.8953 | lr 5.65e-05 | gates=[2.7966612833552063e-05, 0.91777503490448] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990038871765137, 0.000996143906377256]\n",
      "step 670 | loss 4.9110 | lr 5.36e-05 | gates=[0.000996143789961934, 0.9821951985359192] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9987918138504028, 0.0012082239845767617]\n",
      "step 680 | loss 5.0259 | lr 5.07e-05 | gates=[0.0012082241009920835, 0.7774392366409302] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999942183494568, 5.700428118871059e-06]\n",
      "step 690 | loss 1.4982 | lr 4.78e-05 | gates=[5.700428118871059e-06, 0.19446589052677155] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999627470970154, 3.719556116266176e-05]\n",
      "step 700 | loss 1.6749 | lr 4.50e-05 | gates=[3.719556116266176e-05, 0.22752875089645386] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999644756317139, 3.566409577615559e-05]\n",
      "step 710 | loss 1.1634 | lr 4.23e-05 | gates=[3.566409577615559e-05, 0.22022190690040588] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999628067016602, 3.7246241845423356e-05]\n",
      "step 720 | loss 1.5027 | lr 3.96e-05 | gates=[3.7246241845423356e-05, 0.2591099441051483] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9793269634246826, 0.020673036575317383]\n",
      "step 730 | loss 5.0728 | lr 3.70e-05 | gates=[0.020673036575317383, 0.9428027868270874] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998282194137573, 0.00017178976850118488]\n",
      "step 740 | loss 4.8420 | lr 3.45e-05 | gates=[0.0001717897830531001, 0.926231861114502] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998147487640381, 0.00018523538892623037]\n",
      "step 750 | loss 1.3676 | lr 3.20e-05 | gates=[0.00018523538892623037, 0.2831946909427643] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998553991317749, 0.0001446289970772341]\n",
      "step 760 | loss 5.4739 | lr 2.96e-05 | gates=[0.00014462901162914932, 0.9466031789779663] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9987518191337585, 0.0012482462916523218]\n",
      "step 770 | loss 5.5063 | lr 2.73e-05 | gates=[0.0012482464080676436, 0.9469271302223206] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995874762535095, 0.0004125452251173556]\n",
      "step 780 | loss 4.8853 | lr 2.51e-05 | gates=[0.0004125452251173556, 0.9163680076599121] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995540380477905, 0.00044595939107239246]\n",
      "step 790 | loss 4.9608 | lr 2.29e-05 | gates=[0.000445959361968562, 0.9421708583831787] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9896177053451538, 0.010382357984781265]\n",
      "step 800 | loss 4.9173 | lr 2.09e-05 | gates=[0.010382357984781265, 0.9339077472686768] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999714195728302, 0.00028580869548022747]\n",
      "step 810 | loss 1.3686 | lr 2.00e-05 | gates=[0.000285808666376397, 0.21242274343967438] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999417662620544, 5.83188739256002e-05]\n",
      "step 820 | loss 5.4561 | lr 2.00e-05 | gates=[5.8318870287621394e-05, 0.9591422080993652] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9870847463607788, 0.01291526760905981]\n",
      "step 830 | loss 4.9222 | lr 2.00e-05 | gates=[0.01291526760905981, 0.9354809522628784] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9984363317489624, 0.001563745434395969]\n",
      "step 840 | loss 5.4660 | lr 2.00e-05 | gates=[0.001563745434395969, 0.9032887816429138] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999892115592957, 1.0760996701719705e-05]\n",
      "step 850 | loss 1.4403 | lr 2.00e-05 | gates=[1.0760996701719705e-05, 0.3179056942462921] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999263346195221, 0.0007366822683252394]\n",
      "step 860 | loss 4.8877 | lr 2.00e-05 | gates=[0.0007366824429482222, 0.9551268815994263] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999507665634155, 4.932661249767989e-05]\n",
      "step 870 | loss 5.4498 | lr 2.00e-05 | gates=[4.932660885970108e-05, 0.9588058590888977] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994196891784668, 0.0005802924279123545]\n",
      "step 880 | loss 4.8572 | lr 2.00e-05 | gates=[0.0005802924279123545, 0.9895489811897278] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9934481382369995, 0.006551915779709816]\n",
      "step 890 | loss 5.0883 | lr 2.00e-05 | gates=[0.006551915779709816, 0.9824629426002502] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9890168905258179, 0.010983088053762913]\n",
      "step 900 | loss 4.7984 | lr 2.00e-05 | gates=[0.010983087122440338, 0.9189621210098267] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9957637190818787, 0.00423626322299242]\n",
      "step 910 | loss 4.9043 | lr 2.00e-05 | gates=[0.004236262757331133, 0.8660269975662231] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992152452468872, 0.0007847851375117898]\n",
      "step 920 | loss 1.1336 | lr 2.00e-05 | gates=[0.0007847851375117898, 0.22806459665298462] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997989535331726, 0.00020103112910874188]\n",
      "step 930 | loss 1.4022 | lr 2.00e-05 | gates=[0.00020103111455682665, 0.2073519229888916] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996296763420105, 0.00037039763992652297]\n",
      "step 940 | loss 1.3433 | lr 2.00e-05 | gates=[0.0003703976690303534, 0.3001750707626343] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995278120040894, 0.0004721346194855869]\n",
      "step 950 | loss 5.0006 | lr 2.00e-05 | gates=[0.0004721345903817564, 0.9322761297225952] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9978735446929932, 0.0021264364477247]\n",
      "step 960 | loss 4.8792 | lr 2.00e-05 | gates=[0.0021264364477247, 0.9086391925811768] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999931454658508, 6.89803482600837e-06]\n",
      "step 970 | loss 4.9150 | lr 2.00e-05 | gates=[6.898035280755721e-06, 0.959106981754303] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993487000465393, 0.00065135135082528]\n",
      "step 980 | loss 5.4162 | lr 2.00e-05 | gates=[0.000651351292617619, 0.9477023482322693] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999781847000122, 2.1826352167408913e-05]\n",
      "step 990 | loss 1.4070 | lr 2.00e-05 | gates=[2.1826352167408913e-05, 0.27655869722366333] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999759316444397, 0.00024069046776276082]\n",
      "step 1000 | loss 4.9328 | lr 2.00e-05 | gates=[0.00024069043865893036, 0.9000252485275269] | mix=nback\n",
      "[after  E11b_tiered_deeper_late_s1337_1755943893] alloc=9.63 GB | reserved=25.05 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.63 GB | reserved=9.65 GB | free=14.71 GB | total=25.77 GB\n",
      "\n",
      "=== E13_write_sparsity_s1337_1755945422 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-033702-E13_write_sparsity_s1337_1755945422\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E13_write_sparsity_s1337_1755945422] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7959ece858b4cb7a711a192e73857ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8813318610191345, 0.05967889726161957, 0.05898924544453621]\n",
      "step 10 | loss 9.9197 | lr 4.40e-05 | gates=[0.11866813898086548, 0.29818278551101685] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8176863193511963, 0.08694931119680405, 0.09536437690258026]\n",
      "step 20 | loss 9.8345 | lr 8.40e-05 | gates=[0.1823136955499649, 0.3018989861011505] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9536228775978088, 0.012637387029826641, 0.03373980149626732]\n",
      "step 30 | loss 1.9897 | lr 1.24e-04 | gates=[0.04637718200683594, 0.3020365238189697] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7972662448883057, 0.11007903516292572, 0.09265470504760742]\n",
      "step 40 | loss 7.4993 | lr 1.64e-04 | gates=[0.20273372530937195, 0.3123947083950043] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.974845826625824, 0.018635254353284836, 0.006518871523439884]\n",
      "step 50 | loss 6.7290 | lr 2.00e-04 | gates=[0.025154123082756996, 0.9936141967773438] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994534254074097, 0.000451551575679332, 9.502804459771141e-05]\n",
      "step 60 | loss 5.7578 | lr 2.00e-04 | gates=[0.0005465796566568315, 0.9995747804641724] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9968701601028442, 0.0024423187132924795, 0.0006874970276840031]\n",
      "step 70 | loss 2.8921 | lr 2.00e-04 | gates=[0.0031298156827688217, 0.02583175152540207] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996402859687805, 0.00034314190270379186, 1.6608879377599806e-05]\n",
      "step 80 | loss 5.9528 | lr 1.99e-04 | gates=[0.00035975081846117973, 0.9869987964630127] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9972058534622192, 0.002155513968318701, 0.0006386375753208995]\n",
      "step 90 | loss 6.1900 | lr 1.99e-04 | gates=[0.002794151660054922, 0.9951601028442383] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9990806579589844, 0.0008103050058707595, 0.00010909957927651703]\n",
      "step 100 | loss 8.0217 | lr 1.99e-04 | gates=[0.000919404614251107, 0.26285696029663086] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9963594079017639, 0.0033108233474195004, 0.0003297958173789084]\n",
      "step 110 | loss 6.2899 | lr 1.98e-04 | gates=[0.0036406193394213915, 0.42907869815826416] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.998249888420105, 0.0017365337116643786, 1.3572392163041513e-05]\n",
      "step 120 | loss 5.7543 | lr 1.97e-04 | gates=[0.0017501059919595718, 0.9980363845825195] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9786547422409058, 0.02120843343436718, 0.0001367629156447947]\n",
      "step 130 | loss 2.0307 | lr 1.96e-04 | gates=[0.021345196291804314, 0.23985344171524048] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9965157508850098, 0.0031254191417247057, 0.00035876926267519593]\n",
      "step 140 | loss 1.7636 | lr 1.96e-04 | gates=[0.0034841876477003098, 0.057360030710697174] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987223148345947, 0.000914416101295501, 0.00036325311521068215]\n",
      "step 150 | loss 9.0279 | lr 1.94e-04 | gates=[0.0012776694493368268, 0.999835729598999] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997200965881348, 0.00023483032418880612, 4.5073204091750085e-05]\n",
      "step 160 | loss 1.9439 | lr 1.93e-04 | gates=[0.00027990349917672575, 0.0012012239312753081] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997684955596924, 0.0001910867722472176, 4.047776383231394e-05]\n",
      "step 170 | loss 5.2089 | lr 1.92e-04 | gates=[0.0002315645688213408, 0.9999610185623169] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9963173270225525, 0.0028162673115730286, 0.0008663928601890802]\n",
      "step 180 | loss 6.0076 | lr 1.91e-04 | gates=[0.003682660171762109, 0.9990509748458862] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9995805621147156, 0.0003556757583282888, 6.376473902491853e-05]\n",
      "step 190 | loss 1.8058 | lr 1.89e-04 | gates=[0.0004194404755253345, 0.08791043609380722] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999208927154541, 0.0007013718131929636, 8.97170539246872e-05]\n",
      "step 200 | loss 2.1142 | lr 1.88e-04 | gates=[0.0007910889107733965, 0.1617998480796814] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994678497314453, 0.0004483377852011472, 8.374320896109566e-05]\n",
      "step 210 | loss 1.8535 | lr 1.86e-04 | gates=[0.0005320809432305396, 0.02781536802649498] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9953162670135498, 0.003715413622558117, 0.0009683013777248561]\n",
      "step 220 | loss 5.8046 | lr 1.84e-04 | gates=[0.004683715291321278, 0.9988483190536499] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995087385177612, 0.0004104997497051954, 8.067686576396227e-05]\n",
      "step 230 | loss 1.8590 | lr 1.83e-04 | gates=[0.0004911765572614968, 0.009087590500712395] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9971258640289307, 0.002389029134064913, 0.000485129130538553]\n",
      "step 240 | loss 5.7160 | lr 1.81e-04 | gates=[0.0028741583228111267, 0.9972455501556396] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995830059051514, 0.00035648010089062154, 6.057168138795532e-05]\n",
      "step 250 | loss 5.6344 | lr 1.79e-04 | gates=[0.00041705183684825897, 0.9943421483039856] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9984062910079956, 0.001270812121219933, 0.00032290752278640866]\n",
      "step 260 | loss 5.6570 | lr 1.77e-04 | gates=[0.0015937197022140026, 0.9998753070831299] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9988455772399902, 0.0009513108525425196, 0.00020307369413785636]\n",
      "step 270 | loss 5.2250 | lr 1.74e-04 | gates=[0.0011543845757842064, 0.9999352693557739] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9983112812042236, 0.0014160199789330363, 0.00027264285017736256]\n",
      "step 280 | loss 5.2869 | lr 1.72e-04 | gates=[0.0016886628000065684, 0.999904215335846] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9972497224807739, 0.0022784406319260597, 0.0004717945703305304]\n",
      "step 290 | loss 5.3369 | lr 1.70e-04 | gates=[0.0027502356097102165, 0.9992403984069824] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9981222748756409, 0.0015188861871138215, 0.00035879804636351764]\n",
      "step 300 | loss 1.5623 | lr 1.67e-04 | gates=[0.0018776841461658478, 0.019952112808823586] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9984209537506104, 0.0012115902500227094, 0.00036744578392244875]\n",
      "step 310 | loss 1.6263 | lr 1.65e-04 | gates=[0.0015790358884260058, 0.05330338329076767] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9988701939582825, 0.000961398531217128, 0.00016835029236972332]\n",
      "step 320 | loss 1.6660 | lr 1.62e-04 | gates=[0.0011297488817945123, 0.10754101723432541] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985228180885315, 0.0013043242506682873, 0.00017299267346970737]\n",
      "step 330 | loss 1.6790 | lr 1.60e-04 | gates=[0.001477317069657147, 0.22006019949913025] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997983574867249, 0.00017488401499576867, 2.6725769203039818e-05]\n",
      "step 340 | loss 5.9829 | lr 1.57e-04 | gates=[0.00020160980056971312, 0.893077552318573] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999515950679779, 0.0003854362585116178, 9.860448335530236e-05]\n",
      "step 350 | loss 5.2769 | lr 1.54e-04 | gates=[0.00048404070548713207, 0.9957723617553711] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9969608783721924, 0.0023728052619844675, 0.0006663427921012044]\n",
      "step 360 | loss 5.3097 | lr 1.52e-04 | gates=[0.0030391484033316374, 0.9996885657310486] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9986153841018677, 0.0011937622912228107, 0.00019080869969911873]\n",
      "step 370 | loss 3.5654 | lr 1.49e-04 | gates=[0.001384570961818099, 0.7578803896903992] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9975823760032654, 0.0021831237245351076, 0.00023448762658517808]\n",
      "step 380 | loss 1.8079 | lr 1.46e-04 | gates=[0.002417611423879862, 0.0556284599006176] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994854927062988, 0.00042952975491061807, 8.494496432831511e-05]\n",
      "step 390 | loss 5.3345 | lr 1.43e-04 | gates=[0.0005144747556187212, 0.9964982271194458] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9937583208084106, 0.005280242767184973, 0.0009614225709810853]\n",
      "step 400 | loss 5.4678 | lr 1.40e-04 | gates=[0.0062416656874120235, 0.9970451593399048] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9994144439697266, 0.0004960434744134545, 8.94518379936926e-05]\n",
      "step 410 | loss 5.6082 | lr 1.37e-04 | gates=[0.0005854953778907657, 0.9998660087585449] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9900751709938049, 0.009453886188566685, 0.0004709411005023867]\n",
      "step 420 | loss 1.2414 | lr 1.34e-04 | gates=[0.009924827143549919, 0.22838754951953888] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991005659103394, 0.0007473276928067207, 0.00015209164121188223]\n",
      "step 430 | loss 4.9895 | lr 1.31e-04 | gates=[0.0008994193049147725, 0.999497652053833] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9790655970573425, 0.020513342693448067, 0.0004210780607536435]\n",
      "step 440 | loss 1.6439 | lr 1.27e-04 | gates=[0.020934421569108963, 0.002480490133166313] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.997825026512146, 0.0019465098157525063, 0.00022849866945762187]\n",
      "step 450 | loss 6.4685 | lr 1.24e-04 | gates=[0.0021750086452811956, 0.998193621635437] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9243969321250916, 0.07486394792795181, 0.0007392023107968271]\n",
      "step 460 | loss 1.6091 | lr 1.21e-04 | gates=[0.07560314983129501, 0.039736308157444] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9742212295532227, 0.0247793048620224, 0.0009994303109124303]\n",
      "step 470 | loss 5.2807 | lr 1.18e-04 | gates=[0.025778736919164658, 0.9987943172454834] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9989342093467712, 0.0008938216487877071, 0.0001719748106552288]\n",
      "step 480 | loss 5.1239 | lr 1.14e-04 | gates=[0.0010657964739948511, 0.9965589642524719] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998071193695068, 0.00017203876632265747, 2.0809837224078365e-05]\n",
      "step 490 | loss 5.0797 | lr 1.11e-04 | gates=[0.0001928485871758312, 0.9968447089195251] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994498491287231, 0.00044621049892157316, 0.00010387556540081277]\n",
      "step 500 | loss 5.5209 | lr 1.08e-04 | gates=[0.0005500860861502588, 0.994880199432373] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9987687468528748, 0.0009378461400046945, 0.0002933876239694655]\n",
      "step 510 | loss 5.5153 | lr 1.05e-04 | gates=[0.0012312338221818209, 0.9857351779937744] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993640184402466, 0.0005427250871434808, 9.324192069470882e-05]\n",
      "step 520 | loss 5.1685 | lr 1.01e-04 | gates=[0.0006359668914228678, 0.9950250387191772] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9989010095596313, 0.0008159772260114551, 0.0002829823351930827]\n",
      "step 530 | loss 1.5319 | lr 9.80e-05 | gates=[0.00109895970672369, 0.3009430766105652] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9983922839164734, 0.001269470900297165, 0.00033821494434960186]\n",
      "step 540 | loss 1.3599 | lr 9.47e-05 | gates=[0.0016076858155429363, 0.4123361110687256] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995916485786438, 0.0003711964236572385, 3.719221422215924e-05]\n",
      "step 550 | loss 5.4800 | lr 9.14e-05 | gates=[0.00040838864515535533, 0.996851921081543] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9987996816635132, 0.000957437907345593, 0.00024283298989757895]\n",
      "step 560 | loss 5.4781 | lr 8.81e-05 | gates=[0.0012002709554508328, 0.9871692657470703] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9975085258483887, 0.002027677372097969, 0.000463793781818822]\n",
      "step 570 | loss 5.0179 | lr 8.48e-05 | gates=[0.0024914713576436043, 0.9926682710647583] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9989160299301147, 0.0008292071288451552, 0.00025462231133133173]\n",
      "step 580 | loss 1.1847 | lr 8.16e-05 | gates=[0.001083829440176487, 0.3016459047794342] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999065637588501, 0.0006913596298545599, 0.00024295906769111753]\n",
      "step 590 | loss 1.5077 | lr 7.83e-05 | gates=[0.0009343188139609993, 0.16537119448184967] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990633726119995, 0.0007301342557184398, 0.00020644854521378875]\n",
      "step 600 | loss 1.5481 | lr 7.51e-05 | gates=[0.0009365828009322286, 0.35330459475517273] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9957854747772217, 0.0032261607702821493, 0.0009883709717541933]\n",
      "step 610 | loss 4.9901 | lr 7.19e-05 | gates=[0.004214531742036343, 0.9997149109840393] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992321133613586, 0.0005709314136765897, 0.00019684078870341182]\n",
      "step 620 | loss 1.4285 | lr 6.88e-05 | gates=[0.0007677722605876625, 0.22870144248008728] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987455606460571, 0.0010153219336643815, 0.00023910606978461146]\n",
      "step 630 | loss 4.9913 | lr 6.57e-05 | gates=[0.0012544280616566539, 0.999203085899353] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9978514909744263, 0.0016485053347423673, 0.0004999951925128698]\n",
      "step 640 | loss 5.4874 | lr 6.26e-05 | gates=[0.0021485004108399153, 0.9990097880363464] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9985327124595642, 0.001163895707577467, 0.00030335935298353434]\n",
      "step 650 | loss 5.5230 | lr 5.95e-05 | gates=[0.0014672549441456795, 0.9967232346534729] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.99845290184021, 0.0012842586729675531, 0.0002628695801831782]\n",
      "step 660 | loss 5.5081 | lr 5.65e-05 | gates=[0.0015471281949430704, 0.9841511249542236] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.998917818069458, 0.0007976790657266974, 0.0002844861301127821]\n",
      "step 670 | loss 1.5844 | lr 5.36e-05 | gates=[0.001082165283150971, 0.4312874376773834] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999669075012207, 0.00029203775920905173, 3.887961429427378e-05]\n",
      "step 680 | loss 5.0014 | lr 5.07e-05 | gates=[0.0003309173625893891, 0.9381865859031677] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997593760490417, 0.00022674874344374985, 1.3885496628063265e-05]\n",
      "step 690 | loss 4.9764 | lr 4.78e-05 | gates=[0.00024063422461040318, 0.9823279976844788] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990061521530151, 0.0008365309331566095, 0.00015731142775621265]\n",
      "step 700 | loss 4.9773 | lr 4.50e-05 | gates=[0.0009938422590494156, 0.8844594359397888] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993043541908264, 0.0005373386084102094, 0.00015828205505385995]\n",
      "step 710 | loss 1.1848 | lr 4.23e-05 | gates=[0.0006956206634640694, 0.19809457659721375] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990068078041077, 0.0007769343792460859, 0.00021625988301821053]\n",
      "step 720 | loss 5.5296 | lr 3.96e-05 | gates=[0.0009931944077834487, 0.9546486139297485] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9990027546882629, 0.0007910494459792972, 0.0002062308049062267]\n",
      "step 730 | loss 5.0269 | lr 3.70e-05 | gates=[0.0009972803527489305, 0.9415454268455505] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9952332973480225, 0.0028704435098916292, 0.0018962868489325047]\n",
      "step 740 | loss 4.8172 | lr 3.45e-05 | gates=[0.00476673012599349, 0.9227465987205505] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9933382868766785, 0.005303605925291777, 0.0013580919476225972]\n",
      "step 750 | loss 4.8529 | lr 3.20e-05 | gates=[0.006661698222160339, 0.8957099914550781] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999608039855957, 0.0003311086620669812, 6.082958134356886e-05]\n",
      "step 760 | loss 5.0504 | lr 2.96e-05 | gates=[0.0003919382579624653, 0.9700326323509216] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9987149238586426, 0.0011326405219733715, 0.00015240692300722003]\n",
      "step 770 | loss 4.8984 | lr 2.73e-05 | gates=[0.0012850473867729306, 0.9732822775840759] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9982326030731201, 0.0015146005898714066, 0.00025278900284320116]\n",
      "step 780 | loss 5.4926 | lr 2.51e-05 | gates=[0.0017673894762992859, 0.9476732015609741] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994943141937256, 0.0003902742173522711, 0.00011547508620424196]\n",
      "step 790 | loss 1.3462 | lr 2.29e-05 | gates=[0.0005057493108324707, 0.21960093080997467] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9889408349990845, 0.009520253166556358, 0.0015388807514682412]\n",
      "step 800 | loss 4.5958 | lr 2.09e-05 | gates=[0.011059132404625416, 0.8865653872489929] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9994055032730103, 0.0005290878470987082, 6.543526251334697e-05]\n",
      "step 810 | loss 4.9323 | lr 2.00e-05 | gates=[0.000594523036852479, 0.9098587036132812] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9987513422966003, 0.000948307802900672, 0.00030039402190595865]\n",
      "step 820 | loss 1.2150 | lr 2.00e-05 | gates=[0.0012487018248066306, 0.24535837769508362] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998384416103363, 0.0011647475184872746, 0.0004508364654611796]\n",
      "step 830 | loss 1.3137 | lr 2.00e-05 | gates=[0.0016155841294676065, 0.3550906181335449] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987902045249939, 0.0008641242748126388, 0.0003456767008174211]\n",
      "step 840 | loss 1.3963 | lr 2.00e-05 | gates=[0.0012098009465262294, 0.24155579507350922] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989022016525269, 0.0009164935909211636, 0.00018124286725651473]\n",
      "step 850 | loss 4.9901 | lr 2.00e-05 | gates=[0.0010977365309372544, 0.9478603601455688] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9984997510910034, 0.0012293941108509898, 0.00027079350547865033]\n",
      "step 860 | loss 5.4421 | lr 2.00e-05 | gates=[0.0015001874417066574, 0.9790337681770325] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9981365203857422, 0.0015318976948037744, 0.000331565213855356]\n",
      "step 870 | loss 5.4827 | lr 2.00e-05 | gates=[0.0018634629668667912, 0.9689341187477112] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999182403087616, 0.0006852677906863391, 0.0001323515025433153]\n",
      "step 880 | loss 1.5054 | lr 2.00e-05 | gates=[0.000817619264125824, 0.40384092926979065] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9951009750366211, 0.003840736113488674, 0.0010582682443782687]\n",
      "step 890 | loss 4.9180 | lr 2.00e-05 | gates=[0.004899004474282265, 0.9559484720230103] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9993730783462524, 0.0005287578096613288, 9.816525562200695e-05]\n",
      "step 900 | loss 4.9338 | lr 2.00e-05 | gates=[0.0006269229925237596, 0.9603513479232788] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9991782903671265, 0.0006604179507121444, 0.00016126310219988227]\n",
      "step 910 | loss 5.6506 | lr 2.00e-05 | gates=[0.0008216810529120266, 0.9609296321868896] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993393421173096, 0.0005417590145953, 0.00011884484410984442]\n",
      "step 920 | loss 5.0696 | lr 2.00e-05 | gates=[0.0006606038077734411, 0.9643474817276001] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9984104037284851, 0.0012896520784124732, 0.0002999443095177412]\n",
      "step 930 | loss 1.3647 | lr 2.00e-05 | gates=[0.0015895965043455362, 0.3117956221103668] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997832775115967, 0.0001992926117964089, 1.7401362129021436e-05]\n",
      "step 940 | loss 5.4686 | lr 2.00e-05 | gates=[0.0002166939666494727, 0.9675374031066895] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9990209341049194, 0.0007629210595041513, 0.0002161463489755988]\n",
      "step 950 | loss 4.9893 | lr 2.00e-05 | gates=[0.0009790674084797502, 0.9717222452163696] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999415397644043, 0.0004691198992077261, 0.00011536461533978581]\n",
      "step 960 | loss 1.4561 | lr 2.00e-05 | gates=[0.0005844844854436815, 0.19957903027534485] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9960641264915466, 0.0032055526971817017, 0.0007302977610379457]\n",
      "step 970 | loss 4.6872 | lr 2.00e-05 | gates=[0.003935850691050291, 0.9431948065757751] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9990479350090027, 0.0005796559271402657, 0.0003722843830473721]\n",
      "step 980 | loss 1.6594 | lr 2.00e-05 | gates=[0.0009519403101876378, 0.17520178854465485] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991976618766785, 0.000686305167619139, 0.00011602934682741761]\n",
      "step 990 | loss 4.9569 | lr 2.00e-05 | gates=[0.0008023345726542175, 0.9805846214294434] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9838364124298096, 0.013488643802702427, 0.0026749707758426666]\n",
      "step 1000 | loss 4.8180 | lr 2.00e-05 | gates=[0.016163617372512817, 0.9532890319824219] | mix=repeat\n",
      "[after  E13_write_sparsity_s1337_1755945422] alloc=10.70 GB | reserved=32.22 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.70 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "\n",
      "=== E14_curriculum_warmstart_s1337_1755948671 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-043111-E14_curriculum_warmstart_s1337_1755948671\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E14_curriculum_warmstart_s1337_1755948671] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eae4854783c744468b3b63da4047c072"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.842902660369873, 0.08819584548473358, 0.06890152394771576]\n",
      "step 10 | loss 7.2841 | lr 4.40e-05 | gates=[0.15709735453128815, 0.687245786190033] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4024929404258728, 0.4208603501319885, 0.17664667963981628]\n",
      "step 20 | loss 7.0734 | lr 8.40e-05 | gates=[0.5975069999694824, 0.6532856225967407] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9398799538612366, 0.024742722511291504, 0.035377345979213715]\n",
      "step 30 | loss 6.2738 | lr 1.24e-04 | gates=[0.06012007221579552, 0.7696507573127747] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9718606472015381, 0.010401194915175438, 0.01773819327354431]\n",
      "step 40 | loss 5.5504 | lr 1.64e-04 | gates=[0.0281393900513649, 0.7790657877922058] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.984597384929657, 0.005642917938530445, 0.009759693406522274]\n",
      "step 50 | loss 5.9330 | lr 2.00e-04 | gates=[0.015402611345052719, 0.7496401071548462] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9900622963905334, 0.0028397846035659313, 0.0070979297161102295]\n",
      "step 60 | loss 5.5966 | lr 2.00e-04 | gates=[0.009937713854014874, 0.9996137619018555] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9881073832511902, 0.006681832019239664, 0.0052107833325862885]\n",
      "step 70 | loss 5.5239 | lr 2.00e-04 | gates=[0.01189261581748724, 0.9999354481697083] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9501487016677856, 0.022015869617462158, 0.027835451066493988]\n",
      "step 80 | loss 5.6143 | lr 1.99e-04 | gates=[0.04985131323337555, 0.9998561143875122] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9433304071426392, 0.018028996884822845, 0.0386405773460865]\n",
      "step 90 | loss 5.5218 | lr 1.99e-04 | gates=[0.05666957423090935, 0.9998124837875366] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9947599172592163, 0.0008108241017907858, 0.0044292546808719635]\n",
      "step 100 | loss 5.6790 | lr 1.99e-04 | gates=[0.005240078549832106, 0.9999948740005493] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9633380174636841, 0.011607488617300987, 0.02505449764430523]\n",
      "step 110 | loss 6.0837 | lr 1.98e-04 | gates=[0.036661986261606216, 0.999866783618927] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9463465809822083, 0.01491896528750658, 0.03873445466160774]\n",
      "step 120 | loss 5.9428 | lr 1.97e-04 | gates=[0.05365341901779175, 0.9998968839645386] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9765850305557251, 0.014408040791749954, 0.009006896987557411]\n",
      "step 130 | loss 5.7650 | lr 1.96e-04 | gates=[0.023414937779307365, 0.9999309182167053] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9809249639511108, 0.008378583006560802, 0.010696474462747574]\n",
      "step 140 | loss 5.5700 | lr 1.96e-04 | gates=[0.0190750565379858, 0.9999834299087524] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9780743718147278, 0.010780777782201767, 0.011144871823489666]\n",
      "step 150 | loss 7.0920 | lr 1.94e-04 | gates=[0.021925650537014008, 0.9999305605888367] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9268203973770142, 0.043208856135606766, 0.029970712959766388]\n",
      "step 160 | loss 5.2917 | lr 1.93e-04 | gates=[0.07317956537008286, 0.9996080994606018] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9757494926452637, 0.005653423257172108, 0.01859707199037075]\n",
      "step 170 | loss 5.6029 | lr 1.92e-04 | gates=[0.024250496178865433, 0.9999796152114868] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9762253761291504, 0.010568022727966309, 0.013206584379076958]\n",
      "step 180 | loss 5.5581 | lr 1.91e-04 | gates=[0.023774607107043266, 0.9999682307243347] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8941918611526489, 0.020220674574375153, 0.08558745682239532]\n",
      "step 190 | loss 5.3370 | lr 1.89e-04 | gates=[0.10580812394618988, 0.999854564666748] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.991809606552124, 0.004693020135164261, 0.0034973800648003817]\n",
      "step 200 | loss 5.5175 | lr 1.88e-04 | gates=[0.008190400898456573, 0.9999712705612183] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9847884178161621, 0.010785574093461037, 0.00442597595974803]\n",
      "step 210 | loss 5.2054 | lr 1.86e-04 | gates=[0.015211549587547779, 0.7246317267417908] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9789602756500244, 0.0023088722955435514, 0.018730852752923965]\n",
      "step 220 | loss 5.9811 | lr 1.84e-04 | gates=[0.021039724349975586, 0.9592244029045105] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999712705612183, 7.098749392753234e-06, 2.1788529920740984e-05]\n",
      "step 230 | loss 2.2008 | lr 1.83e-04 | gates=[2.8887277949252166e-05, 0.0008243352640420198] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9911907911300659, 0.0040491120889782906, 0.004760053940117359]\n",
      "step 240 | loss 7.3365 | lr 1.81e-04 | gates=[0.00880916602909565, 0.9732284545898438] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9907307028770447, 0.006336962804198265, 0.0029323347844183445]\n",
      "step 250 | loss 5.6155 | lr 1.79e-04 | gates=[0.009269298054277897, 0.9756643176078796] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998103380203247, 6.035296610207297e-05, 0.00012933344987686723]\n",
      "step 260 | loss 1.9608 | lr 1.77e-04 | gates=[0.0001896864123409614, 0.20696979761123657] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995604753494263, 0.00029733602423220873, 0.000142219229019247]\n",
      "step 270 | loss 1.9347 | lr 1.74e-04 | gates=[0.0004395552386995405, 0.03144725412130356] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995182156562805, 0.00031607848359271884, 0.0001656741660553962]\n",
      "step 280 | loss 5.9271 | lr 1.72e-04 | gates=[0.00048175264964811504, 0.9958580732345581] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9978123903274536, 0.0016929381527006626, 0.0004946735571138561]\n",
      "step 290 | loss 5.8386 | lr 1.70e-04 | gates=[0.0021876120008528233, 0.9779412150382996] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997473359107971, 0.00015856881509535015, 9.407933976035565e-05]\n",
      "step 300 | loss 5.6957 | lr 1.67e-04 | gates=[0.00025264814030379057, 0.9980080127716064] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993313550949097, 0.00040821818402037024, 0.00026041376986540854]\n",
      "step 310 | loss 5.6962 | lr 1.65e-04 | gates=[0.0006686319829896092, 0.9998852014541626] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9980987310409546, 0.0011357148177921772, 0.0007655757362954319]\n",
      "step 320 | loss 5.4355 | lr 1.62e-04 | gates=[0.0019012908451259136, 0.9998577833175659] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9953193664550781, 0.0032309950329363346, 0.0014496338553726673]\n",
      "step 330 | loss 5.7503 | lr 1.60e-04 | gates=[0.004680628422647715, 0.9972029328346252] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999198913574219, 3.301325341453776e-05, 4.717049523605965e-05]\n",
      "step 340 | loss 1.9076 | lr 1.57e-04 | gates=[8.018375228857622e-05, 0.000777440785896033] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999735414981842, 0.00013087180559523404, 0.0001336818386334926]\n",
      "step 350 | loss 1.8011 | lr 1.54e-04 | gates=[0.00026455361512489617, 0.03526163101196289] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992745518684387, 0.00041684252209961414, 0.00030866669840179384]\n",
      "step 360 | loss 5.3456 | lr 1.52e-04 | gates=[0.0007255091331899166, 0.9963486194610596] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998166561126709, 8.314362639794126e-05, 0.00010020593617809936]\n",
      "step 370 | loss 2.1895 | lr 1.49e-04 | gates=[0.00018334957712795585, 0.03979131206870079] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990502595901489, 0.0006675492622889578, 0.0002822101814672351]\n",
      "step 380 | loss 5.7229 | lr 1.46e-04 | gates=[0.0009497594437561929, 0.99854576587677] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999833345413208, 0.00010205439320998266, 6.459512951551005e-05]\n",
      "step 390 | loss 5.5954 | lr 1.43e-04 | gates=[0.00016664952272549272, 0.9988671541213989] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997673630714417, 0.00013600956299342215, 9.665267134550959e-05]\n",
      "step 400 | loss 5.4079 | lr 1.40e-04 | gates=[0.00023266224889084697, 0.9986165761947632] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999539256095886, 1.291958506044466e-05, 3.3130465453723446e-05]\n",
      "step 410 | loss 3.0392 | lr 1.37e-04 | gates=[4.6050045057199895e-05, 0.06309659779071808] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998778700828552, 3.236262273276225e-05, 8.977933612186462e-05]\n",
      "step 420 | loss 7.7204 | lr 1.34e-04 | gates=[0.00012214196613058448, 0.08671918511390686] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995619058609009, 0.00012428092304617167, 0.00031378111452795565]\n",
      "step 430 | loss 1.6725 | lr 1.31e-04 | gates=[0.0004380620666779578, 0.2535555958747864] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995330572128296, 0.00015806269948370755, 0.00030891201458871365]\n",
      "step 440 | loss 5.5048 | lr 1.27e-04 | gates=[0.00046697468496859074, 0.9993382692337036] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995287656784058, 0.00033705600071698427, 0.0001342270552413538]\n",
      "step 450 | loss 5.2477 | lr 1.24e-04 | gates=[0.00047128304140642285, 0.9999900460243225] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998633861541748, 4.208521204418503e-05, 9.461324953008443e-05]\n",
      "step 460 | loss 1.6996 | lr 1.21e-04 | gates=[0.00013669845066033304, 0.03817933052778244] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9912969470024109, 0.003759954124689102, 0.004943102598190308]\n",
      "step 470 | loss 5.2735 | lr 1.18e-04 | gates=[0.00870305672287941, 0.999880313873291] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998049736022949, 6.558877794304863e-05, 0.00012933860125485808]\n",
      "step 480 | loss 1.6117 | lr 1.14e-04 | gates=[0.00019492740102577955, 0.048112690448760986] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9984002709388733, 0.0006874949904158711, 0.0009122976334765553]\n",
      "step 490 | loss 1.8382 | lr 1.11e-04 | gates=[0.0015997926238924265, 0.2701279819011688] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987294673919678, 0.0007061988580971956, 0.000564397603739053]\n",
      "step 500 | loss 5.5910 | lr 1.08e-04 | gates=[0.0012705964036285877, 0.9999871850013733] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9981929659843445, 0.0011598916025832295, 0.0006471287924796343]\n",
      "step 510 | loss 5.2762 | lr 1.05e-04 | gates=[0.0018070203950628638, 0.9977221488952637] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9983153939247131, 0.0009634263115003705, 0.0007211436168290675]\n",
      "step 520 | loss 4.9534 | lr 1.01e-04 | gates=[0.001684569870121777, 0.9995957612991333] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990153312683105, 0.0005791224539279938, 0.0004055649624206126]\n",
      "step 530 | loss 5.0717 | lr 9.80e-05 | gates=[0.0009846873581409454, 0.9997484087944031] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9991954565048218, 0.0004411235568113625, 0.00036335602635517716]\n",
      "step 540 | loss 5.2010 | lr 9.47e-05 | gates=[0.0008044796413742006, 0.9994410276412964] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996784925460815, 7.919871131889522e-05, 0.0002422668767394498]\n",
      "step 550 | loss 1.5121 | lr 9.14e-05 | gates=[0.00032146560261026025, 0.27362361550331116] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986720085144043, 0.0007529724389314651, 0.0005750323180109262]\n",
      "step 560 | loss 5.6444 | lr 8.81e-05 | gates=[0.0013280048733577132, 0.9998108148574829] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993486404418945, 0.0004058695922140032, 0.00024547477369196713]\n",
      "step 570 | loss 5.0087 | lr 8.48e-05 | gates=[0.0006513443659059703, 0.9987573027610779] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999518871307373, 0.0001191332921735011, 0.0003620124189183116]\n",
      "step 580 | loss 5.6265 | lr 8.16e-05 | gates=[0.0004811457183677703, 0.9994993209838867] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9986679553985596, 0.000755177577957511, 0.000576922670006752]\n",
      "step 590 | loss 5.5103 | lr 7.83e-05 | gates=[0.0013321000151336193, 0.9987590312957764] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997657537460327, 5.838825018145144e-05, 0.00017590279458090663]\n",
      "step 600 | loss 1.4368 | lr 7.51e-05 | gates=[0.00023429104476235807, 0.21202845871448517] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9960466027259827, 0.0029465188272297382, 0.0010068912524729967]\n",
      "step 610 | loss 5.1057 | lr 7.19e-05 | gates=[0.003953409381210804, 0.9757169485092163] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999220967292786, 2.156290975108277e-05, 5.626272104564123e-05]\n",
      "step 620 | loss 1.4861 | lr 6.88e-05 | gates=[7.78256289777346e-05, 0.16966494917869568] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9983210563659668, 0.0011018847580999136, 0.0005770553834736347]\n",
      "step 630 | loss 5.5775 | lr 6.57e-05 | gates=[0.0016789402579888701, 0.9997221231460571] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999536275863647, 1.6334122847183608e-05, 2.995697650476359e-05]\n",
      "step 640 | loss 1.4759 | lr 6.26e-05 | gates=[4.6291104808915406e-05, 0.26603731513023376] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9971938133239746, 0.0021735811606049538, 0.0006325835129246116]\n",
      "step 650 | loss 5.4787 | lr 5.95e-05 | gates=[0.002806165022775531, 0.9997476935386658] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994629621505737, 0.00033798941876739264, 0.00019902920757886022]\n",
      "step 660 | loss 4.9141 | lr 5.65e-05 | gates=[0.000537018699105829, 0.9992249608039856] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996213912963867, 0.00014346458192449063, 0.0002351242146687582]\n",
      "step 670 | loss 4.9955 | lr 5.36e-05 | gates=[0.0003785887674894184, 0.9989417791366577] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9979879260063171, 0.000944034312851727, 0.001068076235242188]\n",
      "step 680 | loss 5.2079 | lr 5.07e-05 | gates=[0.0020121107809245586, 0.9989266395568848] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9993665218353271, 0.00029260225710459054, 0.0003409860946703702]\n",
      "step 690 | loss 0.9682 | lr 4.78e-05 | gates=[0.0006335883517749608, 0.42993536591529846] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998173713684082, 5.709331526304595e-05, 0.00012557301670312881]\n",
      "step 700 | loss 5.0055 | lr 4.50e-05 | gates=[0.00018266633560415357, 0.995082676410675] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996521472930908, 9.196616883855313e-05, 0.0002558794803917408]\n",
      "step 710 | loss 5.4765 | lr 4.23e-05 | gates=[0.00034784566378220916, 0.9950116872787476] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998045563697815, 0.00010052194556919858, 9.491021774010733e-05]\n",
      "step 720 | loss 1.9979 | lr 3.96e-05 | gates=[0.0001954321633093059, 0.270101398229599] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998342990875244, 5.819285070174374e-05, 0.00010756000847322866]\n",
      "step 730 | loss 1.6937 | lr 3.70e-05 | gates=[0.0001657528628129512, 0.18171463906764984] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.99910569190979, 0.0005550223868340254, 0.00033935951068997383]\n",
      "step 740 | loss 1.6226 | lr 3.45e-05 | gates=[0.0008943819557316601, 0.5400211215019226] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996791481971741, 0.00014693604316562414, 0.00017383279919158667]\n",
      "step 750 | loss 1.4870 | lr 3.20e-05 | gates=[0.00032076879870146513, 0.24914704263210297] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998348355293274, 5.168263669474982e-05, 0.00011345687380526215]\n",
      "step 760 | loss 1.4964 | lr 2.96e-05 | gates=[0.000165139528689906, 0.2705822288990021] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994235038757324, 0.00020598212722688913, 0.00037056778091937304]\n",
      "step 770 | loss 4.9457 | lr 2.73e-05 | gates=[0.0005765499081462622, 0.9965740442276001] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9984180927276611, 0.0007720121066085994, 0.0008098722901195288]\n",
      "step 780 | loss 1.3514 | lr 2.51e-05 | gates=[0.0015818842221051455, 0.4184155762195587] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997887015342712, 5.9353136748541147e-05, 0.00015192897990345955]\n",
      "step 790 | loss 4.9278 | lr 2.29e-05 | gates=[0.00021128213847987354, 0.9807324409484863] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998149871826172, 4.850155164604075e-05, 0.00013655467773787677]\n",
      "step 800 | loss 5.4706 | lr 2.09e-05 | gates=[0.00018505622574593872, 0.9772104620933533] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9959880113601685, 0.002605858724564314, 0.0014061161782592535]\n",
      "step 810 | loss 4.9513 | lr 2.00e-05 | gates=[0.004011975135654211, 0.9853987693786621] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9976284503936768, 0.0005788521375507116, 0.0017927491571754217]\n",
      "step 820 | loss 5.4755 | lr 2.00e-05 | gates=[0.0023716010618954897, 0.9907822608947754] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996641278266907, 0.00015247263945639133, 0.00018340462702326477]\n",
      "step 830 | loss 1.4315 | lr 2.00e-05 | gates=[0.0003358772664796561, 0.27779531478881836] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9921830892562866, 0.002539122011512518, 0.005277785938233137]\n",
      "step 840 | loss 4.7633 | lr 2.00e-05 | gates=[0.00781690701842308, 0.9790683388710022] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998140931129456, 8.719979086890817e-05, 9.872487862594426e-05]\n",
      "step 850 | loss 1.3909 | lr 2.00e-05 | gates=[0.00018592468404676765, 0.36292022466659546] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998680353164673, 4.973040631739423e-05, 8.225932106142864e-05]\n",
      "step 860 | loss 1.5299 | lr 2.00e-05 | gates=[0.0001319897419307381, 0.290900856256485] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9976431131362915, 0.001601913943886757, 0.0007549958536401391]\n",
      "step 870 | loss 5.5083 | lr 2.00e-05 | gates=[0.002356909913942218, 0.9740869998931885] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9989537000656128, 0.0007401952752843499, 0.0003061664174310863]\n",
      "step 880 | loss 5.4766 | lr 2.00e-05 | gates=[0.0010463617509230971, 0.9810585975646973] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999343156814575, 2.3673937903367914e-05, 4.198220631224103e-05]\n",
      "step 890 | loss 1.8396 | lr 2.00e-05 | gates=[6.565614603459835e-05, 0.2830384373664856] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996466636657715, 0.00017081260739360005, 0.00018247414845973253]\n",
      "step 900 | loss 1.2952 | lr 2.00e-05 | gates=[0.00035328674130141735, 0.2783188223838806] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995216727256775, 0.00024112401297315955, 0.0002372073649894446]\n",
      "step 910 | loss 1.3893 | lr 2.00e-05 | gates=[0.00047833137796260417, 0.45975247025489807] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995304346084595, 0.00019377747958060354, 0.00027582276379689574]\n",
      "step 920 | loss 1.3355 | lr 2.00e-05 | gates=[0.00046960022882558405, 0.3680228292942047] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999735414981842, 7.567115244455636e-05, 0.00018890162755269557]\n",
      "step 930 | loss 4.8660 | lr 2.00e-05 | gates=[0.0002645727654453367, 0.9598503112792969] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994460344314575, 0.0002583371533546597, 0.00029562501003965735]\n",
      "step 940 | loss 1.2406 | lr 2.00e-05 | gates=[0.0005539621342904866, 0.2018890380859375] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998575448989868, 5.0337246648268774e-05, 9.208945994032547e-05]\n",
      "step 950 | loss 1.4576 | lr 2.00e-05 | gates=[0.00014242671022657305, 0.27612531185150146] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998569667339325, 0.0007652997155673802, 0.0006650349241681397]\n",
      "step 960 | loss 5.4523 | lr 2.00e-05 | gates=[0.001430334523320198, 0.982893705368042] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9841104745864868, 0.010600371286273003, 0.005289178807288408]\n",
      "step 970 | loss 4.8528 | lr 2.00e-05 | gates=[0.015889549627900124, 0.9729394316673279] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997711777687073, 6.630879215663299e-05, 0.00016252262867055833]\n",
      "step 980 | loss 1.4500 | lr 2.00e-05 | gates=[0.0002288314135512337, 0.22042138874530792] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999171495437622, 2.8186830604681745e-05, 5.4612337407888845e-05]\n",
      "step 990 | loss 1.5300 | lr 2.00e-05 | gates=[8.279916073661298e-05, 0.29403451085090637] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990567564964294, 0.0006621912471018732, 0.0002810740261338651]\n",
      "step 1000 | loss 5.4569 | lr 2.00e-05 | gates=[0.0009432653314433992, 0.9916613101959229] | mix=copy\n",
      "[after  E14_curriculum_warmstart_s1337_1755948671] alloc=10.70 GB | reserved=32.37 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.70 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "# Full E10–E14 sweep\n",
    "run_e10_14_sweep(steps=EXP_STEPS, seeds=SEEDS, base_mix=BASE_MIX, post_haystack=POST_HAY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T12:51:18.492312800Z",
     "start_time": "2025-08-23T08:39:21.472950400Z"
    }
   },
   "id": "857703c66bd17c72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.2 E15 a/b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "458a4a7c610e9cad"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# E15 configuration helpers (on top of the E11b tiered baseline)\n",
    "\n",
    "def set_e11b_baseline():\n",
    "    \"\"\"\n",
    "    E11b: tiered memories (shallower early, deeper late).\n",
    "    Leaves base model frozen and keeps 1 memory expert (K=1), fusion disabled.\n",
    "    \"\"\"\n",
    "    # Per-block memory shapes + mild gate temp biasing; free_bias nudges write-gate\n",
    "    CFG.per_block_cfg = [\n",
    "        {\"N\": 64,  \"W\": 32, \"R\": 1, \"gate_temp\": 1.0, \"free_bias\": +0.30},  # block 0 (shallower)\n",
    "        {\"N\": 128, \"W\": 64, \"R\": 2, \"gate_temp\": 0.9, \"free_bias\": -0.20},  # block 1 (deeper)\n",
    "    ]\n",
    "    # Single memory per block\n",
    "    CFG.mem_experts = 1\n",
    "    # No fusion (we found it heavy/unstable on 24GB)\n",
    "    CFG.fusion_enable = False\n",
    "\n",
    "    # Keep other knobs sane but unobtrusive\n",
    "    CFG.gate_reg_lambda = getattr(CFG, \"gate_reg_lambda\", 2e-4)   # mild entropy-ish gate reg\n",
    "    CFG.write_reg_lambda = 0.0                                    # no write sparsity in baseline\n",
    "    CFG.key_overlap_lambda = getattr(CFG, \"key_overlap_lambda\", 0.0)\n",
    "\n",
    "    # Clear expert overrides so blocks use their own N/W\n",
    "    for k in (\"expert_N\", \"expert_W\", \"expert_gate_temp\"):\n",
    "        if hasattr(CFG, k):\n",
    "            try: delattr(CFG, k)\n",
    "            except Exception: pass\n",
    "\n",
    "    # No forced gating and no mixture/temperature schedules for the baseline\n",
    "    for k in (\"force_g\", \"mixture_schedule\", \"gate_temp_schedule\", \"gate_reg_schedule\"):\n",
    "        if hasattr(CFG, k):\n",
    "            try: delattr(CFG, k)\n",
    "            except Exception: pass\n",
    "\n",
    "\n",
    "def set_e15a_write_sparse_light(lambda_write: float = 5e-5):\n",
    "    \"\"\"\n",
    "    E15a = E11b + light write-sparsity penalty (kept small to preserve HF quality).\n",
    "    \"\"\"\n",
    "    set_e11b_baseline()\n",
    "    CFG.write_reg_lambda = float(lambda_write)  # train loop should already include this term from E13 work\n",
    "\n",
    "\n",
    "def set_e15b_two_experts_smallW(K: int = 2, expert_W: int = 32, expert_gate_temp: float = 1.0):\n",
    "    \"\"\"\n",
    "    E15b = E11b + K=2 memory experts per block with small W to keep VRAM in check.\n",
    "    We let each block keep its own N from per_block_cfg; only override W for experts.\n",
    "    \"\"\"\n",
    "    set_e11b_baseline()\n",
    "    CFG.mem_experts = int(K)\n",
    "    # Use each block's N (per_block_cfg) by not setting expert_N at all.\n",
    "    CFG.expert_W = int(expert_W)                   # experts use narrower read/write width\n",
    "    CFG.expert_gate_temp = float(expert_gate_temp) # softer, avoids hard saturation\n",
    "\n",
    "    # Keep write reg off here to isolate effect of experts\n",
    "    CFG.write_reg_lambda = 0.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T20:37:42.271783400Z",
     "start_time": "2025-08-23T20:37:42.247770900Z"
    }
   },
   "id": "d16b1b8a11c08571"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import time, json, random, numpy as np, torch\n",
    "\n",
    "def run_e15_one(label: str,\n",
    "                setup_fn,\n",
    "                steps: int = 1000,\n",
    "                seed: int = 1337,\n",
    "                mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "                warmup_steps: int = None,\n",
    "                post_haystack: bool = False):\n",
    "    \"\"\"\n",
    "    Single E15 run wrapper: sets seed, applies setup_fn (E15a/b), starts dedicated TB run,\n",
    "    and calls train_experiment with your current training loop.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {label} | seed={seed} ===\")\n",
    "\n",
    "    # Repro\n",
    "    try:\n",
    "        set_seed(seed)\n",
    "    except NameError:\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Apply config\n",
    "    setup_fn()\n",
    "\n",
    "    # Unique TB run for this label/seed\n",
    "    try:\n",
    "        start_tb_run(label)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] start_tb_run failed or unavailable:\", e)\n",
    "\n",
    "    # Echo config snapshot to TB if available\n",
    "    if 'tb' in globals():\n",
    "        try:\n",
    "            tb.add_text(\"run/config\", json.dumps({\n",
    "                \"label\": label,\n",
    "                \"seed\": seed,\n",
    "                \"steps\": steps,\n",
    "                \"mixture\": list(mixture_weights),\n",
    "                \"per_block_cfg\": getattr(CFG, \"per_block_cfg\", None),\n",
    "                \"mem_experts\": getattr(CFG, \"mem_experts\", 1),\n",
    "                \"expert_W\": getattr(CFG, \"expert_W\", None),\n",
    "                \"expert_gate_temp\": getattr(CFG, \"expert_gate_temp\", None),\n",
    "                \"gate_reg_lambda\": getattr(CFG, \"gate_reg_lambda\", None),\n",
    "                \"write_reg_lambda\": getattr(CFG, \"write_reg_lambda\", None),\n",
    "            }, indent=2), 0)\n",
    "        except Exception as e:\n",
    "            print(\"[warn] TB add_text skipped:\", e)\n",
    "\n",
    "    # Clean VRAM between runs\n",
    "    try: free_head_and_cache()\n",
    "    except Exception: pass\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # Train\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=(warmup_steps if warmup_steps is not None else max(10, steps//20)),\n",
    "        mixture_weights=mixture_weights,\n",
    "        mixture_schedule=getattr(CFG, \"mixture_schedule\", None),\n",
    "        gate_temp_schedule=getattr(CFG, \"gate_temp_schedule\", None),\n",
    "        gate_reg_schedule=getattr(CFG, \"gate_reg_schedule\", None),\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    # Optional post‑eval\n",
    "    if post_haystack:\n",
    "        try:\n",
    "            evaluate_haystack(head, steps=50, batch=16, T=256, vocab=1024, tb_step=steps, fast=True)\n",
    "        except Exception as e:\n",
    "            print(\"[warn] haystack eval skipped:\", e)\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}\")\n",
    "    try: free_head_and_cache()\n",
    "    except Exception: pass\n",
    "\n",
    "    return head, tok\n",
    "\n",
    "\n",
    "def run_e15_suite(steps: int = 1000, seeds=(1337, 2027, 4242),\n",
    "                  mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "                  warmup_steps: int = None,\n",
    "                  include_haystack: bool = True):\n",
    "    \"\"\"\n",
    "    Launch E15a (write-sparse‑light) and E15b (two experts with small W) for the given seeds.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for s in seeds:\n",
    "        # E15a\n",
    "        label_a = f\"E15a_write_sparse_light_s{s}_{int(time.time())}\"\n",
    "        run_e15_one(label_a, set_e15a_write_sparse_light,\n",
    "                    steps=steps, seed=s,\n",
    "                    mixture_weights=mixture_weights,\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    post_haystack=include_haystack)\n",
    "\n",
    "        # E15b\n",
    "        label_b = f\"E15b_two_experts_smallW_s{s}_{int(time.time())}\"\n",
    "        run_e15_one(label_b, set_e15b_two_experts_smallW,\n",
    "                    steps=steps, seed=s,\n",
    "                    mixture_weights=mixture_weights,\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    post_haystack=include_haystack)\n",
    "\n",
    "        results.append((label_a, label_b))\n",
    "        # small spacer to avoid TB dir collisions on fast filesystems\n",
    "        time.sleep(1.2)\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T20:39:11.850019400Z",
     "start_time": "2025-08-23T20:39:11.773020900Z"
    }
   },
   "id": "206bb4cde72e33f0"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E15a_write_sparse_light_s1337_1755981882 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-134442-E15a_write_sparse_light_s1337_1755981882\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15a_write_sparse_light_s1337_1755981882] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85443ce201ae4dd3bfdc1eef32d623d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.7963446378707886, 0.20365534722805023]\n",
      "step 10 | loss 9.2285 | lr 4.40e-05 | gates=[0.20365534722805023, 0.36654311418533325] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.97617107629776, 0.023828942328691483]\n",
      "step 20 | loss 1.8761 | lr 8.40e-05 | gates=[0.023828942328691483, 0.1664227694272995] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.07069626450538635, 0.929303765296936]\n",
      "step 30 | loss 6.2480 | lr 1.24e-04 | gates=[0.9293037056922913, 0.12432555109262466] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9162425994873047, 0.08375738561153412]\n",
      "step 40 | loss 6.0591 | lr 1.64e-04 | gates=[0.08375739306211472, 0.7469349503517151] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.544292151927948, 0.4557078778743744]\n",
      "step 50 | loss 6.6627 | lr 2.00e-04 | gates=[0.455707848072052, 0.15452229976654053] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.7967431545257568, 0.20325683057308197]\n",
      "step 60 | loss 2.4098 | lr 2.00e-04 | gates=[0.20325683057308197, 0.001003516255877912] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9413093328475952, 0.058690622448921204]\n",
      "step 70 | loss 2.4803 | lr 2.00e-04 | gates=[0.058690622448921204, 4.893942241324112e-05] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9459106922149658, 0.05408930778503418]\n",
      "step 80 | loss 6.2796 | lr 1.99e-04 | gates=[0.05408930778503418, 0.9569169282913208] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9918524026870728, 0.00814756378531456]\n",
      "step 90 | loss 6.1921 | lr 1.99e-04 | gates=[0.008147564716637135, 0.9878946542739868] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9851992130279541, 0.014800779521465302]\n",
      "step 100 | loss 6.0071 | lr 1.99e-04 | gates=[0.014800779521465302, 0.9850680828094482] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9793478846549988, 0.020652079954743385]\n",
      "step 110 | loss 5.4242 | lr 1.98e-04 | gates=[0.020652081817388535, 0.003936631139367819] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9940054416656494, 0.005994569975882769]\n",
      "step 120 | loss 5.5598 | lr 1.97e-04 | gates=[0.005994569510221481, 0.9973902702331543] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8131232261657715, 0.1868767887353897]\n",
      "step 130 | loss 2.2222 | lr 1.96e-04 | gates=[0.1868767887353897, 0.007895495742559433] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9835096001625061, 0.016490371897816658]\n",
      "step 140 | loss 8.3990 | lr 1.96e-04 | gates=[0.01649037003517151, 0.9289664030075073] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.5623968243598938, 0.437603235244751]\n",
      "step 150 | loss 1.7429 | lr 1.94e-04 | gates=[0.437603235244751, 0.20027726888656616] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.6964840888977051, 0.3035159111022949]\n",
      "step 160 | loss 1.8252 | lr 1.93e-04 | gates=[0.3035159111022949, 0.27549099922180176] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7811509966850281, 0.21884898841381073]\n",
      "step 170 | loss 1.8360 | lr 1.92e-04 | gates=[0.21884898841381073, 0.12285149842500687] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9126050472259521, 0.08739499002695084]\n",
      "step 180 | loss 1.8851 | lr 1.91e-04 | gates=[0.08739499002695084, 0.040213026106357574] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7918805480003357, 0.20811936259269714]\n",
      "step 190 | loss 2.1940 | lr 1.89e-04 | gates=[0.20811937749385834, 0.18945561349391937] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9634660482406616, 0.03653395175933838]\n",
      "step 200 | loss 6.4863 | lr 1.88e-04 | gates=[0.03653395175933838, 0.9764044284820557] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9841952323913574, 0.015804721042513847]\n",
      "step 210 | loss 6.1579 | lr 1.86e-04 | gates=[0.015804719179868698, 0.9981979131698608] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9689077138900757, 0.03109223023056984]\n",
      "step 220 | loss 5.7310 | lr 1.84e-04 | gates=[0.03109223023056984, 0.9951712489128113] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9624972939491272, 0.0375027060508728]\n",
      "step 230 | loss 5.7499 | lr 1.83e-04 | gates=[0.0375027060508728, 0.9993734359741211] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9585607051849365, 0.041439324617385864]\n",
      "step 240 | loss 5.8161 | lr 1.81e-04 | gates=[0.041439320892095566, 0.9990036487579346] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999062180519104, 0.0009378910763189197]\n",
      "step 250 | loss 1.7688 | lr 1.79e-04 | gates=[0.0009378910763189197, 0.05113182216882706] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9930678606033325, 0.0069321198388934135]\n",
      "step 260 | loss 5.1227 | lr 1.77e-04 | gates=[0.006932119373232126, 0.9987245798110962] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9833453893661499, 0.01665463298559189]\n",
      "step 270 | loss 5.8531 | lr 1.74e-04 | gates=[0.01665463112294674, 0.9094270467758179] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9984805583953857, 0.0015194523148238659]\n",
      "step 280 | loss 1.7725 | lr 1.72e-04 | gates=[0.001519452198408544, 0.0070122526958584785] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9975866079330444, 0.002413371577858925]\n",
      "step 290 | loss 1.8138 | lr 1.70e-04 | gates=[0.0024133718106895685, 0.06248673424124718] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9855287671089172, 0.014471227303147316]\n",
      "step 300 | loss 5.0552 | lr 1.67e-04 | gates=[0.014471227303147316, 0.999242901802063] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9855599403381348, 0.014440108090639114]\n",
      "step 310 | loss 1.6194 | lr 1.65e-04 | gates=[0.014440109021961689, 0.14441312849521637] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9691332578659058, 0.030866743996739388]\n",
      "step 320 | loss 1.0913 | lr 1.62e-04 | gates=[0.030866745859384537, 0.03805755451321602] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9937894344329834, 0.0062105609104037285]\n",
      "step 330 | loss 5.9605 | lr 1.60e-04 | gates=[0.006210561376065016, 0.9909877777099609] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9894459247589111, 0.010554014705121517]\n",
      "step 340 | loss 5.6615 | lr 1.57e-04 | gates=[0.010554013773798943, 0.9539052248001099] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9642500877380371, 0.03574986383318901]\n",
      "step 350 | loss 5.5207 | lr 1.54e-04 | gates=[0.03574986010789871, 0.9051554203033447] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9109421968460083, 0.0890578031539917]\n",
      "step 360 | loss 1.7543 | lr 1.52e-04 | gates=[0.0890577957034111, 0.06329117715358734] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.6333317160606384, 0.3666682541370392]\n",
      "step 370 | loss 1.5854 | lr 1.49e-04 | gates=[0.3666682243347168, 0.11062788963317871] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.995932936668396, 0.004067076835781336]\n",
      "step 380 | loss 5.5994 | lr 1.46e-04 | gates=[0.004067076835781336, 0.9682934880256653] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9905612468719482, 0.009438777342438698]\n",
      "step 390 | loss 5.6726 | lr 1.43e-04 | gates=[0.009438776411116123, 0.9159842133522034] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9948418140411377, 0.005158212501555681]\n",
      "step 400 | loss 5.6172 | lr 1.40e-04 | gates=[0.005158212035894394, 0.9241195917129517] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.99173903465271, 0.008260920643806458]\n",
      "step 410 | loss 5.1481 | lr 1.37e-04 | gates=[0.008260920643806458, 0.9998224377632141] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9915825724601746, 0.008417475037276745]\n",
      "step 420 | loss 1.6428 | lr 1.34e-04 | gates=[0.008417475037276745, 0.3872833251953125] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9824577569961548, 0.017542265355587006]\n",
      "step 430 | loss 5.5977 | lr 1.31e-04 | gates=[0.017542267218232155, 0.9997819662094116] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9889322519302368, 0.011067759245634079]\n",
      "step 440 | loss 5.1324 | lr 1.27e-04 | gates=[0.011067758314311504, 0.9996019601821899] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9609706401824951, 0.03902935981750488]\n",
      "step 450 | loss 5.4828 | lr 1.24e-04 | gates=[0.03902935981750488, 0.9999123811721802] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9804732203483582, 0.019526753574609756]\n",
      "step 460 | loss 1.8248 | lr 1.21e-04 | gates=[0.019526753574609756, 0.3027346134185791] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9464080333709717, 0.05359199643135071]\n",
      "step 470 | loss 5.1563 | lr 1.18e-04 | gates=[0.05359199643135071, 0.9997609257698059] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9899619221687317, 0.010038081556558609]\n",
      "step 480 | loss 4.9644 | lr 1.14e-04 | gates=[0.010038081556558609, 0.9997953772544861] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.951224148273468, 0.0487758032977581]\n",
      "step 490 | loss 1.4553 | lr 1.11e-04 | gates=[0.048775799572467804, 0.4313805103302002] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9781477451324463, 0.02185223624110222]\n",
      "step 500 | loss 5.5399 | lr 1.08e-04 | gates=[0.02185223437845707, 0.9989279508590698] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9685900807380676, 0.031409941613674164]\n",
      "step 510 | loss 5.4411 | lr 1.05e-04 | gates=[0.03140994533896446, 0.9989917278289795] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8044286370277405, 0.19557128846645355]\n",
      "step 520 | loss 1.7126 | lr 1.01e-04 | gates=[0.19557128846645355, 0.2651992738246918] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9558331966400146, 0.044166795909404755]\n",
      "step 530 | loss 5.5202 | lr 9.80e-05 | gates=[0.044166795909404755, 0.9968724846839905] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9649385809898376, 0.03506141155958176]\n",
      "step 540 | loss 5.4766 | lr 9.47e-05 | gates=[0.03506141155958176, 0.9449248313903809] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9886423349380493, 0.011357655748724937]\n",
      "step 550 | loss 5.0650 | lr 9.14e-05 | gates=[0.011357654817402363, 0.9029395580291748] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9401642084121704, 0.059835825115442276]\n",
      "step 560 | loss 5.4457 | lr 8.81e-05 | gates=[0.059835825115442276, 0.9775766730308533] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9909285306930542, 0.0090714693069458]\n",
      "step 570 | loss 4.9933 | lr 8.48e-05 | gates=[0.0090714693069458, 0.9296826124191284] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9721891283988953, 0.02781086415052414]\n",
      "step 580 | loss 6.0101 | lr 8.16e-05 | gates=[0.02781086415052414, 0.9231643676757812] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8760126233100891, 0.12398736923933029]\n",
      "step 590 | loss 1.4913 | lr 7.83e-05 | gates=[0.12398736923933029, 0.185137078166008] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9832648038864136, 0.016735156998038292]\n",
      "step 600 | loss 5.4402 | lr 7.51e-05 | gates=[0.016735155135393143, 0.9578976631164551] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9070713520050049, 0.09292862564325333]\n",
      "step 610 | loss 1.4050 | lr 7.19e-05 | gates=[0.09292862564325333, 0.13105174899101257] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9690600037574768, 0.0309399776160717]\n",
      "step 620 | loss 5.5649 | lr 6.88e-05 | gates=[0.0309399776160717, 0.979743242263794] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9812583923339844, 0.018741609528660774]\n",
      "step 630 | loss 5.4752 | lr 6.57e-05 | gates=[0.018741611391305923, 0.9044052362442017] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8118642568588257, 0.18813571333885193]\n",
      "step 640 | loss 1.4964 | lr 6.26e-05 | gates=[0.18813571333885193, 0.21051257848739624] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9932841062545776, 0.006715870462357998]\n",
      "step 650 | loss 5.0041 | lr 5.95e-05 | gates=[0.006715870462357998, 0.9550882577896118] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9934987425804138, 0.006501231342554092]\n",
      "step 660 | loss 4.8973 | lr 5.65e-05 | gates=[0.006501231342554092, 0.9140235185623169] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9879111647605896, 0.012088853865861893]\n",
      "step 670 | loss 4.9266 | lr 5.36e-05 | gates=[0.012088854797184467, 0.9839677810668945] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9936442375183105, 0.006355769466608763]\n",
      "step 680 | loss 4.9749 | lr 5.07e-05 | gates=[0.006355769000947475, 0.8355433940887451] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9283334016799927, 0.07166658341884613]\n",
      "step 690 | loss 1.5049 | lr 4.78e-05 | gates=[0.07166658341884613, 0.17104722559452057] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9005627632141113, 0.0994371548295021]\n",
      "step 700 | loss 1.6736 | lr 4.50e-05 | gates=[0.0994371548295021, 0.19472600519657135] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8930487036705017, 0.10695134103298187]\n",
      "step 710 | loss 1.1601 | lr 4.23e-05 | gates=[0.10695134103298187, 0.17462988197803497] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8290078043937683, 0.17099221050739288]\n",
      "step 720 | loss 1.4906 | lr 3.96e-05 | gates=[0.1709921956062317, 0.1884489357471466] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8915176391601562, 0.10848233103752136]\n",
      "step 730 | loss 5.0263 | lr 3.70e-05 | gates=[0.10848233848810196, 0.9038986563682556] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9868507385253906, 0.01314929686486721]\n",
      "step 740 | loss 4.8767 | lr 3.45e-05 | gates=[0.013149295933544636, 0.9490168690681458] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8476329445838928, 0.15236704051494598]\n",
      "step 750 | loss 1.3690 | lr 3.20e-05 | gates=[0.15236705541610718, 0.24145719408988953] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9819517731666565, 0.018048245459794998]\n",
      "step 760 | loss 5.4767 | lr 2.96e-05 | gates=[0.018048245459794998, 0.9032389521598816] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9829896092414856, 0.017010340467095375]\n",
      "step 770 | loss 5.5083 | lr 2.73e-05 | gates=[0.017010340467095375, 0.9440609216690063] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9861738681793213, 0.013826148584485054]\n",
      "step 780 | loss 4.8804 | lr 2.51e-05 | gates=[0.013826148584485054, 0.9524718523025513] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9399138689041138, 0.06008613854646683]\n",
      "step 790 | loss 4.9299 | lr 2.29e-05 | gates=[0.06008613482117653, 0.9082645177841187] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9243195056915283, 0.07568047940731049]\n",
      "step 800 | loss 4.8719 | lr 2.09e-05 | gates=[0.07568047195672989, 0.8931096792221069] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8628315925598145, 0.13716840744018555]\n",
      "step 810 | loss 1.3597 | lr 2.00e-05 | gates=[0.13716840744018555, 0.14701619744300842] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9737775325775146, 0.026222430169582367]\n",
      "step 820 | loss 5.4519 | lr 2.00e-05 | gates=[0.026222428306937218, 0.9579750895500183] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9214016199111938, 0.07859843969345093]\n",
      "step 830 | loss 4.9117 | lr 2.00e-05 | gates=[0.07859843969345093, 0.9362479448318481] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9742921590805054, 0.025707807391881943]\n",
      "step 840 | loss 5.4635 | lr 2.00e-05 | gates=[0.025707809254527092, 0.9084347486495972] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.7639150619506836, 0.23608484864234924]\n",
      "step 850 | loss 1.4356 | lr 2.00e-05 | gates=[0.23608484864234924, 0.22888821363449097] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9899429082870483, 0.010057130828499794]\n",
      "step 860 | loss 4.8850 | lr 2.00e-05 | gates=[0.010057131759822369, 0.9511280059814453] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9707900881767273, 0.029209919273853302]\n",
      "step 870 | loss 5.4491 | lr 2.00e-05 | gates=[0.029209917411208153, 0.9598574638366699] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9870679378509521, 0.012932052835822105]\n",
      "step 880 | loss 4.8584 | lr 2.00e-05 | gates=[0.012932050973176956, 0.9887281656265259] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9186447858810425, 0.08135524392127991]\n",
      "step 890 | loss 5.0607 | lr 2.00e-05 | gates=[0.08135524392127991, 0.9740326404571533] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8758689165115356, 0.12413109093904495]\n",
      "step 900 | loss 4.7677 | lr 2.00e-05 | gates=[0.12413109093904495, 0.9081529378890991] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9171795845031738, 0.08282042294740677]\n",
      "step 910 | loss 4.8807 | lr 2.00e-05 | gates=[0.08282042294740677, 0.8298822641372681] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8350169658660889, 0.16498304903507233]\n",
      "step 920 | loss 1.1249 | lr 2.00e-05 | gates=[0.16498304903507233, 0.13463248312473297] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8777298331260681, 0.12227015197277069]\n",
      "step 930 | loss 1.4089 | lr 2.00e-05 | gates=[0.12227015197277069, 0.13244973123073578] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7881342768669128, 0.21186567842960358]\n",
      "step 940 | loss 1.3358 | lr 2.00e-05 | gates=[0.21186567842960358, 0.22766943275928497] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9550743103027344, 0.04492568224668503]\n",
      "step 950 | loss 4.9872 | lr 2.00e-05 | gates=[0.04492568224668503, 0.9344673156738281] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9865192174911499, 0.013480805791914463]\n",
      "step 960 | loss 4.8855 | lr 2.00e-05 | gates=[0.013480804860591888, 0.9027618765830994] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.98902428150177, 0.01097569428384304]\n",
      "step 970 | loss 4.9042 | lr 2.00e-05 | gates=[0.010975693352520466, 0.953079104423523] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9826470613479614, 0.017352916300296783]\n",
      "step 980 | loss 5.4106 | lr 2.00e-05 | gates=[0.017352918162941933, 0.9504237174987793] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8331301808357239, 0.16686975955963135]\n",
      "step 990 | loss 1.4152 | lr 2.00e-05 | gates=[0.16686975955963135, 0.18204466998577118] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9898706674575806, 0.010129367932677269]\n",
      "step 1000 | loss 4.9327 | lr 2.00e-05 | gates=[0.010129368863999844, 0.9142166972160339] | mix=nback\n",
      "[after  E15a_write_sparse_light_s1337_1755981882] alloc=9.63 GB | reserved=25.05 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.63 GB | reserved=9.65 GB | free=14.71 GB | total=25.77 GB\n",
      "\n",
      "=== E15b_two_experts_smallW_s1337_1755983525 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250823-141205-E15b_two_experts_smallW_s1337_1755983525\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15b_two_experts_smallW_s1337_1755983525] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1dda16f85cce46e78579fda066ffdaed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.9076195955276489, 0.0313478522002697, 0.061032485216856]\n",
      "step 10 | loss 2.2204 | lr 4.40e-05 | gates=[0.0923803374171257, 0.1458486020565033] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9883792400360107, 0.0071257054805755615, 0.004495005123317242]\n",
      "step 20 | loss 7.1180 | lr 8.40e-05 | gates=[0.011620711535215378, 0.7565804123878479] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990176558494568, 0.0004991888999938965, 0.00048318770132027566]\n",
      "step 30 | loss 1.9076 | lr 1.24e-04 | gates=[0.0009823766304180026, 0.28592449426651] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9561795592308044, 0.03500876575708389, 0.008811662904918194]\n",
      "step 40 | loss 7.9232 | lr 1.64e-04 | gates=[0.04382042586803436, 0.31809091567993164] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.989941418170929, 0.008588459342718124, 0.0014700754545629025]\n",
      "step 50 | loss 6.1138 | lr 2.00e-04 | gates=[0.010058535262942314, 0.7343466281890869] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9955825805664062, 0.0024464537855237722, 0.0019709516782313585]\n",
      "step 60 | loss 1.9509 | lr 2.00e-04 | gates=[0.004417405463755131, 0.23924680054187775] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.08504796028137207, 0.9042969942092896, 0.010655074380338192]\n",
      "step 70 | loss 6.9991 | lr 2.00e-04 | gates=[0.9149520397186279, 0.4671628177165985] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992942214012146, 0.00045241863699629903, 0.00025330905918963253]\n",
      "step 80 | loss 1.9199 | lr 1.99e-04 | gates=[0.000705727725289762, 0.33547499775886536] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985641837120056, 0.0007816883735358715, 0.000654078321531415]\n",
      "step 90 | loss 1.7494 | lr 1.99e-04 | gates=[0.0014357668114826083, 0.05300478637218475] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986510872840881, 0.0008444240083917975, 0.000504470313899219]\n",
      "step 100 | loss 1.7575 | lr 1.99e-04 | gates=[0.0013488944387063384, 0.08299092948436737] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.04929053783416748, 0.9253643751144409, 0.02534511685371399]\n",
      "step 110 | loss 6.7739 | lr 1.98e-04 | gates=[0.9507094621658325, 0.9558722972869873] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.997951328754425, 0.0009590864647179842, 0.0010895031737163663]\n",
      "step 120 | loss 1.8581 | lr 1.97e-04 | gates=[0.0020485897548496723, 0.07576876133680344] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9956716895103455, 0.001242538564838469, 0.003085814416408539]\n",
      "step 130 | loss 1.4621 | lr 1.96e-04 | gates=[0.004328353330492973, 0.4092852771282196] | mix=hf\n",
      "  [experts] block0 top=2 pi=[0.006784426048398018, 0.07065306603908539, 0.9225625395774841]\n",
      "step 140 | loss 6.2024 | lr 1.96e-04 | gates=[0.9932155609130859, 0.6645587086677551] | mix=nback\n",
      "  [experts] block0 top=2 pi=[0.0706104189157486, 0.1453244984149933, 0.7840650677680969]\n",
      "step 150 | loss 7.8058 | lr 1.94e-04 | gates=[0.9293895363807678, 0.1214943677186966] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.15754561126232147, 0.4878648519515991, 0.3545895218849182]\n",
      "step 160 | loss 6.4411 | lr 1.93e-04 | gates=[0.8424544334411621, 0.538631796836853] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997970461845398, 0.00016935922030825168, 3.359679612913169e-05]\n",
      "step 170 | loss 8.8931 | lr 1.92e-04 | gates=[0.00020295599824748933, 0.9990594387054443] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9981340765953064, 0.0013843323104083538, 0.0004816238069906831]\n",
      "step 180 | loss 5.9283 | lr 1.91e-04 | gates=[0.0018659561173990369, 0.9959933757781982] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9953022003173828, 0.002614999655634165, 0.0020828177221119404]\n",
      "step 190 | loss 2.4152 | lr 1.89e-04 | gates=[0.004697817377746105, 0.0019379170844331384] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999433755874634, 3.5730088711716235e-05, 2.0921414034091868e-05]\n",
      "step 200 | loss 7.1427 | lr 1.88e-04 | gates=[5.66515009268187e-05, 0.6765600442886353] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996813535690308, 0.00024396258231718093, 7.468149851774797e-05]\n",
      "step 210 | loss 7.1833 | lr 1.86e-04 | gates=[0.00031864410266280174, 0.9971938133239746] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9995124936103821, 0.00023420556681230664, 0.00025332215591333807]\n",
      "step 220 | loss 6.6055 | lr 1.84e-04 | gates=[0.00048752769362181425, 0.9999566078186035] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999966621398926, 3.1302820389100816e-06, 2.0596242222836736e-07]\n",
      "step 230 | loss 6.6653 | lr 1.83e-04 | gates=[3.3362448448315263e-06, 0.9999475479125977] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997909069061279, 0.00019181135576218367, 1.7298580132774077e-05]\n",
      "step 240 | loss 1.8977 | lr 1.81e-04 | gates=[0.00020910994498990476, 0.572648286819458] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991915225982666, 0.0007614562637172639, 4.701747093349695e-05]\n",
      "step 250 | loss 7.6201 | lr 1.79e-04 | gates=[0.0008084737928584218, 0.9999905824661255] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999985694885254, 1.2329420542300795e-06, 1.8581690142127627e-07]\n",
      "step 260 | loss 5.4262 | lr 1.77e-04 | gates=[1.4187588703862275e-06, 0.9999924898147583] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999943375587463, 5.514619260793552e-06, 1.5315089285650174e-07]\n",
      "step 270 | loss 5.4407 | lr 1.74e-04 | gates=[5.667769073625095e-06, 0.9996857643127441] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999328851699829, 6.406287138815969e-05, 3.029256049558171e-06]\n",
      "step 280 | loss 6.3363 | lr 1.72e-04 | gates=[6.70921363052912e-05, 0.99997878074646] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996839165687561, 0.00027301604859530926, 4.316273771109991e-05]\n",
      "step 290 | loss 1.7656 | lr 1.70e-04 | gates=[0.00031617877539247274, 0.01603216864168644] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998168349266052, 0.00016959162894636393, 1.3602544640889391e-05]\n",
      "step 300 | loss 5.5491 | lr 1.67e-04 | gates=[0.0001831941626733169, 0.9998716115951538] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999998211860657, 1.6893658028038772e-07, 1.3875506965632667e-08]\n",
      "step 310 | loss 5.1967 | lr 1.65e-04 | gates=[1.8281208724602038e-07, 0.9999791383743286] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999926090240479, 6.992393082327908e-06, 4.3863468590643606e-07]\n",
      "step 320 | loss 5.6310 | lr 1.62e-04 | gates=[7.431028734572465e-06, 0.9995341897010803] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999963045120239, 3.6344727050163783e-06, 8.968710574208671e-08]\n",
      "step 330 | loss 5.2333 | lr 1.60e-04 | gates=[3.724159569173935e-06, 0.9993530511856079] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999571442604065, 3.9383045077556744e-05, 3.45811258739559e-06]\n",
      "step 340 | loss 5.5542 | lr 1.57e-04 | gates=[4.284115493646823e-05, 0.9999858736991882] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999979138374329, 1.5499955452469294e-06, 5.373117915041803e-07]\n",
      "step 350 | loss 5.3319 | lr 1.54e-04 | gates=[2.0873073935945285e-06, 0.99989914894104] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999528706073761, 0.0004620415857061744, 9.279406185669359e-06]\n",
      "step 360 | loss 4.8618 | lr 1.52e-04 | gates=[0.000471320963697508, 0.7440773844718933] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997442960739136, 0.0002516583481337875, 3.9957635635801125e-06]\n",
      "step 370 | loss 1.6030 | lr 1.49e-04 | gates=[0.00025565410032868385, 0.2035513073205948] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999813437461853, 1.1039813216484617e-05, 7.647777238162234e-06]\n",
      "step 380 | loss 5.7756 | lr 1.46e-04 | gates=[1.868758954515215e-05, 0.9986289739608765] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999722242355347, 2.5483068384346552e-05, 2.3111408609111095e-06]\n",
      "step 390 | loss 5.6087 | lr 1.43e-04 | gates=[2.7794207198894583e-05, 0.9977822303771973] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999840259552002, 1.1942217497562524e-05, 4.09952463087393e-06]\n",
      "step 400 | loss 5.8560 | lr 1.40e-04 | gates=[1.604174030944705e-05, 0.9988172054290771] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999972939491272, 2.5293509679613635e-05, 1.8393133132121875e-06]\n",
      "step 410 | loss 6.1917 | lr 1.37e-04 | gates=[2.713282265176531e-05, 0.9989396333694458] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996437430381775, 0.000330645649228245, 2.5556664695614018e-05]\n",
      "step 420 | loss 0.9978 | lr 1.34e-04 | gates=[0.000356202304828912, 0.03564184159040451] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995341300964355, 0.000437440030509606, 2.8439380912459455e-05]\n",
      "step 430 | loss 1.4421 | lr 1.31e-04 | gates=[0.0004658794205170125, 0.0663776844739914] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999957084655762, 4.177859409537632e-06, 1.267285085759795e-07]\n",
      "step 440 | loss 5.2548 | lr 1.27e-04 | gates=[4.304587946535321e-06, 0.9993281364440918] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9989193677902222, 0.0010545419063419104, 2.6164567316300236e-05]\n",
      "step 450 | loss 1.7614 | lr 1.24e-04 | gates=[0.0010807065991684794, 0.3091167211532593] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986750483512878, 0.0013059228658676147, 1.9036931917071342e-05]\n",
      "step 460 | loss 1.6731 | lr 1.21e-04 | gates=[0.001324959914200008, 0.3037487864494324] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9983716011047363, 0.0016226947773247957, 5.637336471409071e-06]\n",
      "step 470 | loss 1.6428 | lr 1.18e-04 | gates=[0.0016283320728689432, 0.07483451813459396] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9973404407501221, 0.002638930222019553, 2.0604098608600907e-05]\n",
      "step 480 | loss 1.6212 | lr 1.14e-04 | gates=[0.002659534104168415, 0.22613346576690674] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999983906745911, 1.432754288543947e-06, 2.0909911313538032e-07]\n",
      "step 490 | loss 4.9785 | lr 1.11e-04 | gates=[1.6418534869444557e-06, 0.9397236108779907] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999875426292419, 1.2223612429806963e-05, 2.736452415774693e-07]\n",
      "step 500 | loss 5.3074 | lr 1.08e-04 | gates=[1.249725573870819e-05, 0.9684278964996338] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999666213989258, 3.285499769845046e-05, 5.529412874238915e-07]\n",
      "step 510 | loss 5.5726 | lr 1.05e-04 | gates=[3.340794501127675e-05, 0.9992233514785767] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.988744854927063, 0.011224903166294098, 3.0242516004364006e-05]\n",
      "step 520 | loss 1.6188 | lr 1.01e-04 | gates=[0.011255146935582161, 0.4668228030204773] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9983961582183838, 0.001550748129375279, 5.311060158419423e-05]\n",
      "step 530 | loss 5.1315 | lr 9.80e-05 | gates=[0.0016038588946685195, 0.9991815090179443] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997988343238831, 0.00016552368470001966, 3.563525388017297e-05]\n",
      "step 540 | loss 5.1950 | lr 9.47e-05 | gates=[0.00020115893858019263, 0.9975367188453674] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9959309697151184, 0.003988190088421106, 8.087549940682948e-05]\n",
      "step 550 | loss 1.5952 | lr 9.14e-05 | gates=[0.004069065675139427, 0.02786002866923809] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9730237722396851, 0.02694540098309517, 3.088081211899407e-05]\n",
      "step 560 | loss 1.5775 | lr 8.81e-05 | gates=[0.02697627991437912, 0.01976817660033703] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9774025082588196, 0.02249244414269924, 0.0001050224163918756]\n",
      "step 570 | loss 1.4563 | lr 8.48e-05 | gates=[0.02259746380150318, 0.021961169317364693] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9929547905921936, 0.007014723960310221, 3.0439210604527034e-05]\n",
      "step 580 | loss 1.5341 | lr 8.16e-05 | gates=[0.007045163307338953, 0.08121547102928162] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9730822443962097, 0.026888741180300713, 2.906149711634498e-05]\n",
      "step 590 | loss 1.4513 | lr 7.83e-05 | gates=[0.026917804032564163, 0.10956985503435135] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996312856674194, 0.0003669002908281982, 1.8251281517223106e-06]\n",
      "step 600 | loss 5.5065 | lr 7.51e-05 | gates=[0.00036872539203613997, 0.9106500744819641] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997174739837646, 0.00027356372447684407, 8.941749001678545e-06]\n",
      "step 610 | loss 5.5254 | lr 7.19e-05 | gates=[0.00028250543982721865, 0.9295686483383179] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993650913238525, 0.0002333454613108188, 0.00040154141606763005]\n",
      "step 620 | loss 5.4766 | lr 6.88e-05 | gates=[0.0006348869646899402, 0.9958563446998596] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998111128807068, 0.0001874333102023229, 1.4122124412097037e-06]\n",
      "step 630 | loss 4.9462 | lr 6.57e-05 | gates=[0.00018884550081565976, 0.9865071773529053] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9992506504058838, 0.0006316712824627757, 0.00011768141121137887]\n",
      "step 640 | loss 5.4770 | lr 6.26e-05 | gates=[0.0007493526791222394, 0.920304536819458] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9853110313415527, 0.014607632532715797, 8.13504375400953e-05]\n",
      "step 650 | loss 1.3514 | lr 5.95e-05 | gates=[0.014688982628285885, 0.13873076438903809] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999698400497437, 2.9712420655414462e-05, 4.233864387970243e-07]\n",
      "step 660 | loss 5.1975 | lr 5.65e-05 | gates=[3.013580499100499e-05, 0.8486940860748291] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999831914901733, 1.633282590773888e-05, 4.7066984620869334e-07]\n",
      "step 670 | loss 4.9515 | lr 5.36e-05 | gates=[1.6803498510853387e-05, 0.9505519866943359] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995467662811279, 0.0004506642580963671, 2.6115176297025755e-06]\n",
      "step 680 | loss 4.9934 | lr 5.07e-05 | gates=[0.00045327580301091075, 0.9856042861938477] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9869792461395264, 0.013004557229578495, 1.6209603927563876e-05]\n",
      "step 690 | loss 1.1803 | lr 4.78e-05 | gates=[0.013020766898989677, 0.2708114683628082] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999032020568848, 9.365617006551474e-05, 3.0849325867166044e-06]\n",
      "step 700 | loss 5.0723 | lr 4.50e-05 | gates=[9.674111061030999e-05, 0.8750773668289185] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9750679135322571, 0.024903912097215652, 2.8166341508040205e-05]\n",
      "step 710 | loss 1.5343 | lr 4.23e-05 | gates=[0.024932077154517174, 0.2552069425582886] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992184638977051, 0.0007596975774504244, 2.1854822989553213e-05]\n",
      "step 720 | loss 5.4748 | lr 3.96e-05 | gates=[0.0007815523422323167, 0.9918226003646851] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997570514678955, 0.00023886535200290382, 4.087205525138415e-06]\n",
      "step 730 | loss 5.0758 | lr 3.70e-05 | gates=[0.00024295256298501045, 0.9814834594726562] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9763050675392151, 0.023638349026441574, 5.662369221681729e-05]\n",
      "step 740 | loss 1.2281 | lr 3.45e-05 | gates=[0.023694973438978195, 0.3875437080860138] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995664358139038, 0.0004197904490865767, 1.3758016393694561e-05]\n",
      "step 750 | loss 4.9506 | lr 3.20e-05 | gates=[0.0004335484409239143, 0.9201695919036865] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9817742109298706, 0.018204795196652412, 2.1003219444537535e-05]\n",
      "step 760 | loss 1.5727 | lr 2.96e-05 | gates=[0.01822579838335514, 0.2292487770318985] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9830172061920166, 0.016945820301771164, 3.697695501614362e-05]\n",
      "step 770 | loss 1.4705 | lr 2.73e-05 | gates=[0.016982799395918846, 0.24966613948345184] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.978531002998352, 0.021396582946181297, 7.246429595397785e-05]\n",
      "step 780 | loss 1.3187 | lr 2.51e-05 | gates=[0.02146904729306698, 0.17623986303806305] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9809049963951111, 0.018957825377583504, 0.00013713043881580234]\n",
      "step 790 | loss 1.3113 | lr 2.29e-05 | gates=[0.019094955176115036, 0.3424072265625] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9755515456199646, 0.024314196780323982, 0.0001341751922154799]\n",
      "step 800 | loss 1.4109 | lr 2.09e-05 | gates=[0.024448376148939133, 0.26625901460647583] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9808661937713623, 0.019101908430457115, 3.198409831384197e-05]\n",
      "step 810 | loss 1.4509 | lr 2.00e-05 | gates=[0.01913389191031456, 0.5170175433158875] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999727725982666, 0.00027022205176763237, 2.1351770556066185e-06]\n",
      "step 820 | loss 4.8569 | lr 2.00e-05 | gates=[0.00027235725428909063, 0.9636635184288025] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999626874923706, 3.622730218921788e-05, 1.1438129376983852e-06]\n",
      "step 830 | loss 4.9003 | lr 2.00e-05 | gates=[3.737111546797678e-05, 0.9825548529624939] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996960759162903, 0.0001744055189192295, 0.00012950591917615384]\n",
      "step 840 | loss 5.4388 | lr 2.00e-05 | gates=[0.00030391148175112903, 0.9800350666046143] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9973881244659424, 0.0025896839797496796, 2.2159874788485467e-05]\n",
      "step 850 | loss 5.0634 | lr 2.00e-05 | gates=[0.002611844101920724, 0.962906002998352] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9755808115005493, 0.02438199892640114, 3.7239013181533664e-05]\n",
      "step 860 | loss 1.3968 | lr 2.00e-05 | gates=[0.024419238790869713, 0.2721959352493286] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997513294219971, 0.00024483483866788447, 3.852990630548447e-06]\n",
      "step 870 | loss 4.8831 | lr 2.00e-05 | gates=[0.0002486878074705601, 0.8797522783279419] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9986682534217834, 0.00127701077144593, 5.477271042764187e-05]\n",
      "step 880 | loss 4.8648 | lr 2.00e-05 | gates=[0.0013317835982888937, 0.933998167514801] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999575018882751, 4.1930616134777665e-05, 5.76230661408772e-07]\n",
      "step 890 | loss 5.5246 | lr 2.00e-05 | gates=[4.2506842873990536e-05, 0.9680792093276978] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996093511581421, 0.0003782080893870443, 1.2489448636188172e-05]\n",
      "step 900 | loss 5.0841 | lr 2.00e-05 | gates=[0.00039069753256626427, 0.9757704138755798] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997533559799194, 0.000242578360484913, 4.1167045310430694e-06]\n",
      "step 910 | loss 4.8705 | lr 2.00e-05 | gates=[0.0002466950681991875, 0.9538053274154663] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9782278537750244, 0.021718651056289673, 5.34732716914732e-05]\n",
      "step 920 | loss 1.3233 | lr 2.00e-05 | gates=[0.021772123873233795, 0.20968373119831085] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998997449874878, 8.690118556842208e-05, 1.3363669495447539e-05]\n",
      "step 930 | loss 4.8665 | lr 2.00e-05 | gates=[0.00010026486415881664, 0.9138118028640747] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999790191650391, 2.0804753148695454e-05, 2.288874298983501e-07]\n",
      "step 940 | loss 4.9257 | lr 2.00e-05 | gates=[2.1033638404333033e-05, 0.9385591745376587] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999826192855835, 0.00015810754848644137, 1.570303356857039e-05]\n",
      "step 950 | loss 5.4719 | lr 2.00e-05 | gates=[0.00017381057841703296, 0.9685536623001099] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9817678928375244, 0.018207808956503868, 2.4239108824986033e-05]\n",
      "step 960 | loss 1.3071 | lr 2.00e-05 | gates=[0.01823204755783081, 0.27037370204925537] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986684322357178, 0.0013293465599417686, 2.2524352516484214e-06]\n",
      "step 970 | loss 5.0354 | lr 2.00e-05 | gates=[0.0013315988471731544, 0.9433053731918335] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9758602380752563, 0.024115411564707756, 2.428804691589903e-05]\n",
      "step 980 | loss 1.1067 | lr 2.00e-05 | gates=[0.024139700457453728, 0.2977216839790344] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998792409896851, 0.00012046895426465198, 3.3623382478253916e-07]\n",
      "step 990 | loss 4.9770 | lr 2.00e-05 | gates=[0.00012080519809387624, 0.9588433504104614] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998522996902466, 0.00014691695105284452, 7.218539508357935e-07]\n",
      "step 1000 | loss 4.8821 | lr 2.00e-05 | gates=[0.00014763879880774766, 0.903540849685669] | mix=nback\n",
      "[after  E15b_two_experts_smallW_s1337_1755983525] alloc=10.69 GB | reserved=31.58 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.69 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "\n",
      "=== E15a_write_sparse_light_s2027_1755987068 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250823-151108-E15a_write_sparse_light_s2027_1755987068\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15a_write_sparse_light_s2027_1755987068] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0ba62ad2fd84410bdd45c30cc7a3e03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.6549537181854248, 0.3450463116168976]\n",
      "step 10 | loss 9.0408 | lr 4.40e-05 | gates=[0.3450462818145752, 0.3071179986000061] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9978355169296265, 0.0021645643282681704]\n",
      "step 20 | loss 4.5686 | lr 8.40e-05 | gates=[0.002164564561098814, 0.7058046460151672] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.415398508310318, 0.5846015214920044]\n",
      "step 30 | loss 6.5673 | lr 1.24e-04 | gates=[0.5846015214920044, 0.32852187752723694] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.6869228482246399, 0.3130771517753601]\n",
      "step 40 | loss 6.1414 | lr 1.64e-04 | gates=[0.3130771517753601, 0.4550093710422516] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.02594377100467682, 0.9740562438964844]\n",
      "step 50 | loss 5.9816 | lr 2.00e-04 | gates=[0.9740562438964844, 0.2442379593849182] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.013701247051358223, 0.9862987399101257]\n",
      "step 60 | loss 6.8492 | lr 2.00e-04 | gates=[0.9862987399101257, 0.8319540023803711] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9351589679718018, 0.06484110653400421]\n",
      "step 70 | loss 2.0121 | lr 2.00e-04 | gates=[0.06484110653400421, 9.089086461244733e-07] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.01102808490395546, 0.9889718890190125]\n",
      "step 80 | loss 7.4357 | lr 1.99e-04 | gates=[0.9889719486236572, 2.010202024393948e-06] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4606444835662842, 0.5393555164337158]\n",
      "step 90 | loss 6.9770 | lr 1.99e-04 | gates=[0.5393555164337158, 9.873587259789929e-05] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.4652150273323059, 0.5347850322723389]\n",
      "step 100 | loss 5.7299 | lr 1.99e-04 | gates=[0.5347849726676941, 0.00046460030716843903] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.2545735239982605, 0.7454264163970947]\n",
      "step 110 | loss 6.1808 | lr 1.98e-04 | gates=[0.7454264163970947, 9.527400379738538e-08] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.05797024443745613, 0.9420297741889954]\n",
      "step 120 | loss 6.7059 | lr 1.97e-04 | gates=[0.9420297741889954, 0.0013664286816492677] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.44398894906044006, 0.5560110807418823]\n",
      "step 130 | loss 6.9346 | lr 1.96e-04 | gates=[0.5560110807418823, 6.545889482367784e-05] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.4287744462490082, 0.5712255239486694]\n",
      "step 140 | loss 6.4961 | lr 1.96e-04 | gates=[0.5712255239486694, 4.2778516217367724e-05] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.11973462253808975, 0.8802653551101685]\n",
      "step 150 | loss 6.4029 | lr 1.94e-04 | gates=[0.8802653551101685, 0.0035566752776503563] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.2691264748573303, 0.7308735847473145]\n",
      "step 160 | loss 6.1197 | lr 1.93e-04 | gates=[0.7308735251426697, 9.122667279370944e-07] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.985948920249939, 0.014051160775125027]\n",
      "step 170 | loss 1.8276 | lr 1.92e-04 | gates=[0.014051160775125027, 0.0001606218866072595] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7551758885383606, 0.24482408165931702]\n",
      "step 180 | loss 2.0438 | lr 1.91e-04 | gates=[0.24482408165931702, 0.002305845730006695] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7873165011405945, 0.2126835286617279]\n",
      "step 190 | loss 1.6852 | lr 1.89e-04 | gates=[0.2126835137605667, 0.0012625795789062977] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7064387202262878, 0.29356124997138977]\n",
      "step 200 | loss 6.0703 | lr 1.88e-04 | gates=[0.2935612201690674, 0.0019236602820456028] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.6975327134132385, 0.3024672865867615]\n",
      "step 210 | loss 1.6228 | lr 1.86e-04 | gates=[0.3024672567844391, 0.0018906911136582494] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8756469488143921, 0.1243530884385109]\n",
      "step 220 | loss 1.8847 | lr 1.84e-04 | gates=[0.1243530884385109, 0.0009813940851017833] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8452967405319214, 0.15470321476459503]\n",
      "step 230 | loss 1.7922 | lr 1.83e-04 | gates=[0.15470321476459503, 0.0012906574411317706] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.4747004210948944, 0.5252995491027832]\n",
      "step 240 | loss 6.0831 | lr 1.81e-04 | gates=[0.5252995491027832, 0.0004090742440894246] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.3848494291305542, 0.6151505708694458]\n",
      "step 250 | loss 6.7039 | lr 1.79e-04 | gates=[0.6151505708694458, 0.00025046252994798124] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.21042566001415253, 0.7895743250846863]\n",
      "step 260 | loss 5.8696 | lr 1.77e-04 | gates=[0.789574384689331, 3.830079731415026e-05] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4356893002986908, 0.5643106698989868]\n",
      "step 270 | loss 5.8438 | lr 1.74e-04 | gates=[0.5643106698989868, 0.0006661282386630774] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.32318538427352905, 0.676814615726471]\n",
      "step 280 | loss 5.9004 | lr 1.72e-04 | gates=[0.676814615726471, 0.0003511326212901622] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8423097133636475, 0.15769031643867493]\n",
      "step 290 | loss 1.7270 | lr 1.70e-04 | gates=[0.15769031643867493, 0.0006220300565473735] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.6488768458366394, 0.351123183965683]\n",
      "step 300 | loss 6.1839 | lr 1.67e-04 | gates=[0.351123183965683, 0.00350072979927063] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.23087269067764282, 0.7691273093223572]\n",
      "step 310 | loss 5.1209 | lr 1.65e-04 | gates=[0.7691273093223572, 0.001441824366338551] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.2877213656902313, 0.7122786045074463]\n",
      "step 320 | loss 5.2068 | lr 1.62e-04 | gates=[0.7122786045074463, 0.006999397184699774] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.600980818271637, 0.3990192413330078]\n",
      "step 330 | loss 6.0731 | lr 1.60e-04 | gates=[0.3990192413330078, 0.003974945284426212] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.43242645263671875, 0.5675735473632812]\n",
      "step 340 | loss 5.6330 | lr 1.57e-04 | gates=[0.5675735473632812, 0.0017343881772831082] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.823620080947876, 0.17637991905212402]\n",
      "step 350 | loss 1.5560 | lr 1.54e-04 | gates=[0.17637991905212402, 0.016293397173285484] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.35160112380981445, 0.6483988761901855]\n",
      "step 360 | loss 6.2541 | lr 1.52e-04 | gates=[0.6483988761901855, 0.0010486221872270107] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.15338456630706787, 0.8466154336929321]\n",
      "step 370 | loss 6.5356 | lr 1.49e-04 | gates=[0.8466154336929321, 0.0015098811127245426] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.7405024766921997, 0.2594975531101227]\n",
      "step 380 | loss 5.3897 | lr 1.46e-04 | gates=[0.2594975531101227, 0.006311377044767141] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9762410521507263, 0.023758910596370697]\n",
      "step 390 | loss 1.4068 | lr 1.43e-04 | gates=[0.023758908733725548, 0.0033779283985495567] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8795956373214722, 0.12040439993143082]\n",
      "step 400 | loss 1.7772 | lr 1.40e-04 | gates=[0.12040439248085022, 0.011047953739762306] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.168569415807724, 0.8314305543899536]\n",
      "step 410 | loss 6.1253 | lr 1.37e-04 | gates=[0.8314305543899536, 0.00796915590763092] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.3287962079048157, 0.6712037920951843]\n",
      "step 420 | loss 5.1789 | lr 1.34e-04 | gates=[0.6712038516998291, 0.017937446013092995] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9796160459518433, 0.0203839261084795]\n",
      "step 430 | loss 1.8779 | lr 1.31e-04 | gates=[0.0203839261084795, 0.005307375453412533] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9383538365364075, 0.06164613366127014]\n",
      "step 440 | loss 1.7708 | lr 1.27e-04 | gates=[0.06164614111185074, 0.010238510556519032] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.22038736939430237, 0.7796126008033752]\n",
      "step 450 | loss 5.8408 | lr 1.24e-04 | gates=[0.7796126008033752, 0.6125646233558655] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8614181876182556, 0.1385817676782608]\n",
      "step 460 | loss 1.5462 | lr 1.21e-04 | gates=[0.1385817676782608, 0.00629874411970377] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.4457014799118042, 0.5542985200881958]\n",
      "step 470 | loss 5.4581 | lr 1.18e-04 | gates=[0.5542985200881958, 0.006922830827534199] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4904704988002777, 0.5095294713973999]\n",
      "step 480 | loss 5.1731 | lr 1.14e-04 | gates=[0.5095295310020447, 0.026480786502361298] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.09750203043222427, 0.9024980068206787]\n",
      "step 490 | loss 5.7149 | lr 1.11e-04 | gates=[0.9024980068206787, 0.0024761795066297054] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.3022124171257019, 0.6977875828742981]\n",
      "step 500 | loss 5.1641 | lr 1.08e-04 | gates=[0.6977875232696533, 0.0036197477020323277] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.14073944091796875, 0.8592605590820312]\n",
      "step 510 | loss 5.1389 | lr 1.05e-04 | gates=[0.8592605590820312, 0.0025891000404953957] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.1611584722995758, 0.8388415575027466]\n",
      "step 520 | loss 5.1226 | lr 1.01e-04 | gates=[0.8388415575027466, 0.0020175566896796227] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.7574503421783447, 0.24254964292049408]\n",
      "step 530 | loss 1.7128 | lr 9.80e-05 | gates=[0.24254964292049408, 0.013240055181086063] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.15641354024410248, 0.8435864448547363]\n",
      "step 540 | loss 5.1628 | lr 9.47e-05 | gates=[0.8435864448547363, 0.0069934409111738205] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.5573588013648987, 0.44264116883277893]\n",
      "step 550 | loss 1.3949 | lr 9.14e-05 | gates=[0.44264116883277893, 0.048357244580984116] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.3886742889881134, 0.6113256812095642]\n",
      "step 560 | loss 5.4419 | lr 8.81e-05 | gates=[0.6113256812095642, 0.007448191754519939] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.04867001622915268, 0.9513299465179443]\n",
      "step 570 | loss 5.2139 | lr 8.48e-05 | gates=[0.9513299465179443, 0.0023705344647169113] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.20980000495910645, 0.7901999950408936]\n",
      "step 580 | loss 5.2235 | lr 8.16e-05 | gates=[0.7901999950408936, 0.0024604692589491606] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8741728067398071, 0.12582723796367645]\n",
      "step 590 | loss 1.3942 | lr 7.83e-05 | gates=[0.12582723796367645, 0.09800225496292114] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.34632015228271484, 0.6536798477172852]\n",
      "step 600 | loss 5.0914 | lr 7.51e-05 | gates=[0.6536798477172852, 0.013953806832432747] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.055123165249824524, 0.9448767900466919]\n",
      "step 610 | loss 5.0585 | lr 7.19e-05 | gates=[0.9448767900466919, 0.0046198926866054535] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8759995698928833, 0.12400038540363312]\n",
      "step 620 | loss 1.5348 | lr 6.88e-05 | gates=[0.12400040030479431, 0.07621355354785919] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.135079488158226, 0.8649204969406128]\n",
      "step 630 | loss 5.2686 | lr 6.57e-05 | gates=[0.8649204969406128, 0.024228163063526154] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.349437415599823, 0.650562584400177]\n",
      "step 640 | loss 4.9813 | lr 6.26e-05 | gates=[0.6505625247955322, 0.013848144561052322] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9047181606292725, 0.09528191387653351]\n",
      "step 650 | loss 1.2179 | lr 5.95e-05 | gates=[0.09528191387653351, 0.09309752285480499] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.14000263810157776, 0.8599973320960999]\n",
      "step 660 | loss 5.5447 | lr 5.65e-05 | gates=[0.8599973917007446, 0.008596471510827541] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.23791956901550293, 0.7620804309844971]\n",
      "step 670 | loss 5.1168 | lr 5.36e-05 | gates=[0.7620804309844971, 0.015390569344162941] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.6812767386436462, 0.31872326135635376]\n",
      "step 680 | loss 1.1552 | lr 5.07e-05 | gates=[0.31872329115867615, 0.08926244080066681] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.6908019185066223, 0.3091981112957001]\n",
      "step 690 | loss 1.5200 | lr 4.78e-05 | gates=[0.3091981112957001, 0.10100148618221283] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8751779794692993, 0.12482204288244247]\n",
      "step 700 | loss 1.5947 | lr 4.50e-05 | gates=[0.12482204288244247, 0.08314453065395355] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.17399528622627258, 0.8260047435760498]\n",
      "step 710 | loss 5.0116 | lr 4.23e-05 | gates=[0.8260047435760498, 0.013127843849360943] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.1486828774213791, 0.8513171076774597]\n",
      "step 720 | loss 5.6722 | lr 3.96e-05 | gates=[0.8513171076774597, 0.013333242386579514] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.23150977492332458, 0.768490195274353]\n",
      "step 730 | loss 4.7793 | lr 3.70e-05 | gates=[0.7684902548789978, 0.013651507906615734] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.35139456391334534, 0.6486054062843323]\n",
      "step 740 | loss 4.8521 | lr 3.45e-05 | gates=[0.6486054062843323, 0.01626083254814148] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.828341543674469, 0.17165838181972504]\n",
      "step 750 | loss 1.1949 | lr 3.20e-05 | gates=[0.17165838181972504, 0.07957550883293152] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.761413037776947, 0.23858699202537537]\n",
      "step 760 | loss 1.1779 | lr 2.96e-05 | gates=[0.23858699202537537, 0.09374353289604187] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.08925515413284302, 0.910744845867157]\n",
      "step 770 | loss 4.9843 | lr 2.73e-05 | gates=[0.9107449054718018, 0.008103909902274609] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.08345568925142288, 0.9165443181991577]\n",
      "step 780 | loss 5.0106 | lr 2.51e-05 | gates=[0.9165443181991577, 0.00894007459282875] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.11580526828765869, 0.8841947317123413]\n",
      "step 790 | loss 5.4748 | lr 2.29e-05 | gates=[0.8841947317123413, 0.0149304810911417] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.2810274362564087, 0.7189725637435913]\n",
      "step 800 | loss 4.9000 | lr 2.09e-05 | gates=[0.7189725637435913, 0.02754317596554756] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.12342947721481323, 0.8765705227851868]\n",
      "step 810 | loss 5.0130 | lr 2.00e-05 | gates=[0.876570463180542, 0.018466714769601822] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.10860608518123627, 0.8913938999176025]\n",
      "step 820 | loss 5.4486 | lr 2.00e-05 | gates=[0.8913938999176025, 0.0227028988301754] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.10794276744127274, 0.8920572400093079]\n",
      "step 830 | loss 4.9361 | lr 2.00e-05 | gates=[0.8920572400093079, 0.04035118967294693] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.792182207107544, 0.20781776309013367]\n",
      "step 840 | loss 1.4305 | lr 2.00e-05 | gates=[0.20781777799129486, 0.0944109857082367] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8312138915061951, 0.1687861680984497]\n",
      "step 850 | loss 1.2497 | lr 2.00e-05 | gates=[0.1687861680984497, 0.07562795281410217] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.11295568197965622, 0.8870443105697632]\n",
      "step 860 | loss 4.9544 | lr 2.00e-05 | gates=[0.8870443105697632, 0.03609906882047653] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.1243600845336914, 0.8756399154663086]\n",
      "step 870 | loss 4.9777 | lr 2.00e-05 | gates=[0.8756399154663086, 0.02657988667488098] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.15003593266010284, 0.8499640822410583]\n",
      "step 880 | loss 5.0708 | lr 2.00e-05 | gates=[0.8499640226364136, 0.02549995854496956] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8170485496520996, 0.18295153975486755]\n",
      "step 890 | loss 1.4879 | lr 2.00e-05 | gates=[0.18295153975486755, 0.09569478034973145] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.7483568787574768, 0.2516431510448456]\n",
      "step 900 | loss 1.5287 | lr 2.00e-05 | gates=[0.25164318084716797, 0.07955983281135559] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.26142406463623047, 0.7385759353637695]\n",
      "step 910 | loss 4.9072 | lr 2.00e-05 | gates=[0.7385759353637695, 0.03673781454563141] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.6564369797706604, 0.34356293082237244]\n",
      "step 920 | loss 1.5611 | lr 2.00e-05 | gates=[0.34356293082237244, 0.07554352283477783] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.1311129629611969, 0.8688870668411255]\n",
      "step 930 | loss 4.9252 | lr 2.00e-05 | gates=[0.8688870668411255, 0.017636751756072044] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.13506105542182922, 0.8649389743804932]\n",
      "step 940 | loss 5.5027 | lr 2.00e-05 | gates=[0.8649389743804932, 0.01768922619521618] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.24442699551582336, 0.755573034286499]\n",
      "step 950 | loss 4.7053 | lr 2.00e-05 | gates=[0.755573034286499, 0.025721881538629532] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.16062508523464203, 0.8393748998641968]\n",
      "step 960 | loss 4.9098 | lr 2.00e-05 | gates=[0.8393748998641968, 0.020540140569210052] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.18500208854675293, 0.8149979114532471]\n",
      "step 970 | loss 4.8838 | lr 2.00e-05 | gates=[0.8149979114532471, 0.023815417662262917] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.2042555809020996, 0.7957444190979004]\n",
      "step 980 | loss 4.8234 | lr 2.00e-05 | gates=[0.7957444190979004, 0.034307096153497696] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.5777028203010559, 0.4222971796989441]\n",
      "step 990 | loss 1.0109 | lr 2.00e-05 | gates=[0.4222972095012665, 0.08378089964389801] | mix=hf\n",
      "  [experts] block0 top=1 pi=[0.2811519503593445, 0.7188480496406555]\n",
      "step 1000 | loss 4.8907 | lr 2.00e-05 | gates=[0.7188481092453003, 0.038034532219171524] | mix=repeat\n",
      "[after  E15a_write_sparse_light_s2027_1755987068] alloc=9.63 GB | reserved=26.06 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.63 GB | reserved=9.65 GB | free=14.71 GB | total=25.77 GB\n",
      "\n",
      "=== E15b_two_experts_smallW_s2027_1755988655 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250823-153735-E15b_two_experts_smallW_s2027_1755988655\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15b_two_experts_smallW_s2027_1755988655] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb481363d8cb400f9e415931311b63a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8296784162521362, 0.08364704251289368, 0.08667456358671188]\n",
      "step 10 | loss 2.5194 | lr 4.40e-05 | gates=[0.17032159864902496, 0.2086634337902069] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9660220146179199, 0.02369740419089794, 0.01028058584779501]\n",
      "step 20 | loss 7.3184 | lr 8.40e-05 | gates=[0.03397798910737038, 0.7729206085205078] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9991491436958313, 0.0005307519459165633, 0.00032010592985898256]\n",
      "step 30 | loss 1.6272 | lr 1.24e-04 | gates=[0.0008508579339832067, 0.009282042272388935] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9862990379333496, 0.008131793700158596, 0.005569151137024164]\n",
      "step 40 | loss 5.8134 | lr 1.64e-04 | gates=[0.013700945302844048, 0.8488316535949707] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999624490737915, 2.5839719455689192e-05, 1.1692439329635818e-05]\n",
      "step 50 | loss 2.3644 | lr 2.00e-04 | gates=[3.753215787583031e-05, 0.05808749049901962] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990715980529785, 0.00014789216220378876, 0.0007804669439792633]\n",
      "step 60 | loss 5.8985 | lr 2.00e-04 | gates=[0.000928359164390713, 0.9968130588531494] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999391436576843, 4.269404598744586e-05, 1.8128124793292955e-05]\n",
      "step 70 | loss 2.6896 | lr 2.00e-04 | gates=[6.0822170780738816e-05, 0.567717969417572] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998918175697327, 7.18425726518035e-05, 3.62680948455818e-05]\n",
      "step 80 | loss 1.7276 | lr 1.99e-04 | gates=[0.0001081106674973853, 0.4324492812156677] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998085498809814, 0.00011155666288686916, 7.990405720192939e-05]\n",
      "step 90 | loss 5.8041 | lr 1.99e-04 | gates=[0.00019146075646858662, 0.9930700659751892] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999086260795593, 5.659541784552857e-05, 3.476044730632566e-05]\n",
      "step 100 | loss 1.3671 | lr 1.99e-04 | gates=[9.135586151387542e-05, 0.18986549973487854] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998557567596436, 7.8095770732034e-05, 6.615492020500824e-05]\n",
      "step 110 | loss 7.4445 | lr 1.98e-04 | gates=[0.000144250676385127, 0.9784297347068787] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999192357063293, 5.0042544899042696e-05, 3.069722151849419e-05]\n",
      "step 120 | loss 6.3699 | lr 1.97e-04 | gates=[8.073975914157927e-05, 0.9989138245582581] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999075531959534, 3.2278614526148885e-05, 6.0150036006234586e-05]\n",
      "step 130 | loss 5.5320 | lr 1.96e-04 | gates=[9.242865053238347e-05, 0.9999698996543884] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996885061264038, 0.00020574692462105304, 0.00010576250497251749]\n",
      "step 140 | loss 1.8627 | lr 1.96e-04 | gates=[0.0003115094150416553, 0.040491748601198196] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997957944869995, 0.00013498331827577204, 6.91348104737699e-05]\n",
      "step 150 | loss 1.9412 | lr 1.94e-04 | gates=[0.00020411812874954194, 0.09441440552473068] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996691942214966, 0.00019558101485017687, 0.0001352224062429741]\n",
      "step 160 | loss 1.6469 | lr 1.93e-04 | gates=[0.000330803421093151, 0.019024180248379707] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998341798782349, 0.00011906008148798719, 4.675717354984954e-05]\n",
      "step 170 | loss 1.7154 | lr 1.92e-04 | gates=[0.00016581725503783673, 0.011940926313400269] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995109438896179, 0.0002984498569276184, 0.00019059331680182368]\n",
      "step 180 | loss 1.7246 | lr 1.91e-04 | gates=[0.0004890432464890182, 0.1276751309633255] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994814395904541, 0.00036173564149066806, 0.00015688891289755702]\n",
      "step 190 | loss 1.7629 | lr 1.89e-04 | gates=[0.0005186245543882251, 0.0882781520485878] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998594522476196, 0.00011155071115354076, 2.9031489248154685e-05]\n",
      "step 200 | loss 5.9648 | lr 1.88e-04 | gates=[0.00014058221131563187, 0.9998869299888611] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995591640472412, 0.00031169899739325047, 0.0001291178195970133]\n",
      "step 210 | loss 1.9267 | lr 1.86e-04 | gates=[0.00044081680243834853, 0.5539718866348267] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996369481086731, 0.00024248762929346412, 0.00012051804515067488]\n",
      "step 220 | loss 1.6266 | lr 1.84e-04 | gates=[0.00036300570354796946, 0.12374738603830338] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999833106994629, 9.752209734870121e-06, 6.919225143064978e-06]\n",
      "step 230 | loss 5.2140 | lr 1.83e-04 | gates=[1.6671434423187748e-05, 0.9996868968009949] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999804496765137, 7.608744454046246e-06, 1.1937461749766953e-05]\n",
      "step 240 | loss 6.7865 | lr 1.81e-04 | gates=[1.9546205294318497e-05, 0.9969872236251831] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997525811195374, 0.00016776648408267647, 7.965460827108473e-05]\n",
      "step 250 | loss 5.4631 | lr 1.79e-04 | gates=[0.00024742106324993074, 0.9990816712379456] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999736547470093, 1.1570633432711475e-05, 1.4801372344663832e-05]\n",
      "step 260 | loss 6.0053 | lr 1.77e-04 | gates=[2.6372004867880605e-05, 0.9978760480880737] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996180534362793, 0.0002656356373336166, 0.00011626421473920345]\n",
      "step 270 | loss 1.5242 | lr 1.74e-04 | gates=[0.00038189985207282007, 0.13000544905662537] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999569654464722, 2.9146587621653453e-05, 1.3900751582696103e-05]\n",
      "step 280 | loss 5.2094 | lr 1.72e-04 | gates=[4.3047344661317766e-05, 0.9734079837799072] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998830556869507, 9.104351920541376e-05, 2.5915216610883363e-05]\n",
      "step 290 | loss 5.9818 | lr 1.70e-04 | gates=[0.00011695874127326533, 0.9983564615249634] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993780851364136, 0.00016578915528953075, 0.00045614398550242186]\n",
      "step 300 | loss 6.0173 | lr 1.67e-04 | gates=[0.0006219331407919526, 0.9996805191040039] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9991121888160706, 0.0006658342899754643, 0.00022189071751199663]\n",
      "step 310 | loss 1.7948 | lr 1.65e-04 | gates=[0.0008877249783836305, 0.26867225766181946] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9993909597396851, 0.0004846798547077924, 0.00012429813796188682]\n",
      "step 320 | loss 1.6204 | lr 1.62e-04 | gates=[0.000608977978117764, 0.11982829123735428] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991602897644043, 0.0007017524912953377, 0.0001380117319058627]\n",
      "step 330 | loss 1.8223 | lr 1.60e-04 | gates=[0.0008397642523050308, 0.08500643074512482] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991479516029358, 0.0007204305729828775, 0.00013157866487745196]\n",
      "step 340 | loss 1.7271 | lr 1.57e-04 | gates=[0.0008520092815160751, 0.10041122138500214] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987090229988098, 0.0010988159338012338, 0.00019216891087125987]\n",
      "step 350 | loss 1.7788 | lr 1.54e-04 | gates=[0.001290984801016748, 0.08153987675905228] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9997894167900085, 0.00018663736409507692, 2.395305818936322e-05]\n",
      "step 360 | loss 5.6919 | lr 1.52e-04 | gates=[0.00021059042774140835, 0.9988253116607666] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9962327480316162, 0.003605167381465435, 0.00016198687080759555]\n",
      "step 370 | loss 1.6560 | lr 1.49e-04 | gates=[0.003767154412344098, 0.2208433747291565] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9861616492271423, 0.013445382006466389, 0.0003929909144062549]\n",
      "step 380 | loss 1.9686 | lr 1.46e-04 | gates=[0.013838373124599457, 0.3535010516643524] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9775216579437256, 0.02201670967042446, 0.0004616592777892947]\n",
      "step 390 | loss 1.4593 | lr 1.43e-04 | gates=[0.022478368133306503, 0.23496419191360474] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9979270100593567, 0.0017227673670277, 0.00035021465737372637]\n",
      "step 400 | loss 1.5106 | lr 1.40e-04 | gates=[0.0020729820244014263, 0.15706339478492737] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990602135658264, 0.0007697679102420807, 0.00016998109640553594]\n",
      "step 410 | loss 1.5634 | lr 1.37e-04 | gates=[0.0009397490648552775, 0.6785463094711304] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987249970436096, 0.0010748202912509441, 0.00020013548783026636]\n",
      "step 420 | loss 1.6053 | lr 1.34e-04 | gates=[0.001274955808185041, 0.06076515093445778] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999662041664124, 1.4996564459579531e-05, 1.8809416360454634e-05]\n",
      "step 430 | loss 6.0091 | lr 1.31e-04 | gates=[3.3805976272560656e-05, 0.9824737310409546] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.996911346912384, 0.0027631826233118773, 0.00032552870106883347]\n",
      "step 440 | loss 1.7262 | lr 1.27e-04 | gates=[0.0030887112952768803, 0.4901352822780609] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9950185418128967, 0.004406243097037077, 0.0005752932629548013]\n",
      "step 450 | loss 1.6007 | lr 1.24e-04 | gates=[0.004981536418199539, 0.11111506074666977] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995402097702026, 0.0004066665132995695, 5.3154501074459404e-05]\n",
      "step 460 | loss 5.5287 | lr 1.21e-04 | gates=[0.0004598210216499865, 0.9997532963752747] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9764106869697571, 0.023006513714790344, 0.00058276008348912]\n",
      "step 470 | loss 1.6434 | lr 1.18e-04 | gates=[0.023589272052049637, 0.2822277545928955] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9610444903373718, 0.03814258426427841, 0.0008129270863719285]\n",
      "step 480 | loss 1.6707 | lr 1.14e-04 | gates=[0.038955509662628174, 0.14759217202663422] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991182088851929, 0.0007822848856449127, 9.94938745861873e-05]\n",
      "step 490 | loss 5.1603 | lr 1.11e-04 | gates=[0.0008817787747830153, 0.9980887174606323] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9548739790916443, 0.04377051070332527, 0.0013555983314290643]\n",
      "step 500 | loss 1.5098 | lr 1.08e-04 | gates=[0.045126114040613174, 0.20586051046848297] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9988358020782471, 0.00042541747097857296, 0.0007387511432170868]\n",
      "step 510 | loss 5.3956 | lr 1.05e-04 | gates=[0.0011641687015071511, 0.9905164241790771] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999407529830933, 4.9046841013478115e-05, 1.025427809508983e-05]\n",
      "step 520 | loss 5.1536 | lr 1.01e-04 | gates=[5.9301124565536156e-05, 0.8227461576461792] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.998577356338501, 0.0013124770484864712, 0.00011016654025297612]\n",
      "step 530 | loss 5.5231 | lr 9.80e-05 | gates=[0.0014226434286683798, 0.9886785745620728] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8375721573829651, 0.15871325135231018, 0.0037145281676203012]\n",
      "step 540 | loss 1.5820 | lr 9.47e-05 | gates=[0.16242778301239014, 0.12663838267326355] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9899129271507263, 0.00878569670021534, 0.001301408396102488]\n",
      "step 550 | loss 1.4275 | lr 9.14e-05 | gates=[0.010087104514241219, 0.5087704658508301] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998031497001648, 0.001656234497204423, 0.00031230942113325]\n",
      "step 560 | loss 4.8863 | lr 8.81e-05 | gates=[0.001968543976545334, 0.9121557474136353] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9970764517784119, 0.0024525984190404415, 0.0004709658387582749]\n",
      "step 570 | loss 5.0636 | lr 8.48e-05 | gates=[0.0029235640540719032, 0.9764536619186401] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9511696100234985, 0.04544752091169357, 0.003382788272574544]\n",
      "step 580 | loss 1.5269 | lr 8.16e-05 | gates=[0.0488303117454052, 0.16880592703819275] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9986228346824646, 0.0012013234663754702, 0.00017583725275471807]\n",
      "step 590 | loss 4.9408 | lr 7.83e-05 | gates=[0.0013771607773378491, 0.997025728225708] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9516478180885315, 0.04282897710800171, 0.005523276049643755]\n",
      "step 600 | loss 0.9432 | lr 7.51e-05 | gates=[0.04835225269198418, 0.5053976774215698] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9993515014648438, 0.0005580255528911948, 9.043548925546929e-05]\n",
      "step 610 | loss 5.6473 | lr 7.19e-05 | gates=[0.0006484611076302826, 0.9903273582458496] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994620084762573, 0.0003779585240408778, 0.0001600526156835258]\n",
      "step 620 | loss 4.9518 | lr 6.88e-05 | gates=[0.0005380111397244036, 0.9644433259963989] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.969988226890564, 0.02508879266679287, 0.004922976717352867]\n",
      "step 630 | loss 1.3543 | lr 6.57e-05 | gates=[0.030011773109436035, 0.18206153810024261] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989176988601685, 0.0006570329423993826, 0.0004252899088896811]\n",
      "step 640 | loss 5.0428 | lr 6.26e-05 | gates=[0.0010823227930814028, 0.9626988768577576] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9354791045188904, 0.05588480085134506, 0.008636068552732468]\n",
      "step 650 | loss 1.6189 | lr 5.95e-05 | gates=[0.06452086567878723, 0.25671330094337463] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9941846132278442, 0.004801115021109581, 0.001014274195767939]\n",
      "step 660 | loss 4.8143 | lr 5.65e-05 | gates=[0.0058153895661234856, 0.905665397644043] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997081160545349, 0.00021985475905239582, 7.199554238468409e-05]\n",
      "step 670 | loss 5.4254 | lr 5.36e-05 | gates=[0.0002918503014370799, 0.9869372844696045] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9999071359634399, 6.644046516157687e-05, 2.6450321456650272e-05]\n",
      "step 680 | loss 5.0272 | lr 5.07e-05 | gates=[9.289077570429072e-05, 0.9739758372306824] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996188282966614, 0.00027188536478206515, 0.00010930382995866239]\n",
      "step 690 | loss 5.4836 | lr 4.78e-05 | gates=[0.00038118919474072754, 0.8667812347412109] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999897301197052, 7.872805872466415e-05, 2.3965556465554982e-05]\n",
      "step 700 | loss 4.9068 | lr 4.50e-05 | gates=[0.00010269361519021913, 0.9646632671356201] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.99989914894104, 5.09475139551796e-05, 4.994318442186341e-05]\n",
      "step 710 | loss 5.0056 | lr 4.23e-05 | gates=[0.00010089070565300062, 0.9526674151420593] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994076490402222, 0.0004287765477783978, 0.00016361370217055082]\n",
      "step 720 | loss 5.4202 | lr 3.96e-05 | gates=[0.0005923902499489486, 0.9485911130905151] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.999803364276886, 0.00015060996520332992, 4.602414628607221e-05]\n",
      "step 730 | loss 4.8991 | lr 3.70e-05 | gates=[0.000196634151507169, 0.9669417142868042] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9627434015274048, 0.025130275636911392, 0.012126436457037926]\n",
      "step 740 | loss 1.2483 | lr 3.45e-05 | gates=[0.03725671395659447, 0.4087200164794922] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996423721313477, 0.00023775285808369517, 0.00011991232895525172]\n",
      "step 750 | loss 5.0299 | lr 3.20e-05 | gates=[0.0003576651797629893, 0.9709389209747314] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9587189555168152, 0.026172364130616188, 0.015108628198504448]\n",
      "step 760 | loss 1.3452 | lr 2.96e-05 | gates=[0.04128098860383034, 0.3507651686668396] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9981316328048706, 0.001091130543500185, 0.0007772406679578125]\n",
      "step 770 | loss 5.0321 | lr 2.73e-05 | gates=[0.0018683712696656585, 0.9666379690170288] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999681830406189, 0.00020536943338811398, 0.00011282876948826015]\n",
      "step 780 | loss 5.4886 | lr 2.51e-05 | gates=[0.0003181981446687132, 0.8919333219528198] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9969361424446106, 0.0021988002117723227, 0.0008650612435303628]\n",
      "step 790 | loss 4.8418 | lr 2.29e-05 | gates=[0.0030638612806797028, 0.9313990473747253] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9983130693435669, 0.0011305666994303465, 0.0005563597660511732]\n",
      "step 800 | loss 4.9349 | lr 2.09e-05 | gates=[0.0016869263490661979, 0.8974776268005371] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999290943145752, 0.0004522789386101067, 0.00025674287462607026]\n",
      "step 810 | loss 5.4622 | lr 2.00e-05 | gates=[0.000709021813236177, 0.9565262794494629] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9362208843231201, 0.03911956772208214, 0.02465948462486267]\n",
      "step 820 | loss 1.3037 | lr 2.00e-05 | gates=[0.06377904862165451, 0.21376380324363708] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996235966682434, 0.00027805499848909676, 9.840054553933442e-05]\n",
      "step 830 | loss 4.9146 | lr 2.00e-05 | gates=[0.00037645542761310935, 0.936887264251709] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9972323179244995, 0.001980601344257593, 0.0007871326524764299]\n",
      "step 840 | loss 4.9099 | lr 2.00e-05 | gates=[0.002767733996734023, 0.904084324836731] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9322170615196228, 0.042276717722415924, 0.02550620771944523]\n",
      "step 850 | loss 1.3343 | lr 2.00e-05 | gates=[0.067782923579216, 0.24437358975410461] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9553041458129883, 0.026722963899374008, 0.017972853034734726]\n",
      "step 860 | loss 1.3219 | lr 2.00e-05 | gates=[0.044695816934108734, 0.5399488210678101] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998278617858887, 0.00011640617594821379, 5.5740372772561386e-05]\n",
      "step 870 | loss 4.9060 | lr 2.00e-05 | gates=[0.0001721465669106692, 0.9508707523345947] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9529054760932922, 0.03078809194266796, 0.016306458041071892]\n",
      "step 880 | loss 1.3989 | lr 2.00e-05 | gates=[0.047094546258449554, 0.17463412880897522] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9971960783004761, 0.002065206179395318, 0.0007387145888060331]\n",
      "step 890 | loss 4.7622 | lr 2.00e-05 | gates=[0.002803920768201351, 0.8300518989562988] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9428229928016663, 0.037231918424367905, 0.019945047795772552]\n",
      "step 900 | loss 1.5219 | lr 2.00e-05 | gates=[0.05717696622014046, 0.21474693715572357] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9988924264907837, 0.000868922215886414, 0.00023865234106779099]\n",
      "step 910 | loss 4.8567 | lr 2.00e-05 | gates=[0.0011075746733695269, 0.9377453327178955] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9990817308425903, 0.0006705804262310266, 0.0002476954541634768]\n",
      "step 920 | loss 5.4673 | lr 2.00e-05 | gates=[0.000918275851290673, 0.949615478515625] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.998612642288208, 0.0008924517314881086, 0.0004949552821926773]\n",
      "step 930 | loss 5.4650 | lr 2.00e-05 | gates=[0.0013874070718884468, 0.9298827052116394] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997585415840149, 0.00013995826884638518, 0.00010151856986340135]\n",
      "step 940 | loss 5.4706 | lr 2.00e-05 | gates=[0.00024147683870978653, 0.9673323631286621] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9570470452308655, 0.02734164334833622, 0.01561132911592722]\n",
      "step 950 | loss 1.4352 | lr 2.00e-05 | gates=[0.042952973395586014, 0.33386436104774475] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9405012726783752, 0.038529686629772186, 0.02096899226307869]\n",
      "step 960 | loss 1.4357 | lr 2.00e-05 | gates=[0.05949867516756058, 0.4331798851490021] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998950183391571, 0.000641784630715847, 0.0004080093349330127]\n",
      "step 970 | loss 4.9743 | lr 2.00e-05 | gates=[0.0010497940238565207, 0.9049777984619141] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9966326951980591, 0.0019796928390860558, 0.00138757168315351]\n",
      "step 980 | loss 4.8540 | lr 2.00e-05 | gates=[0.003367264522239566, 0.8702353239059448] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9977068305015564, 0.0016228568274527788, 0.0006703130202367902]\n",
      "step 990 | loss 4.8381 | lr 2.00e-05 | gates=[0.0022931701969355345, 0.899998664855957] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997841119766235, 0.00015092163812369108, 6.500507879536599e-05]\n",
      "step 1000 | loss 4.8720 | lr 2.00e-05 | gates=[0.00021592671691905707, 0.940413236618042] | mix=nback\n",
      "[after  E15b_two_experts_smallW_s2027_1755988655] alloc=10.69 GB | reserved=33.32 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.69 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "\n",
      "=== E15a_write_sparse_light_s4242_1755992098 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250823-163458-E15a_write_sparse_light_s4242_1755992098\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15a_write_sparse_light_s4242_1755992098] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f453a6351d4411e9889f779ddd082ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.6655751466751099, 0.33442485332489014]\n",
      "step 10 | loss 9.1353 | lr 4.40e-05 | gates=[0.33442485332489014, 0.2681189179420471] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.08732841908931732, 0.9126715660095215]\n",
      "step 20 | loss 6.9223 | lr 8.40e-05 | gates=[0.9126715660095215, 0.03181570768356323] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.10383504629135132, 0.8961649537086487]\n",
      "step 30 | loss 5.7412 | lr 1.24e-04 | gates=[0.8961649537086487, 0.14861273765563965] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.06477998197078705, 0.9352200031280518]\n",
      "step 40 | loss 5.9549 | lr 1.64e-04 | gates=[0.9352200031280518, 0.024931583553552628] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.2397720366716385, 0.7602279782295227]\n",
      "step 50 | loss 9.4520 | lr 2.00e-04 | gates=[0.7602279186248779, 0.028303522616624832] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.986657977104187, 0.013342034071683884]\n",
      "step 60 | loss 6.2780 | lr 2.00e-04 | gates=[0.013342034071683884, 0.7727603316307068] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991148710250854, 0.000885136192664504]\n",
      "step 70 | loss 6.4589 | lr 2.00e-04 | gates=[0.000885136250872165, 0.9976367950439453] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9932997226715088, 0.006700297351926565]\n",
      "step 80 | loss 6.8625 | lr 1.99e-04 | gates=[0.0067002978175878525, 0.9983476400375366] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9957007169723511, 0.004299257881939411]\n",
      "step 90 | loss 6.0514 | lr 1.99e-04 | gates=[0.004299257881939411, 0.9976892471313477] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9916180372238159, 0.008381971158087254]\n",
      "step 100 | loss 6.3120 | lr 1.99e-04 | gates=[0.008381972089409828, 0.9975836277008057] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.6184616684913635, 0.38153839111328125]\n",
      "step 110 | loss 3.3391 | lr 1.98e-04 | gates=[0.3815383315086365, 0.011773023754358292] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.991510272026062, 0.008489725179970264]\n",
      "step 120 | loss 1.8902 | lr 1.97e-04 | gates=[0.00848972424864769, 0.0027287909761071205] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9952547550201416, 0.0047452328726649284]\n",
      "step 130 | loss 5.3547 | lr 1.96e-04 | gates=[0.004745232407003641, 0.9997252225875854] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9862111806869507, 0.013788841664791107]\n",
      "step 140 | loss 5.8856 | lr 1.96e-04 | gates=[0.013788840733468533, 0.9993664622306824] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9662865400314331, 0.0337134450674057]\n",
      "step 150 | loss 5.4840 | lr 1.94e-04 | gates=[0.0337134450674057, 0.997639536857605] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9903609156608582, 0.009639035910367966]\n",
      "step 160 | loss 2.4100 | lr 1.93e-04 | gates=[0.009639035910367966, 0.06582473963499069] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9548313021659851, 0.045168668031692505]\n",
      "step 170 | loss 5.8119 | lr 1.92e-04 | gates=[0.0451686717569828, 0.989914059638977] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9805161952972412, 0.019483862444758415]\n",
      "step 180 | loss 5.6704 | lr 1.91e-04 | gates=[0.019483862444758415, 0.9994348287582397] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9268264770507812, 0.07317350059747696]\n",
      "step 190 | loss 6.2513 | lr 1.89e-04 | gates=[0.07317350059747696, 0.9947091937065125] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8966516852378845, 0.10334841907024384]\n",
      "step 200 | loss 1.8688 | lr 1.88e-04 | gates=[0.10334841907024384, 0.011899706907570362] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9055543541908264, 0.09444567561149597]\n",
      "step 210 | loss 5.8101 | lr 1.86e-04 | gates=[0.09444567561149597, 0.9776120185852051] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9205998182296753, 0.07940017431974411]\n",
      "step 220 | loss 1.2039 | lr 1.84e-04 | gates=[0.07940017431974411, 0.006805831100791693] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9031389951705933, 0.09686098247766495]\n",
      "step 230 | loss 6.6665 | lr 1.83e-04 | gates=[0.09686098247766495, 0.9999909996986389] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.7297706604003906, 0.27022939920425415]\n",
      "step 240 | loss 1.9545 | lr 1.81e-04 | gates=[0.27022939920425415, 0.23046912252902985] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.8611798882484436, 0.13882014155387878]\n",
      "step 250 | loss 1.4750 | lr 1.79e-04 | gates=[0.13882014155387878, 0.05929027870297432] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9913549423217773, 0.008645057678222656]\n",
      "step 260 | loss 6.3525 | lr 1.77e-04 | gates=[0.00864505860954523, 0.9999951720237732] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9883471727371216, 0.0116528095677495]\n",
      "step 270 | loss 6.6587 | lr 1.74e-04 | gates=[0.011652810499072075, 0.9999939799308777] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9944624900817871, 0.005537468008697033]\n",
      "step 280 | loss 6.3828 | lr 1.72e-04 | gates=[0.005537467543035746, 0.9999897480010986] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9920397400856018, 0.007960300892591476]\n",
      "step 290 | loss 1.7818 | lr 1.70e-04 | gates=[0.007960300892591476, 0.23743943870067596] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.986727774143219, 0.013272255659103394]\n",
      "step 300 | loss 5.7201 | lr 1.67e-04 | gates=[0.013272254727780819, 0.9996671676635742] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9797710180282593, 0.020228946581482887]\n",
      "step 310 | loss 6.2623 | lr 1.65e-04 | gates=[0.020228946581482887, 0.9997105598449707] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9941406846046448, 0.005859320983290672]\n",
      "step 320 | loss 5.7403 | lr 1.62e-04 | gates=[0.005859321914613247, 0.9999210834503174] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9808775186538696, 0.019122451543807983]\n",
      "step 330 | loss 6.4584 | lr 1.60e-04 | gates=[0.019122453406453133, 0.9993626475334167] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9860639572143555, 0.013936023227870464]\n",
      "step 340 | loss 5.5686 | lr 1.57e-04 | gates=[0.013936023227870464, 0.9998908042907715] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9628337025642395, 0.037166327238082886]\n",
      "step 350 | loss 5.7756 | lr 1.54e-04 | gates=[0.03716632351279259, 0.9998887777328491] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.963614821434021, 0.036385200917720795]\n",
      "step 360 | loss 5.0270 | lr 1.52e-04 | gates=[0.03638520464301109, 0.9997966885566711] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9050281047821045, 0.09497194737195969]\n",
      "step 370 | loss 5.1858 | lr 1.49e-04 | gates=[0.09497193992137909, 0.9995072484016418] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997249841690063, 0.00027507435879670084]\n",
      "step 380 | loss 1.6632 | lr 1.46e-04 | gates=[0.00027507435879670084, 0.17288099229335785] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9658466577529907, 0.034153372049331665]\n",
      "step 390 | loss 5.2041 | lr 1.43e-04 | gates=[0.034153372049331665, 0.9979357123374939] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9980666637420654, 0.0019332572119310498]\n",
      "step 400 | loss 1.7589 | lr 1.40e-04 | gates=[0.0019332569791004062, 0.16183620691299438] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998761415481567, 0.00012384424917399883]\n",
      "step 410 | loss 1.8889 | lr 1.37e-04 | gates=[0.0001238442346220836, 0.008299383334815502] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9661867618560791, 0.03381327539682388]\n",
      "step 420 | loss 5.1215 | lr 1.34e-04 | gates=[0.03381327912211418, 0.9997795820236206] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9963429570198059, 0.0036571042146533728]\n",
      "step 430 | loss 1.1118 | lr 1.31e-04 | gates=[0.0036571042146533728, 0.3543497323989868] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992545247077942, 0.0007455505547113717]\n",
      "step 440 | loss 1.5013 | lr 1.27e-04 | gates=[0.0007455506711266935, 0.06637339293956757] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9894968867301941, 0.010503153316676617]\n",
      "step 450 | loss 1.1858 | lr 1.24e-04 | gates=[0.010503153316676617, 0.3174833655357361] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995115399360657, 0.0004884838708676398]\n",
      "step 460 | loss 1.5276 | lr 1.21e-04 | gates=[0.0004884838126599789, 0.14805841445922852] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9871861934661865, 0.01281376276165247]\n",
      "step 470 | loss 6.1897 | lr 1.18e-04 | gates=[0.01281376276165247, 0.9994049072265625] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9894936084747314, 0.01050636824220419]\n",
      "step 480 | loss 5.2626 | lr 1.14e-04 | gates=[0.010506367310881615, 0.9991517066955566] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9698883295059204, 0.030111737549304962]\n",
      "step 490 | loss 6.2727 | lr 1.11e-04 | gates=[0.030111737549304962, 0.9974247217178345] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9865274429321289, 0.013472514227032661]\n",
      "step 500 | loss 2.2173 | lr 1.08e-04 | gates=[0.013472512364387512, 0.2983439862728119] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.999516487121582, 0.000483524810988456]\n",
      "step 510 | loss 1.5864 | lr 1.05e-04 | gates=[0.000483524810988456, 0.13533806800842285] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9977242350578308, 0.0022758443374186754]\n",
      "step 520 | loss 1.5731 | lr 1.01e-04 | gates=[0.002275844570249319, 0.18192215263843536] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987773895263672, 0.0012225746177136898]\n",
      "step 530 | loss 1.2452 | lr 9.80e-05 | gates=[0.001222574501298368, 0.22674277424812317] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9928559064865112, 0.007144119590520859]\n",
      "step 540 | loss 5.0167 | lr 9.47e-05 | gates=[0.007144120056182146, 0.9899867177009583] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9990171790122986, 0.000982851954177022]\n",
      "step 550 | loss 1.5784 | lr 9.14e-05 | gates=[0.000982851954177022, 0.20843103528022766] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9613969326019287, 0.03860307112336159]\n",
      "step 560 | loss 4.9291 | lr 8.81e-05 | gates=[0.03860307112336159, 0.9826478958129883] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9981822967529297, 0.0018176727462559938]\n",
      "step 570 | loss 1.2480 | lr 8.48e-05 | gates=[0.001817672629840672, 0.30522391200065613] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9984226226806641, 0.0015774228377267718]\n",
      "step 580 | loss 1.2390 | lr 8.16e-05 | gates=[0.0015774228377267718, 0.2246021330356598] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9920052289962769, 0.007994797080755234]\n",
      "step 590 | loss 5.1839 | lr 7.83e-05 | gates=[0.007994798012077808, 0.9951210618019104] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9665288329124451, 0.033471159636974335]\n",
      "step 600 | loss 1.4841 | lr 7.51e-05 | gates=[0.03347116336226463, 0.5051773190498352] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9465941786766052, 0.053405821323394775]\n",
      "step 610 | loss 5.0642 | lr 7.19e-05 | gates=[0.053405821323394775, 0.9126845598220825] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.933401882648468, 0.06659813225269318]\n",
      "step 620 | loss 4.9556 | lr 6.88e-05 | gates=[0.06659812480211258, 0.9708554744720459] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9919371604919434, 0.008062900975346565]\n",
      "step 630 | loss 1.4651 | lr 6.57e-05 | gates=[0.008062900975346565, 0.26910093426704407] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9848448634147644, 0.015155130997300148]\n",
      "step 640 | loss 1.3398 | lr 6.26e-05 | gates=[0.015155130065977573, 0.36452731490135193] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9933385848999023, 0.006661389023065567]\n",
      "step 650 | loss 1.3896 | lr 5.95e-05 | gates=[0.006661389023065567, 0.2732783854007721] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9982456564903259, 0.0017543986905366182]\n",
      "step 660 | loss 1.2313 | lr 5.65e-05 | gates=[0.0017543986905366182, 0.15829230844974518] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9586513638496399, 0.04134862869977951]\n",
      "step 670 | loss 5.0241 | lr 5.36e-05 | gates=[0.04134862869977951, 0.9374260902404785] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9754005670547485, 0.024599429219961166]\n",
      "step 680 | loss 5.5546 | lr 5.07e-05 | gates=[0.024599431082606316, 0.8587831258773804] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.956395149230957, 0.04360486567020416]\n",
      "step 690 | loss 1.4570 | lr 4.78e-05 | gates=[0.04360486939549446, 0.36536964774131775] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9381460547447205, 0.06185394898056984]\n",
      "step 700 | loss 1.1151 | lr 4.50e-05 | gates=[0.06185394898056984, 0.5243850946426392] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9969924092292786, 0.00300768599845469]\n",
      "step 710 | loss 1.4986 | lr 4.23e-05 | gates=[0.00300768599845469, 0.2550147473812103] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9854709506034851, 0.014529054053127766]\n",
      "step 720 | loss 5.5146 | lr 3.96e-05 | gates=[0.014529054053127766, 0.9867843389511108] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9985564947128296, 0.0014436354395002127]\n",
      "step 730 | loss 1.4024 | lr 3.70e-05 | gates=[0.0014436354395002127, 0.23181545734405518] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9968010783195496, 0.0031989943236112595]\n",
      "step 740 | loss 1.2078 | lr 3.45e-05 | gates=[0.0031989943236112595, 0.2313063144683838] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.948026716709137, 0.05197335034608841]\n",
      "step 750 | loss 4.8603 | lr 3.20e-05 | gates=[0.05197335034608841, 0.8692378997802734] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9983952045440674, 0.0016048740362748504]\n",
      "step 760 | loss 1.5069 | lr 2.96e-05 | gates=[0.0016048740362748504, 0.21587179601192474] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9842119216918945, 0.015788057819008827]\n",
      "step 770 | loss 5.4526 | lr 2.73e-05 | gates=[0.015788057819008827, 0.9590816497802734] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.986387312412262, 0.013612654060125351]\n",
      "step 780 | loss 1.6081 | lr 2.51e-05 | gates=[0.013612654060125351, 0.2612152397632599] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9776690602302551, 0.02233092114329338]\n",
      "step 790 | loss 1.4739 | lr 2.29e-05 | gates=[0.02233092300593853, 0.307974249124527] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9068199992179871, 0.09317997097969055]\n",
      "step 800 | loss 4.9578 | lr 2.09e-05 | gates=[0.09317998588085175, 0.9497016072273254] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9887269735336304, 0.011273039504885674]\n",
      "step 810 | loss 5.4527 | lr 2.00e-05 | gates=[0.011273039504885674, 0.9250677824020386] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9405542612075806, 0.059445783495903015]\n",
      "step 820 | loss 5.4566 | lr 2.00e-05 | gates=[0.059445787221193314, 0.9591238498687744] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.987200915813446, 0.012799087911844254]\n",
      "step 830 | loss 5.4289 | lr 2.00e-05 | gates=[0.012799086980521679, 0.9374275207519531] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9876386523246765, 0.012361300177872181]\n",
      "step 840 | loss 4.8760 | lr 2.00e-05 | gates=[0.012361299246549606, 0.9691200256347656] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9791896343231201, 0.020810434594750404]\n",
      "step 850 | loss 1.0719 | lr 2.00e-05 | gates=[0.020810436457395554, 0.4041864275932312] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9978955984115601, 0.002104378305375576]\n",
      "step 860 | loss 1.5175 | lr 2.00e-05 | gates=[0.0021043785382062197, 0.2211521714925766] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9833109974861145, 0.016689041629433632]\n",
      "step 870 | loss 1.6197 | lr 2.00e-05 | gates=[0.016689041629433632, 0.3529152274131775] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.978984534740448, 0.021015487611293793]\n",
      "step 880 | loss 1.2950 | lr 2.00e-05 | gates=[0.02101549133658409, 0.28490009903907776] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9964852333068848, 0.00351480720564723]\n",
      "step 890 | loss 1.0099 | lr 2.00e-05 | gates=[0.00351480720564723, 0.19596724212169647] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9918767213821411, 0.008123237639665604]\n",
      "step 900 | loss 4.9566 | lr 2.00e-05 | gates=[0.008123238570988178, 0.9756211042404175] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9634888172149658, 0.03651115298271179]\n",
      "step 910 | loss 4.8170 | lr 2.00e-05 | gates=[0.03651115670800209, 0.9581480026245117] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9921267628669739, 0.007873248308897018]\n",
      "step 920 | loss 4.9554 | lr 2.00e-05 | gates=[0.007873248308897018, 0.910557210445404] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9978529810905457, 0.0021469779312610626]\n",
      "step 930 | loss 1.4316 | lr 2.00e-05 | gates=[0.0021469779312610626, 0.23172150552272797] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9473559856414795, 0.05264401435852051]\n",
      "step 940 | loss 4.9123 | lr 2.00e-05 | gates=[0.05264401435852051, 0.9479153156280518] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9589437246322632, 0.041056275367736816]\n",
      "step 950 | loss 5.4594 | lr 2.00e-05 | gates=[0.04105627164244652, 0.9587762951850891] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9767717123031616, 0.02322830818593502]\n",
      "step 960 | loss 5.4356 | lr 2.00e-05 | gates=[0.02322830632328987, 0.958532989025116] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9890240430831909, 0.010975916869938374]\n",
      "step 970 | loss 1.4807 | lr 2.00e-05 | gates=[0.010975916869938374, 0.342256098985672] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9941790103912354, 0.0058211032301187515]\n",
      "step 980 | loss 1.4187 | lr 2.00e-05 | gates=[0.0058211032301187515, 0.21528048813343048] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9942575693130493, 0.005742463283240795]\n",
      "step 990 | loss 1.3403 | lr 2.00e-05 | gates=[0.0057424623519182205, 0.18871654570102692] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9794223308563232, 0.020577717572450638]\n",
      "step 1000 | loss 5.4515 | lr 2.00e-05 | gates=[0.020577717572450638, 0.9422471523284912] | mix=copy\n",
      "[after  E15a_write_sparse_light_s4242_1755992098] alloc=9.63 GB | reserved=26.74 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=9.63 GB | reserved=9.65 GB | free=14.71 GB | total=25.77 GB\n",
      "\n",
      "=== E15b_two_experts_smallW_s4242_1755994049 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250823-170729-E15b_two_experts_smallW_s4242_1755994049\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E15b_two_experts_smallW_s4242_1755994049] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f16e5910df9b4fc4abfeca9d24cf7ae6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.6075003743171692, 0.276397705078125, 0.11610183864831924]\n",
      "step 10 | loss 3.3222 | lr 4.40e-05 | gates=[0.39249953627586365, 0.39013808965682983] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9980084300041199, 0.000596475088968873, 0.00139511632733047]\n",
      "step 20 | loss 1.8696 | lr 8.40e-05 | gates=[0.001991591416299343, 0.005576163996011019] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9942731857299805, 0.004270132631063461, 0.0014566569589078426]\n",
      "step 30 | loss 8.0889 | lr 1.24e-04 | gates=[0.005726790055632591, 0.9509265422821045] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9545009136199951, 0.028429334983229637, 0.0170697383582592]\n",
      "step 40 | loss 6.8222 | lr 1.64e-04 | gates=[0.04549907520413399, 0.9673513770103455] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9915063381195068, 0.005374231841415167, 0.00311942002736032]\n",
      "step 50 | loss 6.3531 | lr 2.00e-04 | gates=[0.008493651635944843, 0.9913922548294067] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997721910476685, 5.5363216233672574e-05, 0.00017244656919501722]\n",
      "step 60 | loss 2.2764 | lr 2.00e-04 | gates=[0.00022780979634262621, 0.3865490257740021] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9988886713981628, 0.00024335202760994434, 0.0008679628372192383]\n",
      "step 70 | loss 2.8333 | lr 2.00e-04 | gates=[0.0011113148648291826, 0.0035258769057691097] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9966604709625244, 0.001473047537729144, 0.001866546692326665]\n",
      "step 80 | loss 6.3217 | lr 1.99e-04 | gates=[0.0033395944628864527, 0.9764264822006226] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999854326248169, 3.617815673351288e-05, 0.0001095274492399767]\n",
      "step 90 | loss 2.6233 | lr 1.99e-04 | gates=[0.00014570560597348958, 0.09956562519073486] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991568326950073, 0.0004348725196905434, 0.00040828873170539737]\n",
      "step 100 | loss 7.0175 | lr 1.99e-04 | gates=[0.0008431613096036017, 0.9999417066574097] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9923282861709595, 0.004510052967816591, 0.003161604516208172]\n",
      "step 110 | loss 6.4842 | lr 1.98e-04 | gates=[0.00767165794968605, 0.9995940327644348] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9998152852058411, 4.542496390058659e-05, 0.00013923679944127798]\n",
      "step 120 | loss 1.7919 | lr 1.97e-04 | gates=[0.00018466176697984338, 0.010038644075393677] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9998731017112732, 3.592698703869246e-05, 9.105601930059493e-05]\n",
      "step 130 | loss 2.1858 | lr 1.96e-04 | gates=[0.0001269830099772662, 0.008490386418998241] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996169805526733, 0.00019062601495534182, 0.00019237630476709455]\n",
      "step 140 | loss 5.9223 | lr 1.96e-04 | gates=[0.0003830022760666907, 0.9999234676361084] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9992114305496216, 0.00037996037281118333, 0.00040858815191313624]\n",
      "step 150 | loss 5.5521 | lr 1.94e-04 | gates=[0.000788548612035811, 0.9999428987503052] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9939817190170288, 0.0027765834238380194, 0.0032416749745607376]\n",
      "step 160 | loss 5.8113 | lr 1.93e-04 | gates=[0.006018258165568113, 0.9989827871322632] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9985300302505493, 0.0008386321715079248, 0.0006313248304650187]\n",
      "step 170 | loss 5.5990 | lr 1.92e-04 | gates=[0.0014699570601806045, 0.9964712262153625] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9976261854171753, 0.0010777213610708714, 0.0012960933381691575]\n",
      "step 180 | loss 5.9021 | lr 1.91e-04 | gates=[0.002373814582824707, 0.9826925992965698] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9995943903923035, 0.00010412224946776405, 0.00030144205084070563]\n",
      "step 190 | loss 1.6378 | lr 1.89e-04 | gates=[0.0004055643221363425, 0.360982745885849] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9962940216064453, 0.0019447100348770618, 0.0017612525261938572]\n",
      "step 200 | loss 5.5999 | lr 1.88e-04 | gates=[0.003705962561070919, 0.9999921917915344] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9981335401535034, 0.0008758282056078315, 0.0009906117338687181]\n",
      "step 210 | loss 5.6622 | lr 1.86e-04 | gates=[0.0018664398230612278, 0.9999905824661255] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992950558662415, 0.00015647282998543233, 0.0005484075518324971]\n",
      "step 220 | loss 1.3683 | lr 1.84e-04 | gates=[0.0007048803381621838, 0.31669169664382935] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994983077049255, 0.00010389433009549975, 0.00039782034582458436]\n",
      "step 230 | loss 1.8337 | lr 1.83e-04 | gates=[0.0005017147050239146, 0.2456732988357544] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985413551330566, 0.0003588900144677609, 0.0010996623896062374]\n",
      "step 240 | loss 1.6484 | lr 1.81e-04 | gates=[0.0014585525495931506, 0.07851151376962662] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9975322484970093, 0.0012705200351774693, 0.0011972365900874138]\n",
      "step 250 | loss 5.4728 | lr 1.79e-04 | gates=[0.002467756625264883, 0.8644254207611084] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996694326400757, 8.301248453790322e-05, 0.00024754140758886933]\n",
      "step 260 | loss 2.4722 | lr 1.77e-04 | gates=[0.00033055394305847585, 0.5088065266609192] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9993171095848083, 0.0001830009277909994, 0.0004998414660803974]\n",
      "step 270 | loss 1.6217 | lr 1.74e-04 | gates=[0.0006828424520790577, 0.07093150913715363] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9982230067253113, 0.001098063075914979, 0.0006789482431486249]\n",
      "step 280 | loss 4.9234 | lr 1.72e-04 | gates=[0.0017770114354789257, 0.9996380805969238] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.996616780757904, 0.0010740443831309676, 0.0023091621696949005]\n",
      "step 290 | loss 1.8361 | lr 1.70e-04 | gates=[0.0033832064364105463, 0.1969251185655594] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994838833808899, 0.0001320056471740827, 0.0003841720172204077]\n",
      "step 300 | loss 1.6809 | lr 1.67e-04 | gates=[0.0005161776207387447, 0.07508838176727295] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994403719902039, 0.0001719170977594331, 0.00038772448897361755]\n",
      "step 310 | loss 1.3905 | lr 1.65e-04 | gates=[0.0005596415139734745, 0.030499374493956566] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9887757301330566, 0.00797279179096222, 0.003251513699069619]\n",
      "step 320 | loss 5.8041 | lr 1.62e-04 | gates=[0.011224305257201195, 0.9832327365875244] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9995072484016418, 0.00014703960914630443, 0.0003456412523519248]\n",
      "step 330 | loss 1.6720 | lr 1.60e-04 | gates=[0.0004926808760501444, 0.0019686268642544746] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9988207817077637, 0.0007574856281280518, 0.00042170690721832216]\n",
      "step 340 | loss 5.5201 | lr 1.57e-04 | gates=[0.0011791925644502044, 0.9968278408050537] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.996245801448822, 0.0026318151503801346, 0.0011223966721445322]\n",
      "step 350 | loss 5.4242 | lr 1.54e-04 | gates=[0.003754211822524667, 0.9973412156105042] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9990447163581848, 0.00025810181978158653, 0.0006972115952521563]\n",
      "step 360 | loss 1.4677 | lr 1.52e-04 | gates=[0.0009553134441375732, 0.1180107444524765] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992931485176086, 0.00019895369769074023, 0.0005078791291452944]\n",
      "step 370 | loss 1.2991 | lr 1.49e-04 | gates=[0.0007068327977322042, 0.3364775776863098] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990145564079285, 0.00033851739135570824, 0.0006469256477430463]\n",
      "step 380 | loss 1.3278 | lr 1.46e-04 | gates=[0.000985443126410246, 0.3871943950653076] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991493225097656, 0.00027107956702820957, 0.0005796252517029643]\n",
      "step 390 | loss 1.3674 | lr 1.43e-04 | gates=[0.0008507048478350043, 0.5111932754516602] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992830753326416, 0.0001764544431352988, 0.0005403922405093908]\n",
      "step 400 | loss 1.6144 | lr 1.40e-04 | gates=[0.0007168466690927744, 0.290268212556839] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9971002340316772, 0.0019535471219569445, 0.0009461972513236105]\n",
      "step 410 | loss 5.0346 | lr 1.37e-04 | gates=[0.0028997445479035378, 0.8213484287261963] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9960229992866516, 0.0030746012926101685, 0.0009024053579196334]\n",
      "step 420 | loss 5.1648 | lr 1.34e-04 | gates=[0.003977006301283836, 0.9556984901428223] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9996709227561951, 0.00024417153326794505, 8.488709863740951e-05]\n",
      "step 430 | loss 5.0554 | lr 1.31e-04 | gates=[0.00032905861735343933, 0.9662317037582397] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997330904006958, 0.0001951705344254151, 7.172341429395601e-05]\n",
      "step 440 | loss 5.8280 | lr 1.27e-04 | gates=[0.0002668939414434135, 0.4508622884750366] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9948752522468567, 0.003660723567008972, 0.0014639860019087791]\n",
      "step 450 | loss 4.9790 | lr 1.24e-04 | gates=[0.005124709568917751, 0.9547958374023438] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9991288781166077, 0.0006673879688605666, 0.00020373004372231662]\n",
      "step 460 | loss 5.2527 | lr 1.21e-04 | gates=[0.0008711179834790528, 0.6616761684417725] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9980658292770386, 0.000309933559037745, 0.001624341937713325]\n",
      "step 470 | loss 1.7065 | lr 1.18e-04 | gates=[0.00193427549675107, 0.1401819884777069] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.997730016708374, 0.0016922550275921822, 0.0005777325131930411]\n",
      "step 480 | loss 5.2100 | lr 1.14e-04 | gates=[0.0022699874825775623, 0.9992140531539917] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9994792938232422, 0.00028626908897422254, 0.00023440574295818806]\n",
      "step 490 | loss 5.5465 | lr 1.11e-04 | gates=[0.0005206748610362411, 0.9991865754127502] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9935476183891296, 0.004976155236363411, 0.0014762210194021463]\n",
      "step 500 | loss 5.3093 | lr 1.08e-04 | gates=[0.006452376022934914, 0.9998741745948792] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9987314939498901, 0.0009560565231367946, 0.0003124713839497417]\n",
      "step 510 | loss 5.4388 | lr 1.05e-04 | gates=[0.0012685279361903667, 0.9998296499252319] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9612493515014648, 0.001417973660863936, 0.037332676351070404]\n",
      "step 520 | loss 1.3132 | lr 1.01e-04 | gates=[0.038750648498535156, 0.2815057039260864] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995086789131165, 0.00025208661099895835, 0.00023920921375975013]\n",
      "step 530 | loss 5.2320 | lr 9.80e-05 | gates=[0.0004912958247587085, 0.999821662902832] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995075464248657, 0.00029989899485372007, 0.00019252224592491984]\n",
      "step 540 | loss 5.0814 | lr 9.47e-05 | gates=[0.0004924212116748095, 0.9997735023498535] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9970830678939819, 0.0007727536140009761, 0.0021441306453198195]\n",
      "step 550 | loss 1.4915 | lr 9.14e-05 | gates=[0.0029168843757361174, 0.19962483644485474] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995360970497131, 0.00017082392878364772, 0.0002930855262093246]\n",
      "step 560 | loss 1.5513 | lr 8.81e-05 | gates=[0.00046390941133722663, 0.2175251990556717] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9900062084197998, 0.005170508753508329, 0.004823333118110895]\n",
      "step 570 | loss 4.9475 | lr 8.48e-05 | gates=[0.009993841871619225, 0.9982759952545166] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9968988299369812, 0.0013938543852418661, 0.0017072989139705896]\n",
      "step 580 | loss 5.0558 | lr 8.16e-05 | gates=[0.0031011535320430994, 0.998785138130188] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993639588356018, 0.0002102721919072792, 0.0004256491083651781]\n",
      "step 590 | loss 1.2979 | lr 7.83e-05 | gates=[0.0006359213148243725, 0.3118142783641815] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9991278648376465, 0.0004834667488466948, 0.00038868337287567556]\n",
      "step 600 | loss 5.4609 | lr 7.51e-05 | gates=[0.0008721500635147095, 0.9930704236030579] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997652769088745, 0.0001134744961746037, 0.00012127619265811518]\n",
      "step 610 | loss 1.6292 | lr 7.19e-05 | gates=[0.00023475066700484604, 0.3527025282382965] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9974035024642944, 0.001456436701118946, 0.0011400936637073755]\n",
      "step 620 | loss 5.0956 | lr 6.88e-05 | gates=[0.0025965303648263216, 0.9631045460700989] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999279260635376, 0.00033721199724823236, 0.0003834104281850159]\n",
      "step 630 | loss 1.5244 | lr 6.57e-05 | gates=[0.0007206224254332483, 0.26610735058784485] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992007613182068, 0.00031578438938595355, 0.0004834654391743243]\n",
      "step 640 | loss 5.0124 | lr 6.26e-05 | gates=[0.0007992498576641083, 0.9850162267684937] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995157122612, 0.00022223559790290892, 0.0002619707665871829]\n",
      "step 650 | loss 1.3289 | lr 5.95e-05 | gates=[0.00048420639359392226, 0.31095394492149353] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989814162254333, 0.0004174291098024696, 0.0006011087680235505]\n",
      "step 660 | loss 1.1963 | lr 5.65e-05 | gates=[0.0010185379069298506, 0.1479654610157013] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992262125015259, 0.000366324296919629, 0.0004074862808920443]\n",
      "step 670 | loss 5.4648 | lr 5.36e-05 | gates=[0.0007738106069155037, 0.9380990266799927] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9970566034317017, 0.0016227897722274065, 0.0013206142466515303]\n",
      "step 680 | loss 4.9886 | lr 5.07e-05 | gates=[0.0029434040188789368, 0.9837366938591003] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9891848564147949, 0.004712347406893969, 0.006102824117988348]\n",
      "step 690 | loss 4.9407 | lr 4.78e-05 | gates=[0.010815172456204891, 0.9981580972671509] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9996775388717651, 0.00015408896433655173, 0.00016839201271068305]\n",
      "step 700 | loss 1.3815 | lr 4.50e-05 | gates=[0.0003224809479434043, 0.34109026193618774] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9990466237068176, 0.0003848462365567684, 0.0005684979259967804]\n",
      "step 710 | loss 5.0250 | lr 4.23e-05 | gates=[0.0009533441625535488, 0.9994632005691528] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997321367263794, 0.0001152060431195423, 0.0001526730484329164]\n",
      "step 720 | loss 4.9153 | lr 3.96e-05 | gates=[0.0002678791352082044, 0.9993774890899658] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993274807929993, 0.0003471566888038069, 0.00032528224983252585]\n",
      "step 730 | loss 1.4102 | lr 3.70e-05 | gates=[0.0006724389386363328, 0.23159585893154144] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994624257087708, 0.00015511387027800083, 0.00038245669566094875]\n",
      "step 740 | loss 1.7672 | lr 3.45e-05 | gates=[0.0005375705659389496, 0.24403680860996246] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985863566398621, 0.0006907154456712306, 0.000722875411156565]\n",
      "step 750 | loss 4.9354 | lr 3.20e-05 | gates=[0.0014135908568277955, 0.9981808066368103] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9964116811752319, 0.0019842395558953285, 0.001604064367711544]\n",
      "step 760 | loss 4.9794 | lr 2.96e-05 | gates=[0.003588303690776229, 0.9964448809623718] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9992456436157227, 0.000296151963993907, 0.00045820651575922966]\n",
      "step 770 | loss 4.8814 | lr 2.73e-05 | gates=[0.0007543584797531366, 0.9972831606864929] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9993622303009033, 0.0003183090011589229, 0.0003194394230376929]\n",
      "step 780 | loss 4.9061 | lr 2.51e-05 | gates=[0.0006377483950927854, 0.9969325661659241] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9994468092918396, 0.00026000631623901427, 0.0002931632043328136]\n",
      "step 790 | loss 4.9229 | lr 2.29e-05 | gates=[0.000553169462364167, 0.9943794012069702] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.987548828125, 0.005886713974177837, 0.006564442999660969]\n",
      "step 800 | loss 4.9824 | lr 2.09e-05 | gates=[0.012451156973838806, 0.9878791570663452] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9995411038398743, 0.00021397638192865998, 0.00024482380831614137]\n",
      "step 810 | loss 1.2960 | lr 2.00e-05 | gates=[0.0004588002630043775, 0.24190032482147217] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9994837641716003, 0.00024750884040258825, 0.000268618983682245]\n",
      "step 820 | loss 1.1706 | lr 2.00e-05 | gates=[0.0005161277949810028, 0.5058131217956543] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995696544647217, 0.00018310657469555736, 0.0002472450432833284]\n",
      "step 830 | loss 1.4328 | lr 2.00e-05 | gates=[0.0004303515888750553, 0.4151466488838196] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9992794990539551, 0.00040731183253228664, 0.00031316897366195917]\n",
      "step 840 | loss 4.9192 | lr 2.00e-05 | gates=[0.0007204808061942458, 0.9682255387306213] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9973157048225403, 0.0015011997893452644, 0.001183091546408832]\n",
      "step 850 | loss 5.4733 | lr 2.00e-05 | gates=[0.0026842914521694183, 0.9695905447006226] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9984465837478638, 0.0008013759506866336, 0.0007520103827118874]\n",
      "step 860 | loss 5.4458 | lr 2.00e-05 | gates=[0.0015533862169831991, 0.9622341990470886] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9992752075195312, 0.00029315066058188677, 0.00043164336238987744]\n",
      "step 870 | loss 1.4443 | lr 2.00e-05 | gates=[0.0007247939938679338, 0.2864270806312561] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998058557510376, 0.0012520167510956526, 0.0006893855752423406]\n",
      "step 880 | loss 4.9024 | lr 2.00e-05 | gates=[0.0019414022099226713, 0.9473040103912354] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.999628484249115, 0.0002026040747296065, 0.00016889104153960943]\n",
      "step 890 | loss 4.9196 | lr 2.00e-05 | gates=[0.0003714951453730464, 0.892257809638977] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995992183685303, 0.0002304210065631196, 0.0001703958259895444]\n",
      "step 900 | loss 5.4798 | lr 2.00e-05 | gates=[0.0004008168471045792, 0.9074316024780273] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993719458580017, 0.0003039695438928902, 0.00032405630918219686]\n",
      "step 910 | loss 4.9342 | lr 2.00e-05 | gates=[0.000628025911282748, 0.9603164196014404] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.980353593826294, 0.009927877224981785, 0.009718525223433971]\n",
      "step 920 | loss 4.9108 | lr 2.00e-05 | gates=[0.019646402448415756, 0.9596182107925415] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9994939565658569, 0.0002486254379618913, 0.00025739040574990213]\n",
      "step 930 | loss 1.3630 | lr 2.00e-05 | gates=[0.0005060158437117934, 0.32287973165512085] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9989897012710571, 0.0006525397184304893, 0.0003578217001631856]\n",
      "step 940 | loss 4.8599 | lr 2.00e-05 | gates=[0.0010103614768013358, 0.8978191614151001] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997316598892212, 0.0001488510170020163, 0.00011938353418372571]\n",
      "step 950 | loss 1.4242 | lr 2.00e-05 | gates=[0.000268234551185742, 0.27035972476005554] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9982223510742188, 0.0008743125945329666, 0.0009033864480443299]\n",
      "step 960 | loss 4.8592 | lr 2.00e-05 | gates=[0.0017776989843696356, 0.9495666027069092] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9995356798171997, 0.00018999243911821395, 0.0002742756041698158]\n",
      "step 970 | loss 1.2798 | lr 2.00e-05 | gates=[0.0004642680287361145, 0.29932206869125366] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9979925155639648, 0.0010176774812862277, 0.000989781692624092]\n",
      "step 980 | loss 4.7617 | lr 2.00e-05 | gates=[0.002007459057494998, 0.9263211488723755] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9983053803443909, 0.0010044740047305822, 0.0006901649758219719]\n",
      "step 990 | loss 5.4611 | lr 2.00e-05 | gates=[0.0016946389805525541, 0.9701287746429443] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9994372129440308, 0.00030072100344114006, 0.00026197207625955343]\n",
      "step 1000 | loss 1.2499 | lr 2.00e-05 | gates=[0.0005626931088045239, 0.31703704595565796] | mix=hf\n",
      "[after  E15b_two_experts_smallW_s4242_1755994049] alloc=10.69 GB | reserved=33.27 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.69 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('E15a_write_sparse_light_s1337_1755981882',\n  'E15b_two_experts_smallW_s1337_1755983525'),\n ('E15a_write_sparse_light_s2027_1755987068',\n  'E15b_two_experts_smallW_s2027_1755988655'),\n ('E15a_write_sparse_light_s4242_1755992098',\n  'E15b_two_experts_smallW_s4242_1755994049')]"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_e15_suite(steps=1000, seeds=(1337,2027,4242), include_haystack=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T01:04:16.536365300Z",
     "start_time": "2025-08-23T20:44:42.363665300Z"
    }
   },
   "id": "55db520362204e2a"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=21.38 GB | reserved=21.43 GB | free=2.74 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T05:31:28.619106300Z",
     "start_time": "2025-08-24T05:31:28.453106300Z"
    }
   },
   "id": "9f4060772a99a581"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# --- E16: composite configs (multi-memory experts + tiered per-block + regs + curriculum) ---\n",
    "\n",
    "def set_e16a(\n",
    "    steps:int,\n",
    "    K:int=2,\n",
    "    expert_N_each:int=64,\n",
    "    expert_W:int=None,     # None -> use CFG.dnc_cell_size\n",
    "    expert_R:int=1,\n",
    "    gate_temp:float=0.9,\n",
    "    diversity_lambda:float=1e-3,\n",
    "    warm_stage:int=None,   # None -> auto = min(steps//4, 250)\n",
    "):\n",
    "    \"\"\"\n",
    "    E16a: early block = many/narrow, later = fewer/wider.\n",
    "    + two memory experts per block, diversity reg, and curriculum warm-start (memory-only early).\n",
    "    \"\"\"\n",
    "    # Defaults derived from current CFG to avoid surprises\n",
    "    W_default = getattr(CFG, \"dnc_cell_size\", 64)\n",
    "    expert_W  = int(expert_W or W_default)\n",
    "    warm_S    = int(warm_stage if warm_stage is not None else min(steps // 4, 250))\n",
    "\n",
    "    # core knobs\n",
    "    CFG.mem_experts = int(K)\n",
    "    CFG.expert_N    = [int(expert_N_each)] * K\n",
    "    CFG.expert_W    = int(expert_W)\n",
    "    CFG.expert_R    = int(expert_R)\n",
    "    CFG.expert_gate_temp       = float(gate_temp)\n",
    "    CFG.expert_diversity_lambda = float(diversity_lambda)\n",
    "\n",
    "    # per-block tiering (2 blocks assumed; adjust if CFG.n_blocks != 2)\n",
    "    CFG.per_block_cfg = [\n",
    "        {\"N\":128, \"W\":32, \"R\":1, \"gate_temp\":1.0, \"free_bias\": +0.30},  # block 0: many & shallow\n",
    "        {\"N\": 64, \"W\":64, \"R\":1, \"gate_temp\":0.8, \"free_bias\": -0.20},  # block 1: fewer & deeper\n",
    "    ]\n",
    "\n",
    "    # curriculum: force memory exploration first, then revert to baseline mix\n",
    "    CFG.mixture_schedule    = [(warm_S, (0.0, 0.34, 0.33, 0.33)), (None, (0.4, 0.2, 0.2, 0.2))]\n",
    "    CFG.gate_temp_schedule  = [(warm_S, 0.8), (None, 1.0)]\n",
    "    # keep your existing gate_reg_lambda from CFG; or optionally:\n",
    "    # CFG.gate_reg_schedule = [(warm_S, getattr(CFG, \"gate_reg_lambda\", 2e-4)), (None, getattr(CFG, \"gate_reg_lambda\", 2e-4))]\n",
    "\n",
    "    # keep fusion OFF in E16 composites unless explicitly tested\n",
    "    CFG.fusion_enable = False\n",
    "\n",
    "\n",
    "def set_e16b(\n",
    "    steps:int,\n",
    "    K:int=2,\n",
    "    expert_N_each:int=64,\n",
    "    expert_W:int=None,\n",
    "    expert_R:int=1,\n",
    "    gate_temp:float=0.8,\n",
    "    diversity_lambda:float=1e-3,\n",
    "    warm_stage:int=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    E16b: reversed tiering—early block fewer/wider, later block many/narrow.\n",
    "    Other ingredients same as E16a.\n",
    "    \"\"\"\n",
    "    W_default = getattr(CFG, \"dnc_cell_size\", 64)\n",
    "    expert_W  = int(expert_W or W_default)\n",
    "    warm_S    = int(warm_stage if warm_stage is not None else min(steps // 4, 250))\n",
    "\n",
    "    CFG.mem_experts = int(K)\n",
    "    CFG.expert_N    = [int(expert_N_each)] * K\n",
    "    CFG.expert_W    = int(expert_W)\n",
    "    CFG.expert_R    = int(expert_R)\n",
    "    CFG.expert_gate_temp       = float(gate_temp)\n",
    "    CFG.expert_diversity_lambda = float(diversity_lambda)\n",
    "\n",
    "    CFG.per_block_cfg = [\n",
    "        {\"N\": 64, \"W\":64, \"R\":1, \"gate_temp\":1.0, \"free_bias\": -0.20},  # block 0: fewer & deeper\n",
    "        {\"N\":128, \"W\":32, \"R\":1, \"gate_temp\":0.9, \"free_bias\": +0.30},  # block 1: many & shallow\n",
    "    ]\n",
    "\n",
    "    CFG.mixture_schedule    = [(warm_S, (0.0, 0.34, 0.33, 0.33)), (None, (0.4, 0.2, 0.2, 0.2))]\n",
    "    CFG.gate_temp_schedule  = [(warm_S, 0.8), (None, 1.0)]\n",
    "    CFG.fusion_enable = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T05:00:30.019836900Z",
     "start_time": "2025-08-24T05:00:29.994838200Z"
    }
   },
   "id": "2211883544baaa75"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# --- E16 runners: convenience wrappers ---\n",
    "\n",
    "def _set_seed_all(seed:int):\n",
    "    import random, numpy as _np, torch as _t\n",
    "    random.seed(seed); _np.random.seed(seed); _t.manual_seed(seed)\n",
    "    if _t.cuda.is_available():\n",
    "        _t.cuda.manual_seed_all(seed)\n",
    "\n",
    "def run_e16_once(label:str, steps:int, mode:str=\"a\", seed:int=1337):\n",
    "    \"\"\"\n",
    "    mode='a' -> set_e16a; mode='b' -> set_e16b\n",
    "    Creates a unique TB run for each labeled call.\n",
    "    \"\"\"\n",
    "    _set_seed_all(int(seed))\n",
    "    if mode.lower().startswith(\"a\"):\n",
    "        set_e16a(steps=steps)\n",
    "    else:\n",
    "        set_e16b(steps=steps)\n",
    "\n",
    "    # housekeeping for clean VRAM\n",
    "    free_head_and_cache()\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}-s{seed}\")\n",
    "    time.sleep(1.2)  # keep distinct TB dirs\n",
    "\n",
    "    # start TB run; your start_tb_run(label) helper sets tb writer accordingly\n",
    "    start_tb_run(f\"{label}-s{seed}\")\n",
    "\n",
    "    # optional: echo cfg for reproducibility\n",
    "    if TB_AVAILABLE and 'tb' in globals():\n",
    "        tb.add_text(\"run/config/E16\", json.dumps({\n",
    "            \"label\": label, \"mode\": mode, \"steps\": steps, \"seed\": seed,\n",
    "            \"mem_experts\": getattr(CFG, \"mem_experts\", 1),\n",
    "            \"per_block_cfg\": getattr(CFG, \"per_block_cfg\", None),\n",
    "            \"mixture_schedule\": getattr(CFG, \"mixture_schedule\", None),\n",
    "            \"gate_temp_schedule\": getattr(CFG, \"gate_temp_schedule\", None),\n",
    "            \"diversity_lambda\": getattr(CFG, \"expert_diversity_lambda\", 0.0),\n",
    "        }, indent=2), 0)\n",
    "\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=max(10, steps//20),\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),      # baseline mix; E16 sets schedules on CFG\n",
    "        mixture_schedule=getattr(CFG, \"mixture_schedule\", None),\n",
    "        gate_temp_schedule=getattr(CFG, \"gate_temp_schedule\", None),\n",
    "        gate_reg_schedule=getattr(CFG, \"gate_reg_schedule\", None),\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}-s{seed}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "def run_e16_sweep(steps:int=1000, seeds=(1337, 2027, 4242)):\n",
    "    \"\"\"\n",
    "    Launch E16a and E16b across seeds. Adjust steps to taste for overnight runs.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in seeds:\n",
    "        print(f\"\\n=== E16a (tier A) | seed={s} ===\")\n",
    "        results[(f\"E16a\", s)] = run_e16_once(label=\"E16a\", steps=steps, mode=\"a\", seed=s)\n",
    "    for s in seeds:\n",
    "        print(f\"\\n=== E16b (tier B) | seed={s} ===\")\n",
    "        results[(f\"E16b\", s)] = run_e16_once(label=\"E16b\", steps=steps, mode=\"b\", seed=s)\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T05:00:31.538811200Z",
     "start_time": "2025-08-24T05:00:31.506811200Z"
    }
   },
   "id": "5dce7625919d22c6"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a216f85aa7c240ae92979d853b85a5a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smoke_e16_forward] mode=a ok.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "231bde017b2d48fca0e4578029b5fc4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[smoke_e16_forward] mode=b ok.\n"
     ]
    }
   ],
   "source": [
    "# --- E16 smoke: build config + single forward on tiny batch ---\n",
    "@torch.no_grad()\n",
    "def smoke_e16_forward(mode=\"a\"):\n",
    "    steps = 20\n",
    "    if mode.lower().startswith(\"a\"):\n",
    "        set_e16a(steps=steps, K=2, expert_N_each=32)  # smaller N for a very quick pass\n",
    "    else:\n",
    "        set_e16b(steps=steps, K=2, expert_N_each=32)\n",
    "\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    requires_grad_(base, False)\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    head.eval()\n",
    "\n",
    "    B, T = 2, 8\n",
    "    vocab_guess = getattr(base.get_input_embeddings(), \"num_embeddings\", 1024)\n",
    "    x = torch.randint(0, min(1024, vocab_guess), (B, T), device=device)\n",
    "    logits, gates, aux = head.forward_with_metrics(x)\n",
    "\n",
    "    assert logits.shape[:2] == (B, T)\n",
    "    assert isinstance(gates, (list, tuple)) and len(gates) == getattr(CFG, \"n_blocks\", len(head.blocks))\n",
    "    # sanity: ensure expert metrics are present\n",
    "    for i, blk in enumerate(head.blocks):\n",
    "        if hasattr(blk, \"last_metrics\") and blk.last_metrics:\n",
    "            _ = blk.last_metrics.get(\"experts_pi_entropy\", None)\n",
    "    print(f\"[smoke_e16_forward] mode={mode} ok.\")\n",
    "\n",
    "# quick check:\n",
    "smoke_e16_forward(\"a\")\n",
    "smoke_e16_forward(\"b\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T05:00:48.873297200Z",
     "start_time": "2025-08-24T05:00:36.560130500Z"
    }
   },
   "id": "22b30e82faff5798"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E16a (tier A) | seed=1337 ===\n",
      "[after free] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "[before E16a-s1337] alloc=0.02 GB | reserved=0.03 GB | free=24.33 GB | total=25.77 GB\n",
      "TB run started: ./runs\\dncformer-20250823-220057-E16a-s1337\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "298de9701b6e44ef8e590e59618ee51b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.998247504234314, 0.0003349798498675227, 0.0014175600372254848]\n",
      "step 10 | loss 11.0822 | lr 2.00e-05 | gates=[0.0017525398870930076, 0.009336728602647781] | mix=copy\n",
      "[after  E16a-s1337] alloc=10.70 GB | reserved=28.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=10.70 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "\n",
      "=== E16b (tier B) | seed=1337 ===\n",
      "[after free] alloc=10.70 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "[before E16b-s1337] alloc=10.70 GB | reserved=10.72 GB | free=13.64 GB | total=25.77 GB\n",
      "TB run started: ./runs\\dncformer-20250823-222516-E16b-s1337\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "989de050f8c24c7a85882e7af0704e56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.9990187287330627, 0.00027480811695568264, 0.0007064775563776493]\n",
      "step 10 | loss 9.0837 | lr 2.00e-05 | gates=[0.0009812857024371624, 0.7871806621551514] | mix=copy\n",
      "[after  E16b-s1337] alloc=21.38 GB | reserved=39.21 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=21.38 GB | reserved=21.43 GB | free=2.89 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "#_ = run_e16_sweep(steps=10, seeds=(1337,))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T05:26:47.281923500Z",
     "start_time": "2025-08-24T05:00:55.741925900Z"
    }
   },
   "id": "252156810e72b405"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E16a (tier A) | seed=1337 ===\n",
      "[after free] alloc=21.38 GB | reserved=21.43 GB | free=2.73 GB | total=25.77 GB\n",
      "[before E16a-s1337] alloc=21.38 GB | reserved=21.43 GB | free=2.73 GB | total=25.77 GB\n",
      "TB run started: ./runs\\dncformer-20250823-223458-E16a-s1337\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1415a2f84bf4206a94ec6b4151b552f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8429850339889526, 0.0881362333893776, 0.06887868046760559]\n",
      "step 10 | loss 7.2834 | lr 4.40e-05 | gates=[0.1570149064064026, 0.6872451901435852] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4031856060028076, 0.4203474521636963, 0.1764669418334961]\n",
      "step 20 | loss 7.0721 | lr 8.40e-05 | gates=[0.5968143939971924, 0.6538575887680054] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9398620128631592, 0.024761836975812912, 0.03537613898515701]\n",
      "step 30 | loss 6.2750 | lr 1.24e-04 | gates=[0.060137972235679626, 0.7713171243667603] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9718441367149353, 0.010407068766653538, 0.01774880662560463]\n",
      "step 40 | loss 5.5585 | lr 1.64e-04 | gates=[0.028155872598290443, 0.7868714332580566] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9845647811889648, 0.0056892093271017075, 0.009745968505740166]\n",
      "step 50 | loss 5.8682 | lr 2.00e-04 | gates=[0.015435177832841873, 0.814529299736023] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9901917576789856, 0.002610857365652919, 0.00719738332554698]\n",
      "step 60 | loss 5.9128 | lr 2.00e-04 | gates=[0.009808240458369255, 0.9999359846115112] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9881971478462219, 0.005513257347047329, 0.006289591547101736]\n",
      "step 70 | loss 5.4141 | lr 2.00e-04 | gates=[0.011802848428487778, 0.9999837875366211] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9438258409500122, 0.02050134725868702, 0.0356728658080101]\n",
      "step 80 | loss 5.4641 | lr 1.99e-04 | gates=[0.05617421492934227, 0.9999533891677856] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9415916204452515, 0.020238574594259262, 0.03816981613636017]\n",
      "step 90 | loss 5.4121 | lr 1.99e-04 | gates=[0.05840838700532913, 0.9999822974205017] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9959539771080017, 0.0006829033955000341, 0.003363086376339197]\n",
      "step 100 | loss 5.1340 | lr 1.99e-04 | gates=[0.004045989364385605, 0.9999999403953552] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9738924503326416, 0.011938238516449928, 0.014169312082231045]\n",
      "step 110 | loss 5.6527 | lr 1.98e-04 | gates=[0.02610754780471325, 0.9999985098838806] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9609432220458984, 0.015944577753543854, 0.023112233728170395]\n",
      "step 120 | loss 5.5552 | lr 1.97e-04 | gates=[0.03905681148171425, 0.9999982118606567] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9777857065200806, 0.012430433183908463, 0.009783883579075336]\n",
      "step 130 | loss 5.6286 | lr 1.96e-04 | gates=[0.022214313969016075, 0.9999984502792358] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9794735908508301, 0.004793921019881964, 0.015732455998659134]\n",
      "step 140 | loss 6.1607 | lr 1.96e-04 | gates=[0.020526375621557236, 0.9999997615814209] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9824104309082031, 0.0069169821217656136, 0.01067260466516018]\n",
      "step 150 | loss 5.6387 | lr 1.94e-04 | gates=[0.017589587718248367, 0.9999988079071045] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9436171054840088, 0.030682023614645004, 0.025700921192765236]\n",
      "step 160 | loss 5.3801 | lr 1.93e-04 | gates=[0.05638294667005539, 0.9999951720237732] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9893683791160583, 0.003992210607975721, 0.006639381870627403]\n",
      "step 170 | loss 5.7752 | lr 1.92e-04 | gates=[0.010631591081619263, 0.9999994039535522] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9833724498748779, 0.009812884032726288, 0.006814669352024794]\n",
      "step 180 | loss 5.5080 | lr 1.91e-04 | gates=[0.01662755198776722, 0.9999991655349731] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9280361533164978, 0.027024967595934868, 0.04493885487318039]\n",
      "step 190 | loss 5.5159 | lr 1.89e-04 | gates=[0.07196381688117981, 0.9999982714653015] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9901361465454102, 0.005672206170856953, 0.004191606305539608]\n",
      "step 200 | loss 5.6484 | lr 1.88e-04 | gates=[0.009863811545073986, 0.9999991059303284] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9878665208816528, 0.0033435989171266556, 0.008789855055510998]\n",
      "step 210 | loss 5.5383 | lr 1.86e-04 | gates=[0.012133455835282803, 0.9999996423721313] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9568549394607544, 0.024779420346021652, 0.01836561970412731]\n",
      "step 220 | loss 5.4083 | lr 1.84e-04 | gates=[0.04314504191279411, 0.9999975562095642] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9657739400863647, 0.02231493778526783, 0.011911098845303059]\n",
      "step 230 | loss 5.1249 | lr 1.83e-04 | gates=[0.034226033836603165, 0.9999961853027344] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9975724220275879, 0.0007402723422273993, 0.001687367563135922]\n",
      "step 240 | loss 5.0773 | lr 1.81e-04 | gates=[0.0024276399053633213, 0.9999998807907104] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9719977378845215, 0.016570020467042923, 0.01143223512917757]\n",
      "step 250 | loss 5.5207 | lr 1.79e-04 | gates=[0.028002256527543068, 0.999998927116394] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9971500635147095, 0.0005873870104551315, 0.00226255739107728]\n",
      "step 260 | loss 5.2421 | lr 1.77e-04 | gates=[0.0028499446343630552, 0.9999977946281433] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9944765567779541, 0.0010334528051316738, 0.0044899932108819485]\n",
      "step 270 | loss 5.4923 | lr 1.74e-04 | gates=[0.005523445550352335, 0.9989895820617676] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9925543665885925, 0.004968819208443165, 0.002476808149367571]\n",
      "step 280 | loss 5.7368 | lr 1.72e-04 | gates=[0.007445627357810736, 0.9940847158432007] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.25931084156036377, 0.4277096092700958, 0.3129795789718628]\n",
      "step 290 | loss 4.4997 | lr 1.70e-04 | gates=[0.7406891584396362, 0.3737877309322357] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9995017051696777, 0.00018619104230310768, 0.00031212170142680407]\n",
      "step 300 | loss 2.1397 | lr 1.67e-04 | gates=[0.000498312758281827, 0.0006066769710741937] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.997115433216095, 0.0012532254913821816, 0.001631345134228468]\n",
      "step 310 | loss 5.5228 | lr 1.65e-04 | gates=[0.0028845705091953278, 0.9999926090240479] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.99363112449646, 0.002459955867379904, 0.003908969461917877]\n",
      "step 320 | loss 5.1053 | lr 1.62e-04 | gates=[0.006368924863636494, 0.9994562268257141] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997455477714539, 7.000313780736178e-05, 0.00018446717876940966]\n",
      "step 330 | loss 8.3042 | lr 1.60e-04 | gates=[0.0002544703020248562, 0.3675335645675659] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9897041916847229, 0.003399715293198824, 0.006896087434142828]\n",
      "step 340 | loss 6.1400 | lr 1.57e-04 | gates=[0.010295801796019077, 0.9999986886978149] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9960601925849915, 0.0006897103157825768, 0.0032500820234417915]\n",
      "step 350 | loss 5.4585 | lr 1.54e-04 | gates=[0.0039397915825247765, 0.996697187423706] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9917508959770203, 0.001640656846575439, 0.0066084424033761024]\n",
      "step 360 | loss 5.4537 | lr 1.52e-04 | gates=[0.008249099366366863, 0.999922513961792] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9999584555625916, 1.4664829905086663e-05, 2.688742824830115e-05]\n",
      "step 370 | loss 1.9839 | lr 1.49e-04 | gates=[4.1552255424903706e-05, 0.00016939404304139316] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9507949352264404, 0.024088338017463684, 0.025116680189967155]\n",
      "step 380 | loss 5.0699 | lr 1.46e-04 | gates=[0.04920502007007599, 0.9919211864471436] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9997243285179138, 0.00011534514487721026, 0.00016025477088987827]\n",
      "step 390 | loss 1.7212 | lr 1.43e-04 | gates=[0.00027559991576708853, 0.16978293657302856] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9919261932373047, 0.004090983420610428, 0.003982824273407459]\n",
      "step 400 | loss 5.5628 | lr 1.40e-04 | gates=[0.008073807694017887, 0.9999983310699463] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9954270124435425, 0.0016394141130149364, 0.0029336365405470133]\n",
      "step 410 | loss 5.6620 | lr 1.37e-04 | gates=[0.004573049955070019, 0.9999985098838806] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9947669506072998, 0.00038934190524742007, 0.0048437574878335]\n",
      "step 420 | loss 6.0029 | lr 1.34e-04 | gates=[0.005233099218457937, 0.9990258812904358] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9971646666526794, 0.0020513897761702538, 0.0007839456084184349]\n",
      "step 430 | loss 5.5210 | lr 1.31e-04 | gates=[0.002835335209965706, 0.999951958656311] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9753537178039551, 0.011950215324759483, 0.012696020305156708]\n",
      "step 440 | loss 5.0655 | lr 1.27e-04 | gates=[0.02464623562991619, 0.9955483675003052] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9771775007247925, 0.007301762234419584, 0.015520723536610603]\n",
      "step 450 | loss 5.3348 | lr 1.24e-04 | gates=[0.022822486236691475, 0.9993702173233032] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9980196356773376, 0.0005118765402585268, 0.001468500355258584]\n",
      "step 460 | loss 1.8200 | lr 1.21e-04 | gates=[0.001980376662686467, 0.0044786748476326466] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996086955070496, 7.392491534119472e-05, 0.0003173386794514954]\n",
      "step 470 | loss 2.1950 | lr 1.18e-04 | gates=[0.00039126360206864774, 0.08745928108692169] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9747081995010376, 0.017981261014938354, 0.007310491520911455]\n",
      "step 480 | loss 5.1883 | lr 1.14e-04 | gates=[0.025291752070188522, 0.9941412210464478] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9999259114265442, 1.5545285350526683e-05, 5.84866720600985e-05]\n",
      "step 490 | loss 2.3804 | lr 1.11e-04 | gates=[7.40319665055722e-05, 0.03236811235547066] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9944788813591003, 0.002320315456017852, 0.0032008092384785414]\n",
      "step 500 | loss 5.2336 | lr 1.08e-04 | gates=[0.005521124694496393, 0.9999901056289673] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9998196959495544, 5.1689970860024914e-05, 0.00012870946375187486]\n",
      "step 510 | loss 1.7115 | lr 1.05e-04 | gates=[0.00018039940914604813, 0.10260162502527237] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9798978567123413, 0.0023570284247398376, 0.017745105549693108]\n",
      "step 520 | loss 5.1200 | lr 1.01e-04 | gates=[0.020102132111787796, 0.9999828338623047] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9935927391052246, 0.0024132912512868643, 0.003993961960077286]\n",
      "step 530 | loss 5.4627 | lr 9.80e-05 | gates=[0.006407252978533506, 0.9999423027038574] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9911127090454102, 0.0032535786740481853, 0.005633743479847908]\n",
      "step 540 | loss 5.5177 | lr 9.47e-05 | gates=[0.00888732261955738, 0.9999210834503174] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997707009315491, 6.957552977837622e-05, 0.00015971662651281804]\n",
      "step 550 | loss 1.8077 | lr 9.14e-05 | gates=[0.00022929214173927903, 0.3755846619606018] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9987540245056152, 0.0003401816065888852, 0.0009058137657120824]\n",
      "step 560 | loss 2.4123 | lr 8.81e-05 | gates=[0.001245995401404798, 0.40651437640190125] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9962607622146606, 0.0001828440435929224, 0.003556410316377878]\n",
      "step 570 | loss 5.5437 | lr 8.48e-05 | gates=[0.003739254316315055, 0.9999635219573975] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9992045164108276, 0.0003312818589620292, 0.0004642148269340396]\n",
      "step 580 | loss 1.9175 | lr 8.16e-05 | gates=[0.0007954967441037297, 0.4987294673919678] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9951786398887634, 0.0012789592146873474, 0.0035423957742750645]\n",
      "step 590 | loss 5.0060 | lr 7.83e-05 | gates=[0.004821354523301125, 0.9998820424079895] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9997909665107727, 4.655950397136621e-05, 0.0001623410062165931]\n",
      "step 600 | loss 2.2169 | lr 7.51e-05 | gates=[0.0002089005138259381, 0.6334509253501892] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9744342565536499, 0.003450451185926795, 0.022115331143140793]\n",
      "step 610 | loss 5.0115 | lr 7.19e-05 | gates=[0.025565780699253082, 0.9992400407791138] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9779289364814758, 0.008214361034333706, 0.0138567378744483]\n",
      "step 620 | loss 4.9474 | lr 6.88e-05 | gates=[0.022071098908782005, 0.9995585680007935] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9982059001922607, 0.0006088617374189198, 0.0011852579191327095]\n",
      "step 630 | loss 1.4834 | lr 6.57e-05 | gates=[0.0017941194819286466, 0.1347581148147583] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.993015468120575, 0.004866180010139942, 0.002118341624736786]\n",
      "step 640 | loss 5.4749 | lr 6.26e-05 | gates=[0.006984521169215441, 0.9998732805252075] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9773685336112976, 0.009804069995880127, 0.012827406637370586]\n",
      "step 650 | loss 4.9502 | lr 5.95e-05 | gates=[0.022631477564573288, 0.9978915452957153] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9968850016593933, 0.002113413531333208, 0.0010015587322413921]\n",
      "step 660 | loss 5.1364 | lr 5.65e-05 | gates=[0.003114971797913313, 0.999950647354126] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.999720573425293, 5.0769118388416246e-05, 0.0002286491944687441]\n",
      "step 670 | loss 1.4664 | lr 5.36e-05 | gates=[0.000279418338323012, 0.30690139532089233] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9780778884887695, 0.002578685525804758, 0.019343456253409386]\n",
      "step 680 | loss 5.4975 | lr 5.07e-05 | gates=[0.021922141313552856, 0.999728798866272] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9996728897094727, 9.034772665472701e-05, 0.00023680893355049193]\n",
      "step 690 | loss 1.5854 | lr 4.78e-05 | gates=[0.0003271566529292613, 0.3594760000705719] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9977346062660217, 0.001726220827549696, 0.0005391843733377755]\n",
      "step 700 | loss 4.9693 | lr 4.50e-05 | gates=[0.002265405375510454, 0.9997638463973999] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9938339591026306, 0.0011606019688770175, 0.0050054085440933704]\n",
      "step 710 | loss 5.4800 | lr 4.23e-05 | gates=[0.0061660101637244225, 0.9992004632949829] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9971047639846802, 0.000979484524577856, 0.001915785251185298]\n",
      "step 720 | loss 4.9873 | lr 3.96e-05 | gates=[0.0028952700085937977, 0.9997940063476562] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9996967315673828, 7.734274549875408e-05, 0.00022592685127165169]\n",
      "step 730 | loss 1.5933 | lr 3.70e-05 | gates=[0.00030326959677040577, 0.32774588465690613] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985251426696777, 0.0005358431953936815, 0.0009390423656441271]\n",
      "step 740 | loss 1.4319 | lr 3.45e-05 | gates=[0.0014748856192454696, 0.29176202416419983] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9947799444198608, 0.0027480032294988632, 0.0024720998480916023]\n",
      "step 750 | loss 5.0392 | lr 3.20e-05 | gates=[0.0052201030775904655, 0.9995372295379639] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9810929894447327, 0.012986764311790466, 0.00592023553326726]\n",
      "step 760 | loss 5.4839 | lr 2.96e-05 | gates=[0.01890699937939644, 0.9997817277908325] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9967454671859741, 0.0017874549375846982, 0.0014671499375253916]\n",
      "step 770 | loss 5.1124 | lr 2.73e-05 | gates=[0.0032546045258641243, 0.9988252520561218] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9958463311195374, 0.001665060524828732, 0.002488593338057399]\n",
      "step 780 | loss 5.5775 | lr 2.51e-05 | gates=[0.004153653979301453, 0.9993916749954224] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9975948929786682, 0.0011234255507588387, 0.001281615812331438]\n",
      "step 790 | loss 1.9010 | lr 2.29e-05 | gates=[0.0024050415959209204, 0.44774940609931946] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9969825148582458, 0.002535869600251317, 0.00048162080929614604]\n",
      "step 800 | loss 4.8548 | lr 2.09e-05 | gates=[0.0030174909625202417, 0.9997089505195618] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.997127890586853, 0.0015741476090624928, 0.0012980041792616248]\n",
      "step 810 | loss 4.9848 | lr 2.00e-05 | gates=[0.0028721517883241177, 0.9996156692504883] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9933139681816101, 0.0026243124157190323, 0.004061717074364424]\n",
      "step 820 | loss 4.9360 | lr 2.00e-05 | gates=[0.006686028558760881, 0.9995381832122803] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9995598196983337, 0.0001773012918420136, 0.00026284903287887573]\n",
      "step 830 | loss 1.5222 | lr 2.00e-05 | gates=[0.0004401502956170589, 0.3455987870693207] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9773058295249939, 0.01566043496131897, 0.007033742032945156]\n",
      "step 840 | loss 5.4895 | lr 2.00e-05 | gates=[0.0226941779255867, 0.998903751373291] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.995053231716156, 0.0020924829877913, 0.0028543169610202312]\n",
      "step 850 | loss 4.8990 | lr 2.00e-05 | gates=[0.004946800414472818, 0.9996004104614258] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9979979395866394, 0.0008682917687110603, 0.0011337886098772287]\n",
      "step 860 | loss 1.2780 | lr 2.00e-05 | gates=[0.00200208043679595, 0.27774935960769653] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9980105757713318, 0.0010911596473306417, 0.0008982468862086535]\n",
      "step 870 | loss 5.0094 | lr 2.00e-05 | gates=[0.001989406766369939, 0.9993482828140259] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9749325513839722, 0.014447592198848724, 0.010619854554533958]\n",
      "step 880 | loss 5.1815 | lr 2.00e-05 | gates=[0.025067444890737534, 0.9988276362419128] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9188858866691589, 0.05366123095154762, 0.027452878654003143]\n",
      "step 890 | loss 4.6742 | lr 2.00e-05 | gates=[0.08111410588026047, 0.9978026151657104] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9860028028488159, 0.011569170281291008, 0.002428015461191535]\n",
      "step 900 | loss 5.4744 | lr 2.00e-05 | gates=[0.013997185975313187, 0.9993026256561279] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9993975162506104, 0.00020788896654266864, 0.00039454505895264447]\n",
      "step 910 | loss 1.3355 | lr 2.00e-05 | gates=[0.0006024339818395674, 0.27235502004623413] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9982414841651917, 0.0007296854164451361, 0.0010289308847859502]\n",
      "step 920 | loss 1.3908 | lr 2.00e-05 | gates=[0.001758616417646408, 0.5297868251800537] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9753780364990234, 0.014760785736143589, 0.009861123748123646]\n",
      "step 930 | loss 4.7794 | lr 2.00e-05 | gates=[0.024621909484267235, 0.9990842938423157] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9996450543403625, 0.0001122611720347777, 0.00024274717725347728]\n",
      "step 940 | loss 1.2499 | lr 2.00e-05 | gates=[0.000355008349288255, 0.33015093207359314] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9996546506881714, 0.0001123342226492241, 0.00023297310690395534]\n",
      "step 950 | loss 1.6173 | lr 2.00e-05 | gates=[0.0003453073150012642, 0.4394815266132355] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9936285614967346, 0.00196914654225111, 0.004402283579111099]\n",
      "step 960 | loss 5.4592 | lr 2.00e-05 | gates=[0.006371429655700922, 0.9993207454681396] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9943505525588989, 0.001560410251840949, 0.004089014604687691]\n",
      "step 970 | loss 5.4764 | lr 2.00e-05 | gates=[0.0056494250893592834, 0.9996953010559082] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9998753666877747, 2.3270446035894565e-05, 0.00010137094795936719]\n",
      "step 980 | loss 1.3650 | lr 2.00e-05 | gates=[0.00012464140309020877, 0.23539896309375763] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9981071352958679, 0.0010774352122098207, 0.0008153971866704524]\n",
      "step 990 | loss 4.9993 | lr 2.00e-05 | gates=[0.0018928323406726122, 0.9991554617881775] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9903542399406433, 0.005928071215748787, 0.003717650892212987]\n",
      "step 1000 | loss 5.4923 | lr 2.00e-05 | gates=[0.009645720943808556, 0.9991958141326904] | mix=copy\n",
      "[after  E16a-s1337] alloc=32.07 GB | reserved=53.76 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=32.07 GB | reserved=32.14 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E16a (tier A) | seed=2027 ===\n",
      "[after free] alloc=32.07 GB | reserved=32.14 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E16a-s2027] alloc=32.07 GB | reserved=32.14 GB | free=0.00 GB | total=25.77 GB\n",
      "TB run started: ./runs\\dncformer-20250824-002307-E16a-s2027\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc48f2a519e649b788d4a83d1094ae89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.8297238945960999, 0.08063918352127075, 0.0896369218826294]\n",
      "step 10 | loss 8.6882 | lr 4.40e-05 | gates=[0.17027610540390015, 0.2963612675666809] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.39833226799964905, 0.13602319359779358, 0.4656445384025574]\n",
      "step 20 | loss 6.4253 | lr 8.40e-05 | gates=[0.6016677618026733, 0.2832692563533783] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.6478135585784912, 0.1297212541103363, 0.22246518731117249]\n",
      "step 30 | loss 5.6363 | lr 1.24e-04 | gates=[0.3521864414215088, 0.8580359220504761] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8499336838722229, 0.10212591290473938, 0.04794043302536011]\n",
      "step 40 | loss 6.1248 | lr 1.64e-04 | gates=[0.1500663459300995, 0.7026690244674683] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.5152212977409363, 0.3792222738265991, 0.1055564135313034]\n",
      "step 50 | loss 5.9980 | lr 2.00e-04 | gates=[0.4847787022590637, 0.961904764175415] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.6330140233039856, 0.03021349012851715, 0.33677244186401367]\n",
      "step 60 | loss 5.7236 | lr 2.00e-04 | gates=[0.3669859766960144, 0.7822335958480835] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.2995056211948395, 0.012554807588458061, 0.6879395842552185]\n",
      "step 70 | loss 6.3569 | lr 2.00e-04 | gates=[0.7004944086074829, 0.10139565169811249] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9928224682807922, 0.005147939547896385, 0.0020295782014727592]\n",
      "step 80 | loss 6.2068 | lr 1.99e-04 | gates=[0.007177518680691719, 0.5280345678329468] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.987169623374939, 0.011480741202831268, 0.0013496543979272246]\n",
      "step 90 | loss 5.4059 | lr 1.99e-04 | gates=[0.012830395251512527, 0.684557318687439] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8803398609161377, 0.10233256965875626, 0.01732761412858963]\n",
      "step 100 | loss 5.3160 | lr 1.99e-04 | gates=[0.11966019123792648, 0.5434665083885193] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9007025361061096, 0.08236263692378998, 0.01693480834364891]\n",
      "step 110 | loss 5.1291 | lr 1.98e-04 | gates=[0.09929744899272919, 0.8394817113876343] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9907402396202087, 0.007771545555442572, 0.0014881993411108851]\n",
      "step 120 | loss 5.6303 | lr 1.97e-04 | gates=[0.009259744547307491, 0.9684195518493652] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9907984137535095, 0.007521367631852627, 0.0016802053432911634]\n",
      "step 130 | loss 5.9280 | lr 1.96e-04 | gates=[0.00920157227665186, 0.8409066796302795] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9942861795425415, 0.00497684720903635, 0.0007370192906819284]\n",
      "step 140 | loss 6.1723 | lr 1.96e-04 | gates=[0.005713866092264652, 0.9269298315048218] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9346179366111755, 0.050850145518779755, 0.014531954191625118]\n",
      "step 150 | loss 5.5100 | lr 1.94e-04 | gates=[0.06538210064172745, 0.6755900382995605] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9858602285385132, 0.011956674046814442, 0.002183079021051526]\n",
      "step 160 | loss 5.5727 | lr 1.93e-04 | gates=[0.014139752835035324, 0.9978631734848022] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9286340475082397, 0.06233421340584755, 0.009031688794493675]\n",
      "step 170 | loss 5.4025 | lr 1.92e-04 | gates=[0.07136590778827667, 0.9985218644142151] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9589834213256836, 0.035134151577949524, 0.0058824289590120316]\n",
      "step 180 | loss 5.3065 | lr 1.91e-04 | gates=[0.041016578674316406, 0.998039722442627] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9895821809768677, 0.009647820144891739, 0.000769956037402153]\n",
      "step 190 | loss 5.6785 | lr 1.89e-04 | gates=[0.010417776182293892, 0.9966220855712891] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.910253643989563, 0.0802750438451767, 0.009471315890550613]\n",
      "step 200 | loss 5.3878 | lr 1.88e-04 | gates=[0.08974635601043701, 0.9732452630996704] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9875055551528931, 0.010149105452001095, 0.002345302142202854]\n",
      "step 210 | loss 5.6660 | lr 1.86e-04 | gates=[0.012494406662881374, 0.8366045951843262] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9624824523925781, 0.03188793361186981, 0.005629608873277903]\n",
      "step 220 | loss 5.8522 | lr 1.84e-04 | gates=[0.03751754015684128, 0.9735015630722046] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9391668438911438, 0.05089463293552399, 0.00993852224200964]\n",
      "step 230 | loss 5.0851 | lr 1.83e-04 | gates=[0.0608331635594368, 0.7568663954734802] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.7607600688934326, 0.20441654324531555, 0.03482340648770332]\n",
      "step 240 | loss 5.4081 | lr 1.81e-04 | gates=[0.23923993110656738, 0.9263932108879089] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9811921715736389, 0.014931948855519295, 0.003875898430123925]\n",
      "step 250 | loss 5.4943 | lr 1.79e-04 | gates=[0.018807847052812576, 0.9970270395278931] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9881501197814941, 0.0030846218578517437, 0.008765198290348053]\n",
      "step 260 | loss 5.8081 | lr 1.77e-04 | gates=[0.01184981968253851, 0.9684178829193115] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9994262456893921, 0.000102016914752312, 0.00047173770144581795]\n",
      "step 270 | loss 2.0307 | lr 1.74e-04 | gates=[0.0005737545434385538, 0.16605761647224426] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9721608757972717, 0.004042374901473522, 0.023796753957867622]\n",
      "step 280 | loss 5.4423 | lr 1.72e-04 | gates=[0.02783912606537342, 0.9387087821960449] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9717568755149841, 0.009146615862846375, 0.019096557050943375]\n",
      "step 290 | loss 5.1776 | lr 1.70e-04 | gates=[0.02824316918849945, 0.7813698053359985] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9978828430175781, 0.00032895966432988644, 0.0017882511019706726]\n",
      "step 300 | loss 5.0455 | lr 1.67e-04 | gates=[0.0021172105334699154, 0.9803684949874878] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9977267384529114, 0.00015132015687413514, 0.002121924189850688]\n",
      "step 310 | loss 5.3843 | lr 1.65e-04 | gates=[0.0022732443176209927, 0.9286326766014099] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9969021081924438, 4.5870528992963955e-05, 0.003051981795579195]\n",
      "step 320 | loss 5.9191 | lr 1.62e-04 | gates=[0.003097852459177375, 0.9682812690734863] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9935415387153625, 0.000938260811381042, 0.0055201915092766285]\n",
      "step 330 | loss 5.6372 | lr 1.60e-04 | gates=[0.006458451971411705, 0.782200038433075] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9947772026062012, 0.001033830689266324, 0.004188917577266693]\n",
      "step 340 | loss 6.5110 | lr 1.57e-04 | gates=[0.0052227480337023735, 0.49078404903411865] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9755857586860657, 0.001771345385350287, 0.022642895579338074]\n",
      "step 350 | loss 5.4233 | lr 1.54e-04 | gates=[0.024414241313934326, 0.9807690382003784] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9976754784584045, 7.981595990713686e-05, 0.002244699513539672]\n",
      "step 360 | loss 6.0853 | lr 1.52e-04 | gates=[0.0023245157208293676, 0.5538733005523682] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9721522331237793, 0.0037132189609110355, 0.024134507402777672]\n",
      "step 370 | loss 5.3871 | lr 1.49e-04 | gates=[0.02784772589802742, 0.989712119102478] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9970084428787231, 0.00036046584136784077, 0.0026311338879168034]\n",
      "step 380 | loss 1.9235 | lr 1.46e-04 | gates=[0.0029915994964540005, 0.027269408106803894] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9899139404296875, 0.00011722787166945636, 0.00996886845678091]\n",
      "step 390 | loss 5.5405 | lr 1.43e-04 | gates=[0.010086096823215485, 0.9473526477813721] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9989124536514282, 0.0001741693413350731, 0.0009133705752901733]\n",
      "step 400 | loss 5.3602 | lr 1.40e-04 | gates=[0.0010875400621443987, 0.9641746282577515] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9953218102455139, 0.0006649827118963003, 0.0040132212452590466]\n",
      "step 410 | loss 6.1305 | lr 1.37e-04 | gates=[0.0046782041899859905, 0.8247919082641602] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.998435378074646, 7.959581853356212e-05, 0.0014850193401798606]\n",
      "step 420 | loss 5.2231 | lr 1.34e-04 | gates=[0.0015646149404346943, 0.9705737829208374] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9969092607498169, 0.0006326142465695739, 0.002458119299262762]\n",
      "step 430 | loss 5.5452 | lr 1.31e-04 | gates=[0.003090732963755727, 0.9323890209197998] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9959659576416016, 6.90362139721401e-05, 0.003964982461184263]\n",
      "step 440 | loss 2.3416 | lr 1.27e-04 | gates=[0.004034018609672785, 0.5267036557197571] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9948787689208984, 0.00022473649005405605, 0.004896505735814571]\n",
      "step 450 | loss 5.1762 | lr 1.24e-04 | gates=[0.005121241789311171, 0.9791914820671082] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9866213798522949, 0.0027492481749504805, 0.010629329830408096]\n",
      "step 460 | loss 5.2121 | lr 1.21e-04 | gates=[0.01337857823818922, 0.539240837097168] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9800352454185486, 0.00021894632664043456, 0.0197458378970623]\n",
      "step 470 | loss 1.8985 | lr 1.18e-04 | gates=[0.019964782521128654, 0.3693741261959076] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9092456698417664, 0.00017625922919251025, 0.09057804942131042]\n",
      "step 480 | loss 2.3881 | lr 1.14e-04 | gates=[0.09075430780649185, 0.7323077917098999] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.991212010383606, 0.0005282614147290587, 0.008259723894298077]\n",
      "step 490 | loss 5.6977 | lr 1.11e-04 | gates=[0.008787984028458595, 0.6402372717857361] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997084736824036, 4.4785872887587175e-05, 0.00024676776956766844]\n",
      "step 500 | loss 4.9759 | lr 1.08e-04 | gates=[0.00029155361698940396, 0.9405118227005005] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9949352741241455, 0.0007982544484548271, 0.004266443662345409]\n",
      "step 510 | loss 5.2528 | lr 1.05e-04 | gates=[0.005064697004854679, 0.8819758892059326] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9969440698623657, 0.0006259642541408539, 0.002430038759484887]\n",
      "step 520 | loss 4.8869 | lr 1.01e-04 | gates=[0.003056003013625741, 0.9980146288871765] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9985740184783936, 6.860629218863323e-05, 0.0013573742471635342]\n",
      "step 530 | loss 1.7484 | lr 9.80e-05 | gates=[0.0014259804738685489, 0.2636577785015106] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9782446622848511, 0.001973527017980814, 0.01978178881108761]\n",
      "step 540 | loss 4.9023 | lr 9.47e-05 | gates=[0.021755315363407135, 0.7445224523544312] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9986975193023682, 0.00021652798750437796, 0.001086017000488937]\n",
      "step 550 | loss 1.8465 | lr 9.14e-05 | gates=[0.0013025449588894844, 0.2652118504047394] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.998609185218811, 8.427256398135796e-05, 0.0013064764207229018]\n",
      "step 560 | loss 1.6183 | lr 8.81e-05 | gates=[0.0013907490065321326, 0.15493308007717133] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.991451621055603, 0.001861437107436359, 0.006686930079013109]\n",
      "step 570 | loss 5.6938 | lr 8.48e-05 | gates=[0.008548366837203503, 0.7720934152603149] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9979377388954163, 8.392845484195277e-05, 0.001978254411369562]\n",
      "step 580 | loss 1.6516 | lr 8.16e-05 | gates=[0.0020621828734874725, 0.30172649025917053] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9796122312545776, 0.003128962591290474, 0.017258763313293457]\n",
      "step 590 | loss 5.5740 | lr 7.83e-05 | gates=[0.020387724041938782, 0.979532778263092] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9972153902053833, 6.899479194544256e-05, 0.0027156719006597996]\n",
      "step 600 | loss 1.6204 | lr 7.51e-05 | gates=[0.0027846668381243944, 0.2835797369480133] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.996437132358551, 0.00037245050771161914, 0.0031904089264571667]\n",
      "step 610 | loss 5.5063 | lr 7.19e-05 | gates=[0.0035628590267151594, 0.9464501142501831] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9987795352935791, 5.317066825227812e-05, 0.0011673065600916743]\n",
      "step 620 | loss 5.4862 | lr 6.88e-05 | gates=[0.0012204772792756557, 0.8170052170753479] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9957040548324585, 9.372588829137385e-05, 0.004202219191938639]\n",
      "step 630 | loss 1.5282 | lr 6.57e-05 | gates=[0.004295944701880217, 0.2945828139781952] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9956638813018799, 7.592224574182183e-05, 0.0042602429166436195]\n",
      "step 640 | loss 1.5202 | lr 6.26e-05 | gates=[0.004336164798587561, 0.13887396454811096] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9969359040260315, 0.0001100581866921857, 0.0029540539253503084]\n",
      "step 650 | loss 4.9228 | lr 5.95e-05 | gates=[0.0030641118064522743, 0.8800767064094543] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9517030715942383, 0.0043907081708312035, 0.043906234204769135]\n",
      "step 660 | loss 4.9768 | lr 5.65e-05 | gates=[0.04829694330692291, 0.8355166912078857] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9836104512214661, 7.743051537545398e-05, 0.016312135383486748]\n",
      "step 670 | loss 1.8398 | lr 5.36e-05 | gates=[0.016389567404985428, 0.2220180481672287] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9544628262519836, 0.0001368145749438554, 0.04540034383535385]\n",
      "step 680 | loss 1.3813 | lr 5.07e-05 | gates=[0.045537155121564865, 0.3012319803237915] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9937320947647095, 0.00012145289656473324, 0.0061464132741093636]\n",
      "step 690 | loss 5.4520 | lr 4.78e-05 | gates=[0.006267866585403681, 0.9981571435928345] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9745433330535889, 0.0035621756687760353, 0.02189451828598976]\n",
      "step 700 | loss 4.9584 | lr 4.50e-05 | gates=[0.02545669488608837, 0.9893721342086792] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9698556065559387, 0.00046597656910307705, 0.02967836707830429]\n",
      "step 710 | loss 1.1692 | lr 4.23e-05 | gates=[0.03014434315264225, 0.35280731320381165] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9472972750663757, 0.0005088276229798794, 0.052193980664014816]\n",
      "step 720 | loss 1.5643 | lr 3.96e-05 | gates=[0.05270281434059143, 0.19280394911766052] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9492887854576111, 0.0001425996597390622, 0.050568703562021255]\n",
      "step 730 | loss 1.4743 | lr 3.70e-05 | gates=[0.05071130022406578, 0.18775981664657593] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.994267463684082, 8.874548075255007e-05, 0.005643809679895639]\n",
      "step 740 | loss 4.9549 | lr 3.45e-05 | gates=[0.005732555873692036, 0.9195438623428345] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9866236448287964, 0.00028406595811247826, 0.013092271983623505]\n",
      "step 750 | loss 5.4552 | lr 3.20e-05 | gates=[0.01337633840739727, 0.9416656494140625] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9131978154182434, 0.0002152202359866351, 0.08658697456121445]\n",
      "step 760 | loss 1.6051 | lr 2.96e-05 | gates=[0.08680219203233719, 0.2483440339565277] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9983283281326294, 0.00028629356529563665, 0.0013853630516678095]\n",
      "step 770 | loss 4.9465 | lr 2.73e-05 | gates=[0.001671656733378768, 0.9221868515014648] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9191824793815613, 0.0004918509512208402, 0.08032563328742981]\n",
      "step 780 | loss 1.3921 | lr 2.51e-05 | gates=[0.08081747591495514, 0.2053389549255371] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9934370517730713, 0.001011570799164474, 0.0055514005944132805]\n",
      "step 790 | loss 5.5595 | lr 2.29e-05 | gates=[0.006562971509993076, 0.9556212425231934] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9980316162109375, 9.050835797097534e-05, 0.0018778680823743343]\n",
      "step 800 | loss 5.0349 | lr 2.09e-05 | gates=[0.0019683765713125467, 0.9104161858558655] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9960424900054932, 0.00047059025382623076, 0.0034869445953518152]\n",
      "step 810 | loss 4.9128 | lr 2.00e-05 | gates=[0.0039575351402163506, 0.8908692002296448] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9835983514785767, 0.002204240532591939, 0.014197410084307194]\n",
      "step 820 | loss 4.6659 | lr 2.00e-05 | gates=[0.01640164852142334, 0.8489241600036621] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9972968101501465, 0.00053329614456743, 0.0021698791533708572]\n",
      "step 830 | loss 4.9401 | lr 2.00e-05 | gates=[0.0027031751815229654, 0.9144068956375122] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9373560547828674, 0.00026621430879458785, 0.062377702444791794]\n",
      "step 840 | loss 1.4675 | lr 2.00e-05 | gates=[0.06264392286539078, 0.1813041716814041] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9157421588897705, 0.0003930184175260365, 0.08386484533548355]\n",
      "step 850 | loss 1.4770 | lr 2.00e-05 | gates=[0.08425785601139069, 0.20726139843463898] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9971622824668884, 0.00033043764415197074, 0.00250726705417037]\n",
      "step 860 | loss 4.8740 | lr 2.00e-05 | gates=[0.0028377047274261713, 0.9160608649253845] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9886305332183838, 0.00273829884827137, 0.00863123033195734]\n",
      "step 870 | loss 5.4463 | lr 2.00e-05 | gates=[0.01136952918022871, 0.9286519289016724] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.992389976978302, 0.0004891915014013648, 0.007120858877897263]\n",
      "step 880 | loss 5.4796 | lr 2.00e-05 | gates=[0.007610051427036524, 0.9284507632255554] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.8995017409324646, 0.0002797271008603275, 0.10021854192018509]\n",
      "step 890 | loss 1.4338 | lr 2.00e-05 | gates=[0.100498266518116, 0.3231234848499298] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9305620789527893, 0.00014348681725095958, 0.06929445266723633]\n",
      "step 900 | loss 1.7879 | lr 2.00e-05 | gates=[0.06943794339895248, 0.1990891993045807] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9985324144363403, 0.00015748578880447894, 0.0013101055519655347]\n",
      "step 910 | loss 4.9029 | lr 2.00e-05 | gates=[0.0014675912680104375, 0.9324707984924316] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.8690758347511292, 0.00018711322627495974, 0.1307370662689209]\n",
      "step 920 | loss 1.2222 | lr 2.00e-05 | gates=[0.13092418015003204, 0.30209237337112427] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9908456206321716, 9.045780461747199e-05, 0.009063925594091415]\n",
      "step 930 | loss 5.4869 | lr 2.00e-05 | gates=[0.009154383093118668, 0.854866087436676] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9385020732879639, 0.002632592339068651, 0.058865346014499664]\n",
      "step 940 | loss 4.9438 | lr 2.00e-05 | gates=[0.06149794161319733, 0.9115479588508606] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9444532990455627, 0.00012607745884452015, 0.05542059615254402]\n",
      "step 950 | loss 1.7091 | lr 2.00e-05 | gates=[0.05554667487740517, 0.1763182133436203] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9264518022537231, 0.00016775590484030545, 0.07338043302297592]\n",
      "step 960 | loss 1.4194 | lr 2.00e-05 | gates=[0.07354819029569626, 0.20731870830059052] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9960246682167053, 0.0001742976892273873, 0.003801027312874794]\n",
      "step 970 | loss 5.4326 | lr 2.00e-05 | gates=[0.003975324798375368, 0.8709344863891602] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9379850029945374, 0.00039679562905803323, 0.06161825358867645]\n",
      "step 980 | loss 1.3447 | lr 2.00e-05 | gates=[0.062015049159526825, 0.2158713936805725] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9959434866905212, 9.24317428143695e-05, 0.003964095376431942]\n",
      "step 990 | loss 4.8907 | lr 2.00e-05 | gates=[0.004056527279317379, 0.9821434617042542] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.9483526349067688, 0.0002068342437269166, 0.05144052952528]\n",
      "step 1000 | loss 1.4382 | lr 2.00e-05 | gates=[0.0516473650932312, 0.18748635053634644] | mix=hf\n",
      "[after  E16a-s2027] alloc=42.75 GB | reserved=64.54 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=42.75 GB | reserved=42.83 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E16a (tier A) | seed=4242 ===\n",
      "[after free] alloc=42.75 GB | reserved=42.83 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E16a-s4242] alloc=42.75 GB | reserved=42.83 GB | free=0.00 GB | total=25.77 GB\n",
      "TB run started: ./runs\\dncformer-20250824-042729-E16a-s4242\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7afe1dae438b42d5b273001e18ea25f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [experts] block0 top=0 pi=[0.7936932444572449, 0.06786225736141205, 0.13844448328018188]\n",
      "step 10 | loss 8.3066 | lr 4.40e-05 | gates=[0.20630674064159393, 0.41940373182296753] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.4397725462913513, 0.06371527910232544, 0.49651217460632324]\n",
      "step 20 | loss 7.2165 | lr 8.40e-05 | gates=[0.5602273941040039, 0.5967902541160583] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.29083216190338135, 0.10324284434318542, 0.6059249639511108]\n",
      "step 30 | loss 5.4364 | lr 1.24e-04 | gates=[0.7091678380966187, 0.18113945424556732] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.5081151127815247, 0.013232313096523285, 0.4786525368690491]\n",
      "step 40 | loss 5.9170 | lr 1.64e-04 | gates=[0.49188485741615295, 0.125884547829628] | mix=nback\n",
      "  [experts] block0 top=0 pi=[0.6729072332382202, 0.008411949500441551, 0.3186808228492737]\n",
      "step 50 | loss 6.1366 | lr 2.00e-04 | gates=[0.3270927667617798, 0.17755326628684998] | mix=nback\n",
      "  [experts] block0 top=2 pi=[0.4645317792892456, 0.028943758457899094, 0.5065244436264038]\n",
      "step 60 | loss 5.5262 | lr 2.00e-04 | gates=[0.5354682207107544, 0.36446642875671387] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.9621357917785645, 0.029848512262105942, 0.00801568478345871]\n",
      "step 70 | loss 8.3933 | lr 2.00e-04 | gates=[0.03786419704556465, 0.8807161450386047] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.4742377996444702, 0.4878028631210327, 0.03795934468507767]\n",
      "step 80 | loss 6.4182 | lr 1.99e-04 | gates=[0.5257622003555298, 0.36663109064102173] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8743901252746582, 0.09486132860183716, 0.030748559162020683]\n",
      "step 90 | loss 6.2421 | lr 1.99e-04 | gates=[0.125609889626503, 0.003170173382386565] | mix=nback\n",
      "  [experts] block0 top=2 pi=[0.002859288826584816, 0.1908949613571167, 0.806245744228363]\n",
      "step 100 | loss 5.7932 | lr 1.99e-04 | gates=[0.9971407651901245, 0.2550426721572876] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.000748840335290879, 0.014238867908716202, 0.9850122928619385]\n",
      "step 110 | loss 6.3388 | lr 1.98e-04 | gates=[0.999251127243042, 0.7891533374786377] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.0012314242776483297, 0.018818754702806473, 0.9799498319625854]\n",
      "step 120 | loss 6.0379 | lr 1.97e-04 | gates=[0.9987685680389404, 0.6521264314651489] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.0015730693703517318, 0.1810547113418579, 0.81737220287323]\n",
      "step 130 | loss 5.9554 | lr 1.96e-04 | gates=[0.9984269142150879, 0.009429918602108955] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.0028011882677674294, 0.7754546403884888, 0.22174414992332458]\n",
      "step 140 | loss 6.1681 | lr 1.96e-04 | gates=[0.9971988201141357, 0.00028010638197883964] | mix=repeat\n",
      "  [experts] block0 top=1 pi=[0.0010546103585511446, 0.6242702007293701, 0.3746751844882965]\n",
      "step 150 | loss 6.6869 | lr 1.94e-04 | gates=[0.9989453554153442, 1.11199333332479e-05] | mix=nback\n",
      "  [experts] block0 top=2 pi=[0.010235564783215523, 0.08865266293287277, 0.9011117815971375]\n",
      "step 160 | loss 5.5685 | lr 1.93e-04 | gates=[0.9897644519805908, 1.8708085917751305e-05] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.021762792021036148, 0.29754623770713806, 0.6806910037994385]\n",
      "step 170 | loss 5.7411 | lr 1.92e-04 | gates=[0.9782372117042542, 9.153674909612164e-06] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.012983491644263268, 0.37178534269332886, 0.6152311563491821]\n",
      "step 180 | loss 5.4849 | lr 1.91e-04 | gates=[0.9870165586471558, 5.8777699450729415e-05] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.09179550409317017, 0.02825224958360195, 0.8799522519111633]\n",
      "step 190 | loss 5.2611 | lr 1.89e-04 | gates=[0.9082044959068298, 3.1740040867589414e-05] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.18035702407360077, 0.08203002065420151, 0.7376129627227783]\n",
      "step 200 | loss 5.4567 | lr 1.88e-04 | gates=[0.819642961025238, 6.128090171841905e-05] | mix=repeat\n",
      "  [experts] block0 top=2 pi=[0.13523352146148682, 0.023654300719499588, 0.8411122560501099]\n",
      "step 210 | loss 5.8508 | lr 1.86e-04 | gates=[0.8647664785385132, 3.94192920794012e-06] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.16369810700416565, 0.5765479803085327, 0.259753942489624]\n",
      "step 220 | loss 6.1247 | lr 1.84e-04 | gates=[0.836301863193512, 2.210480670328252e-05] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.258482962846756, 0.6701326370239258, 0.07138442248106003]\n",
      "step 230 | loss 5.6228 | lr 1.83e-04 | gates=[0.7415170669555664, 0.00021933669631835073] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.18591374158859253, 0.6736654043197632, 0.1404208540916443]\n",
      "step 240 | loss 5.6641 | lr 1.81e-04 | gates=[0.8140862584114075, 3.0191946279956028e-05] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.0653967559337616, 0.05730036646127701, 0.877302885055542]\n",
      "step 250 | loss 5.6637 | lr 1.79e-04 | gates=[0.934603214263916, 2.3892713215900585e-05] | mix=copy\n",
      "  [experts] block0 top=2 pi=[0.33654868602752686, 0.01737324148416519, 0.6460781097412109]\n",
      "step 260 | loss 5.9875 | lr 1.77e-04 | gates=[0.6634513139724731, 0.00021291668235789984] | mix=repeat\n",
      "  [experts] block0 top=0 pi=[0.8046892285346985, 0.15991544723510742, 0.03539533168077469]\n",
      "step 270 | loss 2.9901 | lr 1.74e-04 | gates=[0.1953107863664627, 0.00022402626927942038] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9950017929077148, 0.002978271571919322, 0.002019911538809538]\n",
      "step 280 | loss 1.4688 | lr 1.72e-04 | gates=[0.0049981833435595036, 6.616682367166504e-05] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.9999006986618042, 2.570952165115159e-05, 7.351498061325401e-05]\n",
      "step 290 | loss 2.0923 | lr 1.70e-04 | gates=[9.922449680743739e-05, 0.00018106357310898602] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.99250727891922, 0.005043096840381622, 0.002449640305712819]\n",
      "step 300 | loss 9.9662 | lr 1.67e-04 | gates=[0.007492736913263798, 0.0007464638911187649] | mix=copy\n",
      "  [experts] block0 top=0 pi=[0.9997483491897583, 1.708595300442539e-05, 0.00023460030206479132]\n",
      "step 310 | loss 1.9463 | lr 1.65e-04 | gates=[0.000251686287811026, 0.00020767963724210858] | mix=hf\n",
      "  [experts] block0 top=0 pi=[0.888054609298706, 0.10729368031024933, 0.004651754163205624]\n",
      "step 320 | loss 6.5371 | lr 1.62e-04 | gates=[0.11194544285535812, 0.00028572481824085116] | mix=copy\n",
      "  [experts] block0 top=1 pi=[0.18165287375450134, 0.8088834285736084, 0.009463692083954811]\n",
      "step 330 | loss 6.2637 | lr 1.60e-04 | gates=[0.8183470964431763, 0.00020237884018570185] | mix=nback\n",
      "  [experts] block0 top=1 pi=[0.12404822558164597, 0.854201078414917, 0.02175069786608219]\n",
      "step 340 | loss 5.8628 | lr 1.57e-04 | gates=[0.8759517669677734, 0.0003286871942691505] | mix=copy\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[97], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mrun_e16_sweep\u001B[49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1337\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2027\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4242\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[93], line 60\u001B[0m, in \u001B[0;36mrun_e16_sweep\u001B[1;34m(steps, seeds)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m seeds:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== E16a (tier A) | seed=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 60\u001B[0m     results[(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE16a\u001B[39m\u001B[38;5;124m\"\u001B[39m, s)] \u001B[38;5;241m=\u001B[39m \u001B[43mrun_e16_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mE16a\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ma\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m seeds:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== E16b (tier B) | seed=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[93], line 39\u001B[0m, in \u001B[0;36mrun_e16_once\u001B[1;34m(label, steps, mode, seed)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TB_AVAILABLE \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m():\n\u001B[0;32m     30\u001B[0m     tb\u001B[38;5;241m.\u001B[39madd_text(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun/config/E16\u001B[39m\u001B[38;5;124m\"\u001B[39m, json\u001B[38;5;241m.\u001B[39mdumps({\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: label, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m: mode, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m: steps, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmem_experts\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mgetattr\u001B[39m(CFG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmem_experts\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiversity_lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mgetattr\u001B[39m(CFG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpert_diversity_lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0.0\u001B[39m),\n\u001B[0;32m     37\u001B[0m     }, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m), \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 39\u001B[0m head, tok \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_experiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarmup_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmixture_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# baseline mix; E16 sets schedules on CFG\u001B[39;49;00m\n\u001B[0;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmixture_schedule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmixture_schedule\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgate_temp_schedule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgate_temp_schedule\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgate_reg_schedule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgate_reg_schedule\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mviz_memory_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda_report\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m(): cuda_report(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mafter  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-s\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseed\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     50\u001B[0m free_head_and_cache()\n",
      "Cell \u001B[1;32mIn[88], line 37\u001B[0m, in \u001B[0;36mtrain_experiment\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(CFG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgate_temp\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m CFG\u001B[38;5;241m.\u001B[39mgate_temp \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     36\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[guard] CFG.gate_temp <= 0; routing may degenerate.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _orig_train_experiment(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[86], line 87\u001B[0m, in \u001B[0;36mtrain_experiment\u001B[1;34m(steps, batch_size, warmup_steps, min_lr_ratio, mixture_weights, hf_dataset, hf_max_items, log_every, viz_memory_after, viz_prompt, viz_max_T, mixture_schedule, gate_temp_schedule, gate_reg_schedule)\u001B[0m\n\u001B[0;32m     85\u001B[0m in_ids \u001B[38;5;241m=\u001B[39m mixer(batch_size)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mamp_dtype, enabled\u001B[38;5;241m=\u001B[39m(amp_dtype \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat32)):\n\u001B[1;32m---> 87\u001B[0m     logits, gates, aux \u001B[38;5;241m=\u001B[39m \u001B[43mhead\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_with_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgate_override\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforce_g\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m     loss \u001B[38;5;241m=\u001B[39m lm_shift_labels(in_ids, logits, tok)\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;66;03m# Optional: encourage expert usage diversity (higher entropy across (vanilla + K experts))\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[76], line 70\u001B[0m, in \u001B[0;36mDNCFormerHead.forward_with_metrics\u001B[1;34m(self, input_ids, attention_mask, gate_override)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m K \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(st_in, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m     69\u001B[0m     st_in \u001B[38;5;241m=\u001B[39m [st_in] \u001B[38;5;241m*\u001B[39m K\n\u001B[1;32m---> 70\u001B[0m h, dnc_states[i], g \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdnc_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mst_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgate_override\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgate_override\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m gates_raw\u001B[38;5;241m.\u001B[39mappend(g)\n\u001B[0;32m     72\u001B[0m gates_det\u001B[38;5;241m.\u001B[39mappend(g\u001B[38;5;241m.\u001B[39mdetach())\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[87], line 27\u001B[0m, in \u001B[0;36m_peb_forward_multi\u001B[1;34m(self, x, dnc_state, gate_override)\u001B[0m\n\u001B[0;32m     24\u001B[0m last_read_feat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# (B,T,W) if provided by DNCformerBlock\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m, st \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdncblocks, states_in):\n\u001B[1;32m---> 27\u001B[0m     dt, st2 \u001B[38;5;241m=\u001B[39m \u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mst\u001B[49m\u001B[43m)\u001B[49m          \u001B[38;5;66;03m# compute with original x (native path)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dt\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m dtype_v:\n\u001B[0;32m     29\u001B[0m         dt \u001B[38;5;241m=\u001B[39m dt\u001B[38;5;241m.\u001B[39mto(dtype_v)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[74], line 77\u001B[0m, in \u001B[0;36mDNCformerBlock.forward\u001B[1;34m(self, x, state)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(T):\n\u001B[0;32m     76\u001B[0m     x_if \u001B[38;5;241m=\u001B[39m {k: v[:,t] \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m iface\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m---> 77\u001B[0m     r_t, new_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_if\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_state\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# r_t: (B,R,W)\u001B[39;00m\n\u001B[0;32m     78\u001B[0m     r_list\u001B[38;5;241m.\u001B[39mappend(r_t)\n\u001B[0;32m     80\u001B[0m Rseq \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(r_list, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (B,T,R,W)\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[72], line 72\u001B[0m, in \u001B[0;36mDNCMemory.forward\u001B[1;34m(self, x_if, state)\u001B[0m\n\u001B[0;32m     68\u001B[0m B, N, W \u001B[38;5;241m=\u001B[39m M\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# --- Usage update (faithful, stable) ---\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# previous write weights; keep as (B,1,N) for easy broadcasting\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m ww_prev \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mww\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# writes increase usage\u001B[39;00m\n\u001B[0;32m     74\u001B[0m u \u001B[38;5;241m=\u001B[39m u \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m u) \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m torch\u001B[38;5;241m.\u001B[39mprod(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m ww_prev, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))   \u001B[38;5;66;03m# -> (B,N)\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "_ = run_e16_sweep(steps=1000, seeds=(1337, 2027, 4242))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T12:38:47.612058100Z",
     "start_time": "2025-08-24T05:34:57.484386400Z"
    }
   },
   "id": "38f8ae2f0d0950b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 00. Misc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a94ff038fb1b7b65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensorboard log dump"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c869b656957faac2"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard version: 2.20.0\n",
      "Discovered 3 run(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": "        label                                run_id  n_event_files  \\\n0  E16a-s1337  dncformer-20250823-223458-E16a-s1337              1   \n1  E16a-s2027  dncformer-20250824-002307-E16a-s2027              1   \n2  E16a-s4242  dncformer-20250824-042729-E16a-s4242              1   \n\n   steps_logged  loss_start~5  loss_end~10  loss_delta   lr_last  \\\n0          1000      6.411446     3.316509    3.094937  0.000020   \n1          1000      6.574523     3.279030    3.295493  0.000020   \n2           340      6.602650     4.877860    1.724789  0.000157   \n\n   haystack_acc_last  haystack_loss_last  ... gmean_b1_copy  gmean_b0_repeat  \\\n0                NaN                 NaN  ...      0.999302         0.035101   \n1                NaN                 NaN  ...      0.907705         0.034682   \n2                NaN                 NaN  ...      0.000307         0.797100   \n\n   gmean_b1_repeat  gmean_b0_nback  gmean_b1_nback  g_b0_Q1_mean  \\\n0         0.998633        0.003567        0.999494      0.040112   \n1         0.865977        0.002666        0.924079      0.050180   \n2         0.000102        0.779932        0.000211      0.632134   \n\n   g_b0_Q2_mean  g_b0_Q3_mean  g_b0_Q4_mean  \\\n0      0.003146      0.001473      0.000781   \n1      0.019344      0.022893      0.025952   \n2      0.648412      0.652287      0.650466   \n\n                                     run_dir  \n0  runs\\dncformer-20250823-223458-E16a-s1337  \n1  runs\\dncformer-20250824-002307-E16a-s2027  \n2  runs\\dncformer-20250824-042729-E16a-s4242  \n\n[3 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>run_id</th>\n      <th>n_event_files</th>\n      <th>steps_logged</th>\n      <th>loss_start~5</th>\n      <th>loss_end~10</th>\n      <th>loss_delta</th>\n      <th>lr_last</th>\n      <th>haystack_acc_last</th>\n      <th>haystack_loss_last</th>\n      <th>...</th>\n      <th>gmean_b1_copy</th>\n      <th>gmean_b0_repeat</th>\n      <th>gmean_b1_repeat</th>\n      <th>gmean_b0_nback</th>\n      <th>gmean_b1_nback</th>\n      <th>g_b0_Q1_mean</th>\n      <th>g_b0_Q2_mean</th>\n      <th>g_b0_Q3_mean</th>\n      <th>g_b0_Q4_mean</th>\n      <th>run_dir</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>1</td>\n      <td>1000</td>\n      <td>6.411446</td>\n      <td>3.316509</td>\n      <td>3.094937</td>\n      <td>0.000020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.999302</td>\n      <td>0.035101</td>\n      <td>0.998633</td>\n      <td>0.003567</td>\n      <td>0.999494</td>\n      <td>0.040112</td>\n      <td>0.003146</td>\n      <td>0.001473</td>\n      <td>0.000781</td>\n      <td>runs\\dncformer-20250823-223458-E16a-s1337</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E16a-s2027</td>\n      <td>dncformer-20250824-002307-E16a-s2027</td>\n      <td>1</td>\n      <td>1000</td>\n      <td>6.574523</td>\n      <td>3.279030</td>\n      <td>3.295493</td>\n      <td>0.000020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.907705</td>\n      <td>0.034682</td>\n      <td>0.865977</td>\n      <td>0.002666</td>\n      <td>0.924079</td>\n      <td>0.050180</td>\n      <td>0.019344</td>\n      <td>0.022893</td>\n      <td>0.025952</td>\n      <td>runs\\dncformer-20250824-002307-E16a-s2027</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E16a-s4242</td>\n      <td>dncformer-20250824-042729-E16a-s4242</td>\n      <td>1</td>\n      <td>340</td>\n      <td>6.602650</td>\n      <td>4.877860</td>\n      <td>1.724789</td>\n      <td>0.000157</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.000307</td>\n      <td>0.797100</td>\n      <td>0.000102</td>\n      <td>0.779932</td>\n      <td>0.000211</td>\n      <td>0.632134</td>\n      <td>0.648412</td>\n      <td>0.652287</td>\n      <td>0.650466</td>\n      <td>runs\\dncformer-20250824-042729-E16a-s4242</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: analysis\\run_level_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "         label                                run_id  task  block  step  \\\n0   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0    20   \n1   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1    20   \n2   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0    50   \n3   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1    50   \n4   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   110   \n5   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   110   \n6   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   130   \n7   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   130   \n8   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   140   \n9   E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   140   \n10  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   150   \n11  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   150   \n12  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   170   \n13  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   170   \n14  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   180   \n15  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   180   \n16  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   200   \n17  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   200   \n18  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      0   210   \n19  E16a-s1337  dncformer-20250823-223458-E16a-s1337  copy      1   210   \n\n        loss    g_mean  g_frac>0.5        lr   g_b0_Q1   g_b0_Q2   g_b0_Q3  \\\n0   7.072079  0.596814    0.953125  0.000084  0.633502  0.588711  0.582837   \n1   7.072079  0.653858    0.997070  0.000084       NaN       NaN       NaN   \n2   5.868248  0.015435    0.014648  0.000200  0.059879  0.000742  0.000558   \n3   5.868248  0.814529    1.000000  0.000200       NaN       NaN       NaN   \n4   5.652671  0.026108    0.023438  0.000198  0.101993  0.000887  0.000785   \n5   5.652671  0.999999    1.000000  0.000198       NaN       NaN       NaN   \n6   5.628609  0.022214    0.022461  0.000196  0.085761  0.001084  0.001011   \n7   5.628609  0.999998    1.000000  0.000196       NaN       NaN       NaN   \n8   6.160719  0.020526    0.019531  0.000196  0.077643  0.001594  0.001462   \n9   6.160719  1.000000    1.000000  0.000196       NaN       NaN       NaN   \n10  5.638692  0.017590    0.017578  0.000194  0.065738  0.001716  0.001430   \n11  5.638692  0.999999    1.000000  0.000194       NaN       NaN       NaN   \n12  5.775231  0.010632    0.008789  0.000192  0.040460  0.000787  0.000645   \n13  5.775231  0.999999    1.000000  0.000192       NaN       NaN       NaN   \n14  5.507951  0.016628    0.017578  0.000191  0.064702  0.000624  0.000591   \n15  5.507951  0.999999    1.000000  0.000191       NaN       NaN       NaN   \n16  5.648383  0.009864    0.009766  0.000188  0.037588  0.000744  0.000575   \n17  5.648383  0.999999    1.000000  0.000188       NaN       NaN       NaN   \n18  5.538263  0.012133    0.006836  0.000186  0.045915  0.001080  0.000986   \n19  5.538263  1.000000    1.000000  0.000186       NaN       NaN       NaN   \n\n     g_b0_Q4  \n0   0.582207  \n1        NaN  \n2   0.000561  \n3        NaN  \n4   0.000766  \n5        NaN  \n6   0.001001  \n7        NaN  \n8   0.001406  \n9        NaN  \n10  0.001475  \n11       NaN  \n12  0.000634  \n13       NaN  \n14  0.000594  \n15       NaN  \n16  0.000548  \n17       NaN  \n18  0.000553  \n19       NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>run_id</th>\n      <th>task</th>\n      <th>block</th>\n      <th>step</th>\n      <th>loss</th>\n      <th>g_mean</th>\n      <th>g_frac&gt;0.5</th>\n      <th>lr</th>\n      <th>g_b0_Q1</th>\n      <th>g_b0_Q2</th>\n      <th>g_b0_Q3</th>\n      <th>g_b0_Q4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>20</td>\n      <td>7.072079</td>\n      <td>0.596814</td>\n      <td>0.953125</td>\n      <td>0.000084</td>\n      <td>0.633502</td>\n      <td>0.588711</td>\n      <td>0.582837</td>\n      <td>0.582207</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>20</td>\n      <td>7.072079</td>\n      <td>0.653858</td>\n      <td>0.997070</td>\n      <td>0.000084</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>50</td>\n      <td>5.868248</td>\n      <td>0.015435</td>\n      <td>0.014648</td>\n      <td>0.000200</td>\n      <td>0.059879</td>\n      <td>0.000742</td>\n      <td>0.000558</td>\n      <td>0.000561</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>50</td>\n      <td>5.868248</td>\n      <td>0.814529</td>\n      <td>1.000000</td>\n      <td>0.000200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>110</td>\n      <td>5.652671</td>\n      <td>0.026108</td>\n      <td>0.023438</td>\n      <td>0.000198</td>\n      <td>0.101993</td>\n      <td>0.000887</td>\n      <td>0.000785</td>\n      <td>0.000766</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>110</td>\n      <td>5.652671</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>0.000198</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>130</td>\n      <td>5.628609</td>\n      <td>0.022214</td>\n      <td>0.022461</td>\n      <td>0.000196</td>\n      <td>0.085761</td>\n      <td>0.001084</td>\n      <td>0.001011</td>\n      <td>0.001001</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>130</td>\n      <td>5.628609</td>\n      <td>0.999998</td>\n      <td>1.000000</td>\n      <td>0.000196</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>140</td>\n      <td>6.160719</td>\n      <td>0.020526</td>\n      <td>0.019531</td>\n      <td>0.000196</td>\n      <td>0.077643</td>\n      <td>0.001594</td>\n      <td>0.001462</td>\n      <td>0.001406</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>140</td>\n      <td>6.160719</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000196</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>150</td>\n      <td>5.638692</td>\n      <td>0.017590</td>\n      <td>0.017578</td>\n      <td>0.000194</td>\n      <td>0.065738</td>\n      <td>0.001716</td>\n      <td>0.001430</td>\n      <td>0.001475</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>150</td>\n      <td>5.638692</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>0.000194</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>170</td>\n      <td>5.775231</td>\n      <td>0.010632</td>\n      <td>0.008789</td>\n      <td>0.000192</td>\n      <td>0.040460</td>\n      <td>0.000787</td>\n      <td>0.000645</td>\n      <td>0.000634</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>170</td>\n      <td>5.775231</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>0.000192</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>180</td>\n      <td>5.507951</td>\n      <td>0.016628</td>\n      <td>0.017578</td>\n      <td>0.000191</td>\n      <td>0.064702</td>\n      <td>0.000624</td>\n      <td>0.000591</td>\n      <td>0.000594</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>180</td>\n      <td>5.507951</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>0.000191</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>200</td>\n      <td>5.648383</td>\n      <td>0.009864</td>\n      <td>0.009766</td>\n      <td>0.000188</td>\n      <td>0.037588</td>\n      <td>0.000744</td>\n      <td>0.000575</td>\n      <td>0.000548</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>200</td>\n      <td>5.648383</td>\n      <td>0.999999</td>\n      <td>1.000000</td>\n      <td>0.000188</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>210</td>\n      <td>5.538263</td>\n      <td>0.012133</td>\n      <td>0.006836</td>\n      <td>0.000186</td>\n      <td>0.045915</td>\n      <td>0.001080</td>\n      <td>0.000986</td>\n      <td>0.000553</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>E16a-s1337</td>\n      <td>dncformer-20250823-223458-E16a-s1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>210</td>\n      <td>5.538263</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000186</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: analysis\\per_task_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== TensorBoard event analyzer for DNCFormer runs (robust) =====\n",
    "# Discovers TB runs, merges multiple event files per run, summarizes, and exports granular CSVs.\n",
    "\n",
    "import os, re, math, json, time, glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- TensorBoard imports (version-agnostic handling) ---\n",
    "try:\n",
    "    import tensorboard as _tb\n",
    "    print(\"TensorBoard version:\", getattr(_tb, \"__version__\", \"unknown\"))\n",
    "except Exception as _e:\n",
    "    print(\"TensorBoard import note:\", _e)\n",
    "\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. Install via: pip install tensorboard\") from e\n",
    "\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _size_guidance_version_safe():\n",
    "    \"\"\"Build size_guidance dict usable across TB versions.\"\"\"\n",
    "    sg = {}\n",
    "    keys = [\"SCALARS\", \"HISTOGRAMS\", \"IMAGES\", \"COMPRESSED_HISTOGRAMS\", \"AUDIO\", \"TENSORS\"]\n",
    "    for k in keys:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "\n",
    "def _infer_label_from_run_dir(run_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Expect 'dncformer-YYYYMMDD-HHMMSS-<LABEL>' or just 'dncformer-YYYYMMDD-HHMMSS'.\n",
    "    Returns <LABEL> if present, else the directory name.\n",
    "    \"\"\"\n",
    "    name = run_dir.name\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return name\n",
    "\n",
    "\n",
    "def _load_scalars_from_event_file(ev_path: str) -> dict:\n",
    "    \"\"\"Load scalars from a single event file: tag -> list[(step, value)].\"\"\"\n",
    "    acc = EventAccumulator(ev_path, size_guidance=_size_guidance_version_safe())\n",
    "    acc.Reload()\n",
    "    tags = acc.Tags().get('scalars', []) or []\n",
    "    out = {}\n",
    "    for tag in tags:\n",
    "        vals = acc.Scalars(tag)\n",
    "        out[tag] = [(int(x.step), float(x.value)) for x in vals]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _merge_scalar_dicts(list_of_scalar_dicts):\n",
    "    \"\"\"\n",
    "    Merge multiple event files for the same run.\n",
    "    For each tag: keep the last value seen per (step), then return sorted by step.\n",
    "    \"\"\"\n",
    "    merged = defaultdict(dict)  # tag -> {step: value}\n",
    "    for scal in list_of_scalar_dicts:\n",
    "        for tag, series in scal.items():\n",
    "            d = merged[tag]\n",
    "            for step, val in series:\n",
    "                d[step] = val  # 'last wins' is fine; event files are append-only per run\n",
    "    # Convert to tag -> sorted list[(step, value)]\n",
    "    out = {}\n",
    "    for tag, d in merged.items():\n",
    "        steps_sorted = sorted(d.keys())\n",
    "        out[tag] = [(s, d[s]) for s in steps_sorted]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _detect_tasks_and_blocks(scalars: dict):\n",
    "    \"\"\"Detect task names and block ids from present tags. Returns (sorted_tasks, sorted_blocks).\"\"\"\n",
    "    tasks = set()\n",
    "    blocks = set()\n",
    "\n",
    "    # tasks from \"loss_by_task/<task>\"\n",
    "    for tag in scalars.keys():\n",
    "        if tag.startswith(\"loss_by_task/\"):\n",
    "            tasks.add(tag.split(\"/\", 1)[1])\n",
    "\n",
    "    # blocks from \"gates_by_task/block_<b>_mean/<task>\" or \"gates/block_<b>_mean\"\n",
    "    for tag in scalars.keys():\n",
    "        m = re.match(r\"gates_by_task/block_(\\d+)_\", tag)\n",
    "        if m:\n",
    "            blocks.add(int(m.group(1)))\n",
    "        m2 = re.match(r\"gates/block_(\\d+)_mean$\", tag)\n",
    "        if m2:\n",
    "            blocks.add(int(m2.group(1)))\n",
    "\n",
    "    # sensible defaults if nothing is found\n",
    "    if not tasks:\n",
    "        tasks = {\"hf\", \"copy\", \"repeat\", \"nback\"}\n",
    "    if not blocks:\n",
    "        blocks = {0, 1}\n",
    "\n",
    "    return sorted(tasks), sorted(blocks)\n",
    "\n",
    "\n",
    "def s_last(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[-1])\n",
    "    return float(np.nanmean(arr[-k:]))\n",
    "\n",
    "\n",
    "def s_first(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[0])\n",
    "    return float(np.nanmean(arr[:k]))\n",
    "\n",
    "\n",
    "def s_mean(vals):\n",
    "    if not vals: return np.nan\n",
    "    return float(np.nanmean([v for _, v in vals]))\n",
    "\n",
    "\n",
    "def s_count(vals):\n",
    "    return len(vals) if vals else 0\n",
    "\n",
    "\n",
    "# ---------------- discover runs ----------------\n",
    "# Option 1: auto-discover all event files and group by their parent directory\n",
    "FOUND_EVENTS = sorted(glob.glob(\"runs/**/events.out.tfevents.*\", recursive=True))\n",
    "\n",
    "# Option 2: pin a subset manually\n",
    "FOUND_EVENTS = [\n",
    "    r\"runs/dncformer-20250823-223458-E16a-s1337/events.out.tfevents.1756013698.Persephone.25868.16\",\n",
    "    r\"runs/dncformer-20250824-002307-E16a-s2027/events.out.tfevents.1756020187.Persephone.25868.17\",\n",
    "    r\"runs/dncformer-20250824-042729-E16a-s4242/events.out.tfevents.1756034849.Persephone.25868.18\",\n",
    "]\n",
    "\n",
    "assert FOUND_EVENTS, \"No event files found under ./runs. Have you executed any experiments?\"\n",
    "\n",
    "# Group event files by run directory\n",
    "run_groups = defaultdict(list)   # run_dir_path -> [event_file_paths]\n",
    "for p in FOUND_EVENTS:\n",
    "    run_groups[str(Path(p).parent)].append(p)\n",
    "\n",
    "print(f\"Discovered {len(run_groups)} run(s).\")\n",
    "\n",
    "\n",
    "# ---------------- summarize each run ----------------\n",
    "run_summaries = []\n",
    "per_run_scalars = {}   # run_dir_name -> merged scalars dict\n",
    "\n",
    "for run_dir, files in sorted(run_groups.items()):\n",
    "    run_dir_path = Path(run_dir)\n",
    "    label = _infer_label_from_run_dir(run_dir_path)\n",
    "    run_id = run_dir_path.name\n",
    "\n",
    "    try:\n",
    "        # load and merge all files for this run\n",
    "        scal_dicts = [_load_scalars_from_event_file(f) for f in sorted(files)]\n",
    "        scal = _merge_scalar_dicts(scal_dicts)\n",
    "        per_run_scalars[run_dir_path.name] = scal\n",
    "\n",
    "        # detect tasks and blocks present\n",
    "        TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "        # basics\n",
    "        loss_series = scal.get(\"train/loss\", [])\n",
    "        lr_series   = scal.get(\"train/lr\", [])\n",
    "        steps_logged = max([s for s, _ in loss_series], default=np.nan) if loss_series else np.nan\n",
    "        loss0   = s_first(loss_series, k=5)\n",
    "        lossT   = s_last(loss_series,  k=10)\n",
    "        ldelta  = (loss0 - lossT) if not any(map(math.isnan, [loss0, lossT])) else np.nan\n",
    "        lr_last = s_last(lr_series, k=1)\n",
    "\n",
    "        # gates (global)\n",
    "        g_means, g_entropy, g_frac_avg = {}, {}, {}\n",
    "        for b in BLOCKS:\n",
    "            g_means[b]   = s_last(scal.get(f\"gates/block_{b}_mean\", []), k=10)\n",
    "            g_entropy[b] = s_last(scal.get(f\"gates/block_{b}_entropy\", []), k=10)\n",
    "            fracs = []\n",
    "            for t in TASKS:\n",
    "                tag = f\"gates_by_task/block_{b}_frac>0.5/{t}\"\n",
    "                if tag in scal:\n",
    "                    fracs.append(s_last(scal[tag], k=10))\n",
    "            g_frac_avg[b] = float(np.nanmean(fracs)) if fracs else np.nan\n",
    "\n",
    "        # per-task losses (mean of last quarter of points for that task)\n",
    "        task_loss_last, task_counts = {}, {}\n",
    "        for t in TASKS:\n",
    "            ts = scal.get(f\"loss_by_task/{t}\", [])\n",
    "            task_counts[t] = s_count(ts)\n",
    "            if ts:\n",
    "                k = max(1, len(ts)//4)\n",
    "                task_loss_last[t] = s_last(ts, k=k)\n",
    "            else:\n",
    "                task_loss_last[t] = np.nan\n",
    "\n",
    "        # per-task gate means (avg last quarter)\n",
    "        task_gmeans = {t: {} for t in TASKS}\n",
    "        for t in TASKS:\n",
    "            for b in BLOCKS:\n",
    "                ts = scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])\n",
    "                if ts:\n",
    "                    k = max(1, len(ts)//4)\n",
    "                    task_gmeans[t][b] = s_last(ts, k=k)\n",
    "                else:\n",
    "                    task_gmeans[t][b] = np.nan\n",
    "\n",
    "        # quartiles (block0) if present\n",
    "        q_means = {}\n",
    "        for qi in range(1, 5):\n",
    "            vals = []\n",
    "            # May be logged globally or by task; check both\n",
    "            tag_global = f\"gates/block0_q{qi}_mean\"\n",
    "            if tag_global in scal:\n",
    "                vals.append(s_last(scal[tag_global], k=10))\n",
    "            else:\n",
    "                for t in TASKS:\n",
    "                    tag_task = f\"gates/block0_q{qi}_mean/{t}\"\n",
    "                    if tag_task in scal:\n",
    "                        vals.append(s_last(scal[tag_task], k=10))\n",
    "            q_means[qi] = float(np.nanmean(vals)) if vals else np.nan\n",
    "\n",
    "        # haystack eval if present\n",
    "        hay_acc  = s_last(scal.get(\"eval/haystack_acc\",  []), k=1)\n",
    "        hay_loss = s_last(scal.get(\"eval/haystack_loss\", []), k=1)\n",
    "\n",
    "        # forced-g guess heuristic\n",
    "        forced_guess = None\n",
    "        gm_all = [g_means[b] for b in g_means if not math.isnan(g_means[b])]\n",
    "        if gm_all:\n",
    "            m = float(np.nanmean(gm_all))\n",
    "            if m < 0.02:   forced_guess = \"force_g=0\"\n",
    "            elif m > 0.98: forced_guess = \"force_g=1\"\n",
    "\n",
    "        summary = {\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"run_dir\": str(run_dir_path),\n",
    "            \"n_event_files\": len(files),\n",
    "            \"steps_logged\": steps_logged,\n",
    "            \"loss_start~5\": loss0,\n",
    "            \"loss_end~10\":  lossT,\n",
    "            \"loss_delta\":   ldelta,\n",
    "            \"lr_last\":      lr_last,\n",
    "            \"haystack_acc_last\":  hay_acc,\n",
    "            \"haystack_loss_last\": hay_loss,\n",
    "            \"forced_guess\": forced_guess,\n",
    "        }\n",
    "\n",
    "        # flatten gate summaries\n",
    "        for b in BLOCKS:\n",
    "            summary[f\"g_mean_b{b}\"]     = g_means.get(b, np.nan)\n",
    "            summary[f\"g_entropy_b{b}\"]  = g_entropy.get(b, np.nan)\n",
    "            summary[f\"g_frac>0.5_b{b}\"] = g_frac_avg.get(b, np.nan)\n",
    "\n",
    "        # flatten per-task last losses and per-task mean gates\n",
    "        for t in TASKS:\n",
    "            summary[f\"loss_{t}_last\"] = task_loss_last[t]\n",
    "            for b in BLOCKS:\n",
    "                summary[f\"gmean_b{b}_{t}\"] = task_gmeans[t][b]\n",
    "\n",
    "        # quartiles\n",
    "        for qi in range(1, 5):\n",
    "            summary[f\"g_b0_Q{qi}_mean\"] = q_means[qi]\n",
    "\n",
    "        run_summaries.append(summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[analyzer] Skipped run {run_dir}: {e}\")\n",
    "\n",
    "# -------- assemble run-level summary --------\n",
    "df_runs = pd.DataFrame(run_summaries)\n",
    "if df_runs.empty:\n",
    "    print(\"No runs summarized.\")\n",
    "else:\n",
    "    # Sort by timestamp embedded in run_id if possible\n",
    "    def _ts_key(name: str):\n",
    "        m = re.search(r\"(\\d{8})-(\\d{6})\", name or \"\")\n",
    "        return (m.group(1), m.group(2)) if m else (\"\", \"\")\n",
    "    df_runs = df_runs.sort_values(by=[\"run_id\"], key=lambda s: s.map(_ts_key), ignore_index=True)\n",
    "\n",
    "    # Arrange prominent columns\n",
    "    front_cols = [c for c in [\n",
    "        \"label\",\"run_id\",\"n_event_files\",\"steps_logged\",\n",
    "        \"loss_start~5\",\"loss_end~10\",\"loss_delta\",\"lr_last\",\n",
    "        \"haystack_acc_last\",\"haystack_loss_last\",\"forced_guess\",\n",
    "        \"g_mean_b0\",\"g_mean_b1\",\"g_entropy_b0\",\"g_entropy_b1\",\n",
    "        \"g_frac>0.5_b0\",\"g_frac>0.5_b1\",\n",
    "        \"loss_hf_last\",\"loss_copy_last\",\"loss_repeat_last\",\"loss_nback_last\",\n",
    "        \"gmean_b0_hf\",\"gmean_b1_hf\",\"gmean_b0_copy\",\"gmean_b1_copy\",\n",
    "        \"gmean_b0_repeat\",\"gmean_b1_repeat\",\"gmean_b0_nback\",\"gmean_b1_nback\",\n",
    "        \"g_b0_Q1_mean\",\"g_b0_Q2_mean\",\"g_b0_Q3_mean\",\"g_b0_Q4_mean\",\n",
    "        \"run_dir\"\n",
    "    ] if c in df_runs.columns]\n",
    "    df_runs = df_runs[[*front_cols, *[c for c in df_runs.columns if c not in front_cols]]]\n",
    "\n",
    "    display(df_runs)\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_runs.to_csv(out_dir / \"run_level_summary.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"run_level_summary.csv\")\n",
    "\n",
    "\n",
    "# -------- granular per-task time series export --------\n",
    "rows = []\n",
    "for run_dir_name, scal in per_run_scalars.items():\n",
    "    label = _infer_label_from_run_dir(Path(run_dir_name))\n",
    "    # detect tasks/blocks present in this run\n",
    "    TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "    # pick up LR series for convenient join\n",
    "    lr_by_step = {int(s): float(v) for s, v in scal.get(\"train/lr\", [])}\n",
    "\n",
    "    # per-task loss and per-task gate metrics\n",
    "    for t in TASKS:\n",
    "        loss_series = scal.get(f\"loss_by_task/{t}\", [])\n",
    "        if not loss_series:\n",
    "            continue\n",
    "\n",
    "        gmean_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "        gfrac_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_frac>0.5/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "\n",
    "        # Optional quartile logs (block 0)\n",
    "        q_by_step = {qi: {int(s): float(v) for s, v in scal.get(f\"gates/block0_q{qi}_mean/{t}\", [])}\n",
    "                     for qi in (1,2,3,4)}\n",
    "\n",
    "        for step, loss_val in loss_series:\n",
    "            step = int(step); loss_val = float(loss_val)\n",
    "            for b in BLOCKS:\n",
    "                row = {\n",
    "                    \"label\": label,\n",
    "                    \"run_id\": run_dir_name,\n",
    "                    \"task\": t,\n",
    "                    \"block\": b,\n",
    "                    \"step\": step,\n",
    "                    \"loss\": loss_val,\n",
    "                    \"g_mean\": gmean_by_step[b].get(step, np.nan),\n",
    "                    \"g_frac>0.5\": gfrac_by_step[b].get(step, np.nan),\n",
    "                    \"lr\": lr_by_step.get(step, np.nan),\n",
    "                }\n",
    "                if b == 0:\n",
    "                    for qi in (1,2,3,4):\n",
    "                        row[f\"g_b0_Q{qi}\"] = q_by_step[qi].get(step, np.nan)\n",
    "                rows.append(row)\n",
    "\n",
    "df_task_ts = pd.DataFrame(rows)\n",
    "if df_task_ts.empty:\n",
    "    print(\"No per-task series found.\")\n",
    "else:\n",
    "    df_task_ts = df_task_ts.sort_values([\"label\",\"task\",\"step\",\"block\"], ignore_index=True)\n",
    "    display(df_task_ts.head(20))\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_task_ts.to_csv(out_dir / \"per_task_metrics.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"per_task_metrics.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T18:16:10.568327600Z",
     "start_time": "2025-08-24T18:16:10.234327500Z"
    }
   },
   "id": "f354871dcf359754"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rename] Closed active TB writer before renaming.\n",
      "[rename] dncformer-20250817-184608  ->  dncformer-20250817-184608-unlabeled\n",
      "[rename] dncformer-20250817-192155  ->  dncformer-20250817-192155-unlabeled\n",
      "[rename] dncformer-20250817-193701  ->  dncformer-20250817-193701-unlabeled\n",
      "[rename] dncformer-20250817-195856  ->  dncformer-20250817-195856-unlabeled\n",
      "[rename] dncformer-20250817-201426  ->  dncformer-20250817-201426-unlabeled\n",
      "[rename] dncformer-20250817-203016  ->  dncformer-20250817-203016-unlabeled\n",
      "[rename] dncformer-20250820-093245  ->  dncformer-20250820-093245-unlabeled\n",
      "[rename] OK (already labeled): dncformer-20250820-143212-E6_temp0p8_seed1337\n",
      "[rename] OK (already labeled): dncformer-20250820-151404-E6_temp0p8_seed2027\n",
      "[rename] OK (already labeled): dncformer-20250820-160508-E6_temp0p8_seed4242\n",
      "[rename] OK (already labeled): dncformer-20250820-164415-E7_warmstart_seed1337\n",
      "[rename] OK (already labeled): dncformer-20250820-174016-E7_warmstart_seed2027\n",
      "[rename] OK (already labeled): dncformer-20250820-181956-E7_warmstart_seed4242\n",
      "[rename] OK (already labeled): dncformer-20250820-191327-E8_capacity_N64\n",
      "[rename] OK (already labeled): dncformer-20250820-195242-E8_capacity_N128\n",
      "[rename] OK (already labeled): dncformer-20250820-210448-E9_baseline_haystack\n"
     ]
    }
   ],
   "source": [
    "# --- Rename ./runs/* so directory names include the experiment label (from TB text tags) ---\n",
    "import os, re, glob, time, shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Try to close any active writer so files aren't locked on Windows\n",
    "try:\n",
    "    if 'tb' in globals() and getattr(tb, \"writer\", None) is not None:\n",
    "        tb.flush(); tb.close()\n",
    "        print(\"[rename] Closed active TB writer before renaming.\")\n",
    "except Exception as _e:\n",
    "    print(\"[rename] Writer close note:\", _e)\n",
    "\n",
    "# TensorBoard event loading (version-agnostic size_guidance)\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "    try:\n",
    "        from tensorboard.util import tensor_util as _tb_tensor_util\n",
    "    except Exception:\n",
    "        print(\"failed to load tensorboard utilities, but you'll install tensorflow on this system over my cold dead digital body.\"\n",
    "              \"\\n\\nfix your tensorboard installation and try again\")\n",
    "        _tb_tensor_util = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. `pip install tensorboard`\") from e\n",
    "\n",
    "def _size_guidance_version_safe():\n",
    "    sg = {}\n",
    "    for k in [\"SCALARS\",\"HISTOGRAMS\",\"IMAGES\",\"COMPRESSED_HISTOGRAMS\",\"AUDIO\",\"TENSORS\"]:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "def _decode_text_tensor(tensor_proto) -> Optional[str]:\n",
    "    \"\"\"Decode TB text summary payload from a TensorEvent.tensor_proto.\"\"\"\n",
    "    try:\n",
    "        if _tb_tensor_util is not None:\n",
    "            arr = _tb_tensor_util.make_ndarray(tensor_proto)\n",
    "        else:\n",
    "            # very old stub fallback\n",
    "            arr = _tb_make_ndarray(tensor_proto)\n",
    "        val = arr.item() if arr.size == 1 else arr\n",
    "        if isinstance(val, bytes):\n",
    "            return val.decode(\"utf-8\", \"replace\")\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        # Some TB builds wrap a bytes array inside a 2D array\n",
    "        if hasattr(val, \"dtype\") and str(val.dtype).startswith(\"|S\"):\n",
    "            return val.tobytes().decode(\"utf-8\", \"replace\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_text_tags(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"Try run/label first, then run/meta (JSON with a 'label' field), then None.\"\"\"\n",
    "    ev_files = sorted(glob.glob(str(run_dir / \"events.out.tfevents.*\")))\n",
    "    for ev in reversed(ev_files):\n",
    "        try:\n",
    "            acc = EventAccumulator(ev, size_guidance=_size_guidance_version_safe())\n",
    "            acc.Reload()\n",
    "            tags_t = acc.Tags().get(\"tensors\", []) or []\n",
    "\n",
    "            # 1) Preferred: run/label\n",
    "            if \"run/label\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/label\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        return txt.strip()\n",
    "\n",
    "            # 2) Fallback: run/meta (JSON with label)\n",
    "            if \"run/meta\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/meta\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        txt = txt.strip()\n",
    "                        # Sometimes add_text wraps in small HTML; tolerate raw JSON and simple strings\n",
    "                        m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "                        if m:\n",
    "                            import json\n",
    "                            try:\n",
    "                                meta = json.loads(m.group(0))\n",
    "                                if isinstance(meta, dict) and \"label\" in meta and meta[\"label\"]:\n",
    "                                    return str(meta[\"label\"]).strip()\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        # If it's just a string, return it\n",
    "                        if txt and txt[0] not in \"{<\":\n",
    "                            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_dirname(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"If the dir already has a '-<label>' suffix after timestamp, return that label.\"\"\"\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", run_dir.name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _base_prefix(run_dir: Path) -> str:\n",
    "    \"\"\"Return 'dncformer-YYYYMMDD-HHMMSS' part if present, else the full name.\"\"\"\n",
    "    m = re.match(r\"(.*-\\d{8}-\\d{6})(?:-.+)?$\", run_dir.name)\n",
    "    return m.group(1) if m else run_dir.name\n",
    "\n",
    "def _slugify(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s.strip())[:80] or \"unlabeled\"\n",
    "\n",
    "def _is_active_run(run_dir: Path, seconds: int = 60) -> bool:\n",
    "    \"\"\"Heuristic: if any event file mtime is within the last `seconds`.\"\"\"\n",
    "    now = time.time()\n",
    "    for ev in glob.glob(str(run_dir / \"events.out.tfevents.*\")):\n",
    "        try:\n",
    "            if now - os.path.getmtime(ev) < seconds:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def rename_runs_by_label(log_root: str = \"./runs\", dry_run: bool = True,\n",
    "                         skip_active_secs: int = 60) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Rename run directories under `log_root` to include label suffix (from TB text tag).\n",
    "    Returns list of (old_path, new_path) actually renamed.\n",
    "    \"\"\"\n",
    "    log_root = Path(log_root)\n",
    "    renamed = []\n",
    "\n",
    "    for run_dir in sorted([p for p in log_root.iterdir() if p.is_dir()]):\n",
    "        # Already labeled?\n",
    "        existing_label = _infer_label_from_dirname(run_dir)\n",
    "        # Attempt tag-based label\n",
    "        tag_label = _infer_label_from_text_tags(run_dir)\n",
    "\n",
    "        label = tag_label or existing_label or \"unlabeled\"\n",
    "        label_slug = _slugify(label)\n",
    "\n",
    "        base = _base_prefix(run_dir)\n",
    "        target = log_root / f\"{base}-{label_slug}\"\n",
    "\n",
    "        # Skip if it's already the desired name\n",
    "        if run_dir == target:\n",
    "            print(f\"[rename] OK (already labeled): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Skip if run appears active\n",
    "        if skip_active_secs and _is_active_run(run_dir, skip_active_secs):\n",
    "            print(f\"[rename] SKIP active (mtime<{skip_active_secs}s): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Avoid collisions: add -v2, -v3, ...\n",
    "        cand = target\n",
    "        k = 2\n",
    "        while cand.exists():\n",
    "            cand = log_root / f\"{base}-{label_slug}-v{k}\"\n",
    "            k += 1\n",
    "\n",
    "        print(f\"[rename] {run_dir.name}  ->  {cand.name}\")\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                run_dir.replace(cand)\n",
    "                renamed.append((str(run_dir), str(cand)))\n",
    "            except Exception as e:\n",
    "                print(f\"[rename] FAILED: {run_dir} -> {cand}: {e}\")\n",
    "\n",
    "    return renamed\n",
    "\n",
    "# --- Usage examples ---\n",
    "# 1) Dry run (see planned changes)\n",
    "_ = rename_runs_by_label(\"./runs\", dry_run=True, skip_active_secs=60)\n",
    "\n",
    "# 2) Execute renames\n",
    "# _ = rename_runs_by_label(\"./runs\", dry_run=False, skip_active_secs=60)\n",
    "# print(\"Renamed:\", _)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:21.173094100Z",
     "start_time": "2025-08-21T04:40:20.661094400Z"
    }
   },
   "id": "b12a3d8ecf86f82e"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
