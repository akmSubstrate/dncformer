{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "This notebook implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f6d41fd5a7206b"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:47.726643800Z",
     "start_time": "2025-08-19T19:27:47.675646600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "import os, math, random, time, numpy as np\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import contextlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n",
    "\n",
    "# SDPA configured via SDPA_CTX() above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7891e",
   "metadata": {},
   "source": [
    "## 1. Config & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:48.415196800Z",
     "start_time": "2025-08-19T19:27:48.389196500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n",
      "AMP dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "print(\"AMP dtype:\", amp_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# --- Config patch toggles ---\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class _Tmp: pass\n",
    "    CFG = _Tmp()\n",
    "# safe defaults if not already present\n",
    "if not hasattr(CFG, 'batch_size'): CFG.batch_size = 8\n",
    "if not hasattr(CFG, 'gate_reg_lambda'): CFG.gate_reg_lambda = 0.0   # only applied on memory-tagged batches\n",
    "if not hasattr(CFG, 'hist_every'): CFG.hist_every = 200             # histogram cadence\n",
    "if not hasattr(CFG, 'force_g'): CFG.force_g = None                  # None, or 0.0 or 1.0\n",
    "if not hasattr(CFG, 'gate_temp'): CFG.gate_temp = 1.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:48.627942800Z",
     "start_time": "2025-08-19T19:27:48.608942300Z"
    }
   },
   "id": "682d4e57"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:48.840459500Z",
     "start_time": "2025-08-19T19:27:48.826458100Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- SDPA selection (prefer PyTorch SDPA; avoid flash-attn) ---\n",
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:49.223824700Z",
     "start_time": "2025-08-19T19:27:49.212825400Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n",
    "\n",
    "def get_or_build_model_and_tokenizer():\n",
    "    mid = CFG.base_model_id\n",
    "    if _GLOBAL.get(\"head\") is not None and _GLOBAL.get(\"mid\") == mid:\n",
    "        # Reuse the already-loaded GPU model\n",
    "        return _GLOBAL[\"tok\"], _GLOBAL[\"head\"]\n",
    "    # Otherwise build fresh\n",
    "    tok, base = load_base_model(mid)             # your existing GPU-only loader\n",
    "    head = DNCFormerHead(base, CFG).to(device)   # enrichment head on CUDA\n",
    "    _GLOBAL.update({\"tok\": tok, \"head\": head, \"mid\": mid})\n",
    "    return tok, head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# --- Gate metrics utils (mean, frac>0.5, entropy) ---\n",
    "def _reduce_gate_tensor(g: torch.Tensor) -> torch.Tensor:\n",
    "    # g: (B,T,*) -> (B,T)\n",
    "    if g.dim() == 3:\n",
    "        return g.mean(dim=-1)\n",
    "    return g\n",
    "\n",
    "@torch.no_grad()\n",
    "def _gate_metrics(g: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns scalar (mean, frac>0.5, entropy)\n",
    "    \"\"\"\n",
    "    g2 = _reduce_gate_tensor(g.detach())\n",
    "    mean_val = float(g2.mean().item())\n",
    "    frac = float((g2 > 0.5).float().mean().item())\n",
    "    eps = 1e-6\n",
    "    p = g2.clamp(eps, 1 - eps)\n",
    "    # binary entropy (natural log base)\n",
    "    ent = float((-(p * (p + eps).log() + (1 - p) * (1 - p + eps).log())).mean().item())\n",
    "    return mean_val, frac, ent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:49.434859800Z",
     "start_time": "2025-08-19T19:27:49.418270200Z"
    }
   },
   "id": "dad9cdfcce225bcd"
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 3. DNC Memory (compact reference implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:49.814778900Z",
     "start_time": "2025-08-19T19:27:49.785764200Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        # aggregated lightweight stats (per-step)\n",
    "        try:\n",
    "            _stats = {}\n",
    "            with torch.no_grad():\n",
    "                _stats[\"u_mean\"] = state[\"u\"].mean().detach()\n",
    "                try:\n",
    "                    _stats[\"M_norm_mean\"] = state[\"M\"].norm(dim=-1).mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"rw_max_mean\"] = rw.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"ww_max_mean\"] = ww.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            state[\"stats\"] = _stats\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe(\n",
    "                {\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1), # (B,)\n",
    "                }\n",
    "            )\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "## 4. Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:50.202901600Z",
     "start_time": "2025-08-19T19:27:50.187902Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "## 5. DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:50.593089200Z",
     "start_time": "2025-08-19T19:27:50.584085100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "## 6. Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:50.986973900Z",
     "start_time": "2025-08-19T19:27:50.975974700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None, gate_override: None = None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None, gate_override: None = None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        \n",
    "        # collect per-block metrics if requested\n",
    "        if self.collect_metrics:\n",
    "            try:\n",
    "                import math, torch as _t\n",
    "                eps = 1e-6\n",
    "                g_clamp = g.clamp(min=eps, max=1-eps)\n",
    "                g_entropy = (-(g_clamp*_t.log(g_clamp) + (1-g_clamp)*_t.log(1-g_clamp))).mean()\n",
    "                vt_norm = vt.norm(dim=-1).mean()\n",
    "                dt_norm = dt.norm(dim=-1).mean()\n",
    "                _stats = dnc_state.get(\"stats\", {}) if isinstance(dnc_state, dict) else {}\n",
    "                # ensure CPU-detached tiny tensors\n",
    "                self.last_metrics = {\n",
    "                    \"g_mean\": g.mean().detach(),\n",
    "                    \"g_entropy\": g_entropy.detach(),\n",
    "                    \"vt_norm\": vt_norm.detach(),\n",
    "                    \"dt_norm\": dt_norm.detach(),\n",
    "                    **_stats\n",
    "                }\n",
    "            except Exception:\n",
    "                self.last_metrics = None\n",
    "        else:\n",
    "            self.last_metrics = None\n",
    "            return out, dnc_state, g \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "## 7. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:51.380271200Z",
     "start_time": "2025-08-19T19:27:51.369271300Z"
    }
   },
   "outputs": [],
   "source": [
    "def spda_ctx():\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception as e:\n",
    "        return f\"error {e} encountered ->\\n{contextlib.nullcontext()}\"\n",
    "\n",
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with spda_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n",
    "\n",
    "\n",
    "\n",
    "    def forward_with_metrics(self, input_ids: torch.Tensor, attention_mask: \"Optional[torch.Tensor]\" = None,\n",
    "                             gate_override: \"Optional[float]\" = None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1].to(device)  # ensure CUDA\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates_det = []\n",
    "        gates_raw = []\n",
    "        per_block = []\n",
    "        # enable metrics collection per block\n",
    "        for blk in self.blocks:\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = True\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i], gate_override=gate_override)\n",
    "            gates_raw.append(g)\n",
    "            gates_det.append(g.detach())\n",
    "            if hasattr(blk, \"last_metrics\") and blk.last_metrics is not None:\n",
    "                per_block.append(blk.last_metrics)\n",
    "            else:\n",
    "                per_block.append({})\n",
    "            # reset flag to avoid overhead elsewhere\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = False\n",
    "        # lm head on its own device, then back to CUDA\n",
    "        lm_dev = self.base.lm_head.weight.device\n",
    "        y = self.proj_out(h).to(lm_dev, dtype=self.base.lm_head.weight.dtype)\n",
    "        logits = self.base.lm_head(y).to(device)\n",
    "        aux = {\"per_block\": per_block, \"gates_raw\": gates_raw, \"gates_detached\": gates_det}\n",
    "        aux[\"g_entropy_block\"] = []\n",
    "        for g in gates_det:\n",
    "            _, _, ent = _gate_metrics(g)\n",
    "            aux[\"g_entropy_block\"].append(ent)\n",
    "        return logits, gates_det, aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 8. Data: Synthetic tasks + simple instruction-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:51.809006400Z",
     "start_time": "2025-08-19T19:27:51.781008500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0322be9",
   "metadata": {},
   "source": [
    "## 9. Training loop (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:52.919428800Z",
     "start_time": "2025-08-19T19:27:52.892430200Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Synthetic generators + HF dataset integration + MixtureSampler (with tagging) ---\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n",
    "\n",
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\"); out = ex.get(o_field, \"\")\n",
    "        if instr and out: pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items: break\n",
    "    random.shuffle(pairs); return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]; out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n",
    "\n",
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float], names: Optional[List[str]] = None):\n",
    "        self.gens = gens\n",
    "        import torch as _t\n",
    "        self.weights = list(map(float, weights))\n",
    "        self.p = _t.tensor(self.weights, dtype=_t.float32)  # CPU is fine for multinomial\n",
    "        self.p /= (self.p.sum() + 1e-8)\n",
    "        self.names = names if names is not None else [f\"g{i}\" for i in range(len(gens))]\n",
    "        self.last_name = None\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch as _t\n",
    "        idx = _t.multinomial(self.p, 1).item()\n",
    "        self.last_name = self.names[idx]\n",
    "        return self.gens[idx](batch)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Update mixture probabilities at runtime (used by schedules).\"\"\"\n",
    "        import torch as _t\n",
    "        ws = list(map(float, weights))\n",
    "        t = _t.tensor(ws, dtype=_t.float32, device=self.p.device)\n",
    "        s = float(t.sum().item())\n",
    "        if s <= 0:\n",
    "            raise ValueError(\"Mixture weights must sum to > 0\")\n",
    "        self.p = t / s\n",
    "        self.weights = ws\n",
    "        # Optional sanity: warn if names length mismatches weights\n",
    "        if hasattr(self, \"names\") and len(self.names) != len(ws):\n",
    "            print(f\"[MixtureSampler] Warning: len(names)={len(self.names)} != len(weights)={len(ws)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "p1: [0.30000001192092896, 0.30000001192092896, 0.25, 0.15000000596046448]\n"
     ]
    }
   ],
   "source": [
    "# mixture sampler smoke test\n",
    "ms = MixtureSampler(gens=[lambda b: None, lambda b: None, lambda b: None, lambda b: None],\n",
    "                    weights=[0.4,0.2,0.2,0.2],\n",
    "                    names=[\"hf\",\"copy\",\"repeat\",\"nback\"])\n",
    "print(\"p0:\", ms.p.tolist())    # ~[0.4,0.2,0.2,0.2]\n",
    "ms.set_weights([0.3,0.3,0.25,0.15])\n",
    "print(\"p1:\", ms.p.tolist())    # ~[0.3,0.3,0.25,0.15]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:53.331822500Z",
     "start_time": "2025-08-19T19:27:53.314822900Z"
    }
   },
   "id": "e11f39842acad840"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6be4ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:53.795035900Z",
     "start_time": "2025-08-19T19:27:53.777037400Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b44d73c717b8b58c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:54.364824500Z",
     "start_time": "2025-08-19T19:27:54.342294Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- TensorBoard Logger (lightweight) ---\n",
    "import os, time, json\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        self.writer = None\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        logdir = logdir or \"./runs\"\n",
    "        run_name = run_name or time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def add_histogram(self, tag: str, values: \"torch.Tensor\", step: int, bins: int = 50):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        v = values\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.detach()\n",
    "            if v.device.type != \"cpu\": v = v.cpu()\n",
    "            v = v.reshape(-1).float()\n",
    "        self.writer.add_histogram(tag, v, global_step=step, bins=bins)\n",
    "\n",
    "    def add_text(self, tag: str, text: str, step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_text(tag, text, step)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB active: True | logdir: ./runs\\dncformer-20250819-122754\n"
     ]
    }
   ],
   "source": [
    "tb = TBLogger(logdir=\"./runs\")\n",
    "try:\n",
    "    tb\n",
    "except NameError:\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "print(\"TB active:\", TB_AVAILABLE, \"| logdir:\", getattr(tb, \"path\", None))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:54.939298900Z",
     "start_time": "2025-08-19T19:27:54.922777700Z"
    }
   },
   "id": "4f8ff807b1355133"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    #print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(head, tok):\n",
    "    head.eval()\n",
    "    prompt = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(enc.input_ids)\n",
    "    print(\"Gates (mean):\", [g.mean().item() for g in gates])\n",
    "    return gates\n",
    "\n",
    "def train_small():\n",
    "    tok, head = get_or_build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, CFG.warmup_steps, CFG.train_steps, min_lr_ratio=0.10)\n",
    "    head.train()\n",
    "\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    for step in range(1, CFG.train_steps+1):\n",
    "        in_ids, lab_ids = tokenize_instruction_pairs(tok, INSTR_PAIRS, max_len=min(CFG.max_seq_len, 256))\n",
    "        in_ids = in_ids.to(device)\n",
    "        \n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            logits, gates = head(in_ids)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step()\n",
    "            \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % CFG.log_every == 0:\n",
    "            gm = [g.mean().item() for g in gates]\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e} | gates={gm}\")\n",
    "            tb.log_scalars(step, float(loss.item()), float(lr), gm)\n",
    "            \n",
    "    evaluate_simple(head, tok)\n",
    "    return head, tok"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:55.574801Z",
     "start_time": "2025-08-19T19:27:55.558799900Z"
    }
   },
   "id": "a70a7bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Eval harness (needle-in-haystack, copy, reverse)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9ee90"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# --- Haystack (needle) eval: long-span retrieval of a key's value ---\n",
    "def make_haystack_batch(batch: int, T: int = 256, vocab: int = 1024, sentinel: int = 3):\n",
    "    \"\"\"\n",
    "    Build sequences: ... K V ... K ?  (next token should be V)\n",
    "    Returns: input_ids [B,T], answer_ids [B], query_pos [B] (position of '?')\n",
    "    \"\"\"\n",
    "    assert T >= 12, \"T too small for haystack layout\"\n",
    "    x = torch.randint(5, vocab, (batch, T), dtype=torch.long)\n",
    "    K = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "    V = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "\n",
    "    # Place (K,V) in the first half\n",
    "    p1 = torch.randint(low=T//8, high=T//2 - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p1] = K\n",
    "    x[torch.arange(batch), p1 + 1] = V\n",
    "\n",
    "    # Place (K, sentinel) in the last quarter\n",
    "    p2 = torch.randint(low=3*T//4, high=T - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p2] = K\n",
    "    x[torch.arange(batch), p2 + 1] = sentinel  # '?'\n",
    "    query_pos = p2 + 1  # position of '?'; we will look at logits at this position\n",
    "\n",
    "    return x, V, query_pos\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_haystack(head, steps: int = 50, batch: int = 16, T: int = 256, vocab: int = 1024,\n",
    "                      tb_step: int = None, fast: bool = False):\n",
    "    \"\"\"\n",
    "    Long-span retrieval probe: ... K V ... K ? => predict V at '?'.\n",
    "    fast=True uses inference_mode() and smaller defaults to speed up sweeps.\n",
    "    \"\"\"\n",
    "    from torch.amp import autocast\n",
    "    head.eval()\n",
    "\n",
    "    # Fast path defaults (can still be overridden by explicit args)\n",
    "    if fast:\n",
    "        steps = min(steps, 10)\n",
    "        batch = min(batch, 8)\n",
    "        T = min(T, 128)\n",
    "\n",
    "    device_ = next(head.parameters()).device\n",
    "    use_amp = (amp_dtype in (torch.float16, torch.bfloat16)) and torch.cuda.is_available()\n",
    "\n",
    "    accs, losses = [], []\n",
    "    ctx = torch.inference_mode() if fast else torch.no_grad()\n",
    "    with ctx:\n",
    "        for _ in range(steps):\n",
    "            x, V, qpos = make_haystack_batch(batch, T=T, vocab=vocab)\n",
    "            x = x.to(device_); V = V.to(device_); qpos = qpos.to(device_)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    logits, _g = head(x)\n",
    "            else:\n",
    "                logits, _g = head(x)\n",
    "\n",
    "            idx = torch.arange(x.size(0), device=device_)\n",
    "            logits_q = logits[idx, qpos, :].float()\n",
    "            loss = F.cross_entropy(logits_q, V)\n",
    "            pred = logits_q.argmax(dim=-1)\n",
    "\n",
    "            accs.append((pred == V).float().mean().item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    acc_m = float(np.mean(accs)); loss_m = float(np.mean(losses))\n",
    "\n",
    "    if TB_AVAILABLE and ('tb' in globals()):\n",
    "        tb.writer.add_scalar(\"eval/haystack_acc\", acc_m, tb_step if tb_step is not None else 0)\n",
    "        tb.writer.add_scalar(\"eval/haystack_loss\", loss_m, tb_step if tb_step is not None else 0)\n",
    "        tb.flush()\n",
    "\n",
    "    head.train()\n",
    "    print(f\"[Haystack] acc={acc_m:.3f} | loss={loss_m:.3f} | fast={fast}\")\n",
    "    return acc_m, loss_m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:56.701724900Z",
     "start_time": "2025-08-19T19:27:56.679731900Z"
    }
   },
   "id": "efb2dc08fe1942ca"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# --- Memory tracer & TensorBoard visualizer ---\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:57.453881800Z",
     "start_time": "2025-08-19T19:27:57.434361500Z"
    }
   },
   "id": "028e39a3"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "#cuda_report(\"before\")\n",
    "#list_head_refs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:58.065798600Z",
     "start_time": "2025-08-19T19:27:58.034799700Z"
    }
   },
   "id": "9dc850ece5144e62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medium-size training loop test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597e27677e1220b4"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000) -> MixtureSampler:\n",
    "    \"\"\"\n",
    "    Build a mixture of generators: [HF, copy, repeat, nback].\n",
    "    - If hf_dataset is Alpaca-like (has instruction/output), we use hf_instruction_loader + make_hf_batch.\n",
    "    - Else we treat it as text-only (e.g., roneneldan/TinyStories), tokenizing and making windowed sequences.\n",
    "    - If HF fails/empty, we drop it and renormalize over synthetics.\n",
    "    \"\"\"\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "\n",
    "    gens, wts, names = [], [], []\n",
    "\n",
    "    hf_ok = False\n",
    "    hf_reason = \"\"\n",
    "    gen_hf = None\n",
    "\n",
    "    if hf_dataset:\n",
    "        try:\n",
    "            # Heuristic: use instruction loader for alpaca-like IDs\n",
    "            if \"alpaca\" in hf_dataset.lower():\n",
    "                pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\", \"output\"),\n",
    "                                              max_items=hf_max_items)\n",
    "                if pairs:\n",
    "                    def gen_hf(b): \n",
    "                        return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "                    hf_ok = True\n",
    "                else:\n",
    "                    hf_reason = \"hf_instruction_loader returned 0 pairs\"\n",
    "            else:\n",
    "                # Text-only fallback (e.g., roneneldan/TinyStories)\n",
    "                from datasets import load_dataset\n",
    "                # Try streaming first, then non-streaming\n",
    "                try:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\", streaming=True)\n",
    "                except Exception:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\")\n",
    "\n",
    "                # Find a usable text key\n",
    "                text_key = None\n",
    "                common_keys = (\"text\", \"content\", \"story\", \"document\", \"body\", \"article\")\n",
    "\n",
    "                feats = getattr(ds, \"features\", None)\n",
    "                if feats:\n",
    "                    for k in common_keys:\n",
    "                        if k in feats:\n",
    "                            text_key = k\n",
    "                            break\n",
    "\n",
    "                if text_key is None:\n",
    "                    # Probe first example (works for streaming iterable)\n",
    "                    try:\n",
    "                        first_ex = next(iter(ds))\n",
    "                        for k in common_keys:\n",
    "                            if k in first_ex:\n",
    "                                text_key = k\n",
    "                                break\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "\n",
    "                if text_key is None:\n",
    "                    hf_reason = \"no usable text field (tried: %s)\" % \",\".join(common_keys)\n",
    "                else:\n",
    "                    import random as _rnd\n",
    "                    import torch\n",
    "                    samples = []\n",
    "                    for ex in ds:\n",
    "                        txt = ex.get(text_key, None)\n",
    "                        if not txt:\n",
    "                            continue\n",
    "                        ids = tok(txt, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "                        if ids.numel() < 8:\n",
    "                            continue\n",
    "                        samples.append(ids.cpu())\n",
    "                        if len(samples) >= int(hf_max_items):\n",
    "                            break\n",
    "\n",
    "                    if len(samples) == 0:\n",
    "                        hf_reason = \"collected 0 tokenized samples\"\n",
    "                    else:\n",
    "                        def gen_hf(b: int) -> torch.Tensor:\n",
    "                            out = []\n",
    "                            for _ in range(b):\n",
    "                                ids = _rnd.choice(samples)\n",
    "                                n = ids.numel()\n",
    "                                if n >= mx:\n",
    "                                    s = _rnd.randint(0, n - mx)\n",
    "                                    seq = ids[s:s+mx]\n",
    "                                else:\n",
    "                                    pad = torch.full((mx - n,), pad_id, dtype=torch.long)\n",
    "                                    seq = torch.cat([ids, pad], dim=0)\n",
    "                                out.append(seq.unsqueeze(0))\n",
    "                            return torch.cat(out, dim=0)\n",
    "                        hf_ok = True\n",
    "        except Exception as e:\n",
    "            hf_reason = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "    # Assemble mixture in the agreed order (hf, copy, repeat, nback)\n",
    "    if hf_ok and gen_hf is not None:\n",
    "        gens.append(gen_hf); wts.append(weights[0]); names.append(\"hf\")\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        if hf_dataset:\n",
    "            print(f\"HF dataset unavailable or empty; using synthetic only. reason={hf_reason}\")\n",
    "        s_w = list(weights[1:])  # keep caller's relative weights for synthetics\n",
    "\n",
    "    # Synthetic tasks (your existing closures)\n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100, pad_id=pad_id, device=\"cpu\")\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "\n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback])\n",
    "    wts.extend(s_w)\n",
    "    names.extend([\"copy\", \"repeat\", \"nback\"])\n",
    "\n",
    "    return MixtureSampler(gens, wts, names=names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:27:59.608583500Z",
     "start_time": "2025-08-19T19:27:59.578584200Z"
    }
   },
   "id": "d629080a"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# GPU VRAM diagnostics/test\n",
    "import torch, gc, contextlib\n",
    "\n",
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "\n",
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:00.144490Z",
     "start_time": "2025-08-19T19:28:00.115982100Z"
    }
   },
   "id": "2af2ed601f9c7606"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=29.34 GB | reserved=29.44 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T23:09:53.843561700Z",
     "start_time": "2025-08-19T23:09:53.663514700Z"
    }
   },
   "id": "a83b2d08c4a137e7"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:02.892841100Z",
     "start_time": "2025-08-19T19:28:02.870843300Z"
    }
   },
   "id": "80e53881"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train_medium(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 5000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = False,\n",
    "    viz_prompt: str = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "    # NEW: optional schedules (each a list of (until_step, value))\n",
    "    mixture_schedule=None,         # e.g., [(100, (0.3,0.3,0.2,0.2)), (None, (0.4,0.2,0.2,0.2))]\n",
    "    gate_temp_schedule=None,       # e.g., [(100, 0.8), (None, 1.0)]\n",
    "    gate_reg_schedule=None,        # e.g., [(100, 3e-4), (None, 2e-4)]\n",
    "):\n",
    "    cfg = CFG\n",
    "    steps = int(steps or cfg.train_steps)\n",
    "    batch_size = int(batch_size or getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "\n",
    "    # Model & tokenizer\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "\n",
    "    # Sampler (and allow schedules to change weights)\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    def _apply_schedules(step: int, mix_name_hint=None):\n",
    "        # mixture schedule\n",
    "        if mixture_schedule:\n",
    "            for until, ws in mixture_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    try:\n",
    "                        mixer.set_weights(ws)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "        # gate temp schedule\n",
    "        if gate_temp_schedule:\n",
    "            for until, temp in gate_temp_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_temp\", float(temp))\n",
    "                    break\n",
    "        # gate reg schedule\n",
    "        if gate_reg_schedule:\n",
    "            for until, lam in gate_reg_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_reg_lambda\", float(lam))\n",
    "                    break\n",
    "\n",
    "    # TB setup\n",
    "    global tb\n",
    "    if TB_AVAILABLE:\n",
    "        try:\n",
    "            tb  # NameError if not defined\n",
    "        except NameError:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        if not isinstance(tb, TBLogger) or getattr(tb, \"writer\", None) is None:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        tblog = True\n",
    "        tb.add_text(\"run/config\", json.dumps({\n",
    "            \"steps\": steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    else:\n",
    "        tblog = False\n",
    "\n",
    "    head.train()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(amp_dtype in (torch.float16, torch.bfloat16)))\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        _apply_schedules(step)\n",
    "\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "        with torch.autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype != torch.float32)):\n",
    "            logits, gates, aux = head.forward_with_metrics(in_ids, gate_override=getattr(CFG, \"force_g\", None))\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "            # Optional: mild gate-usage regularizer (same as before)\n",
    "            lam = float(getattr(CFG, \"gate_reg_lambda\", 0.0))\n",
    "            if lam > 0 and isinstance(gates, (list, tuple)) and len(gates) > 0:\n",
    "                reg = 0.0\n",
    "                for g in gates:\n",
    "                    g2 = _reduce_gate_tensor(g)\n",
    "                    # Encourage decisive routing on memory batches only? For now, uniform\n",
    "                    reg = reg + (g2.mean() * 0.0 + (g2 * (1 - g2)).mean())  # small entropy-like penalty\n",
    "                loss = loss + lam * reg\n",
    "\n",
    "        use_scaler = (amp_dtype in (torch.float16, torch.bfloat16))\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step(); optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # --- Logging (TB) ---\n",
    "        if step % log_every == 0 and tblog:\n",
    "            # global scalars\n",
    "            tb.writer.add_scalar(\"train/loss\", float(loss.item()), step)\n",
    "            tb.writer.add_scalar(\"train/lr\", float(scheduler.get_last_lr()[0]), step)\n",
    "\n",
    "            # gate metrics per block (global)\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_mean\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_frac>0.5\", g_frac, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_entropy\", g_ent, step)\n",
    "\n",
    "                # per-task metrics (using the sampler's last batch type)\n",
    "                mix_name = getattr(mixer, \"last_name\", None) or \"unknown\"\n",
    "                tb.writer.add_scalar(f\"loss_by_task/{mix_name}\", float(loss.item()), step)\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_mean/{mix_name}\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_frac>0.5/{mix_name}\", g_frac, step)\n",
    "\n",
    "                # optional: quartiles (block 0)\n",
    "                if len(gates) > 0:\n",
    "                    g0 = _reduce_gate_tensor(gates[0].detach())\n",
    "                    T = g0.size(1); q = max(1, T // 4)\n",
    "                    slices = [(0, q), (q, 2*q), (2*q, 3*q), (3*q, T)]\n",
    "                    for qi, (s, e) in enumerate(slices, start=1):\n",
    "                        tb.writer.add_scalar(f\"gates/block0_q{qi}_mean/{mix_name}\", float(g0[:, s:e].mean().item()), step)\n",
    "\n",
    "            tb.flush()\n",
    "\n",
    "        # --- Console echo (always) ---\n",
    "        if step % log_every == 0:\n",
    "            g_means_print = []\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for g in gates:\n",
    "                    g_means_print.append(float(_reduce_gate_tensor(g).mean().item()))\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {scheduler.get_last_lr()[0]:.2e} | \"\n",
    "                  f\"gates={g_means_print} | mix={getattr(mixer,'last_name','?')}\")\n",
    "\n",
    "    # Optional memory viz\n",
    "    if viz_memory_after:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "    \n",
    "    # flush log      \n",
    "    if tblog:\n",
    "        tb.flush()\n",
    "\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:03.518082600Z",
     "start_time": "2025-08-19T19:28:03.485560300Z"
    }
   },
   "id": "f8879a898faf706a"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# # --- Patch: robust forward for ParallelEnrichmentBlock ---\n",
    "def _peb_forward(self, x, dnc_state=None, gate_override=None):\n",
    "    \"\"\"\n",
    "    Returns: (out, dnc_state, g)\n",
    "      out: (B,T,D), mixed from vanilla (vt) and DNC (dt)\n",
    "      dnc_state: updated state dict from DNC path\n",
    "      g: (B,T,D) or (B,T,1) gate values in [0,1]\n",
    "    \"\"\"\n",
    "    # 1) Vanilla transformer path\n",
    "    T = x.size(1)\n",
    "    mask = causal_mask(T, device=x.device)  # (T,T) causal attn mask\n",
    "    x_cast = x.to(self.vanilla.ln1.weight.dtype)\n",
    "    vt = self.vanilla(x_cast, attn_mask=mask)    # (B,T,D)\n",
    "\n",
    "    # 2) DNC path (controller+memory)\n",
    "    dt, dnc_state = self.dncblock(x, state=dnc_state)  # dt: (B,T,D)\n",
    "\n",
    "    # 3) Gate computation (optionally LN before gate)\n",
    "    import torch as _t\n",
    "    z = _t.cat([vt, dt], dim=-1)  # (B,T,2D)\n",
    "    h = self.pre_gate_ln(z) if hasattr(self, \"pre_gate_ln\") and self.pre_gate_ln is not None else z\n",
    "    g_pre = self.gate(h)                 # typically (B,T,D) or (B,T,1)\n",
    "    tau = float(getattr(CFG, 'gate_temp', 1.0))\n",
    "    g = torch.sigmoid(g_pre / max(tau, 1e-6))\n",
    "\n",
    "    # Optional ablation override: force g to 0.0 (vanilla) or 1.0 (DNC)\n",
    "    if gate_override is not None:\n",
    "        g = _t.full_like(g, float(gate_override))\n",
    "\n",
    "    # 4) Blend paths\n",
    "    out = g * dt + (1.0 - g) * vt        # (B,T,D)\n",
    "\n",
    "    # 5) Lightweight metrics (only if requested)\n",
    "    self.last_metrics = None\n",
    "    if getattr(self, \"collect_metrics\", False):\n",
    "        try:\n",
    "            eps = 1e-6\n",
    "            gc = g.clamp(min=eps, max=1.0 - eps)\n",
    "            g_entropy = (-(gc * gc.log() + (1 - gc) * (1 - gc).log())).mean()\n",
    "            vt_norm = vt.norm(dim=-1).mean()\n",
    "            dt_norm = dt.norm(dim=-1).mean()\n",
    "            mstats = {}\n",
    "            # Pull aggregated memory stats if present\n",
    "            if isinstance(dnc_state, dict) and isinstance(dnc_state.get(\"stats\", None), dict):\n",
    "                for k in (\"u_mean\", \"rw_max_mean\", \"ww_max_mean\", \"M_norm_mean\"):\n",
    "                    if k in dnc_state[\"stats\"]:\n",
    "                        mstats[k] = dnc_state[\"stats\"][k]\n",
    "            self.last_metrics = {\n",
    "                \"g_mean\": g.mean().detach(),\n",
    "                \"g_entropy\": g_entropy.detach(),\n",
    "                \"vt_norm\": vt_norm.detach(),\n",
    "                \"dt_norm\": dt_norm.detach(),\n",
    "                **mstats,\n",
    "            }\n",
    "        except Exception:\n",
    "            self.last_metrics = None\n",
    "\n",
    "    return out, dnc_state, g\n",
    "\n",
    "# Apply the patch\n",
    "ParallelEnrichmentBlock.forward = _peb_forward\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:04.058714900Z",
     "start_time": "2025-08-19T19:28:04.045716300Z"
    }
   },
   "id": "3eabc7991c7e743d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Unit-like tests (sanity)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3036ac"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "\n",
    "def run_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:05.870823500Z",
     "start_time": "2025-08-19T19:28:05.824785700Z"
    }
   },
   "id": "2cb70f70"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# --- Evaluator unit tests (no heavy model) ---\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:06.466809600Z",
     "start_time": "2025-08-19T19:28:06.436767900Z"
    }
   },
   "id": "db24ba31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unit tests/basic sanity checks\n",
    "currently all passing, but self-reminder here to comment out if architecture changes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f24f2641cb05940"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# memory tracer smoke test\n",
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:10.681906700Z",
     "start_time": "2025-08-19T19:28:10.667900400Z"
    }
   },
   "id": "d842ad22"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_copy batch shape: torch.Size([3, 128]) | dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# generator smoke test\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:12.062587400Z",
     "start_time": "2025-08-19T19:28:12.026665800Z"
    }
   },
   "id": "be0b78c71554906c"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "360da31ded284bd8b602d9f6a5fb3daf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n"
     ]
    }
   ],
   "source": [
    "# smoke test for _build_mixer\n",
    "# 1) With Alpaca (instruction/output)\n",
    "tok, _ = build_model_and_tokenizer()\n",
    "m1 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=500)\n",
    "print(\"names:\", m1.names, \"| p:\", m1.p.tolist())  # expect 'hf' present\n",
    "\n",
    "# 2) With TinyStories (text)\n",
    "m2 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"roneneldan/TinyStories\", hf_max_items=500)\n",
    "print(\"names:\", m2.names, \"| p:\", m2.p.tolist())  # expect 'hf' present (text path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:28:35.239602300Z",
     "start_time": "2025-08-19T19:28:13.235048200Z"
    }
   },
   "id": "92366ea6aeb2c846"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d792beb74bf460cba79436b438234bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=7.64 GB | reserved=7.64 GB | free=16.79 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "# cuda allocation smoke test\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-19T17:39:33.661604100Z"
    }
   },
   "id": "4f0be150819cf745"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a16f7263e3ae411eaa61c6399dbccdf9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Haystack] acc=0.000 | loss=10.112\n",
      "Haystack smoke: 0.0 10.111836910247803\n"
     ]
    }
   ],
   "source": [
    "# haystack smoke test\n",
    "tok, head = build_model_and_tokenizer()\n",
    "acc, loss = evaluate_haystack(head, steps=2, batch=4, T=64, vocab=512, tb_step=0)\n",
    "print(\"Haystack smoke:\", acc, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T19:32:13.539602300Z",
     "start_time": "2025-08-19T19:31:54.065716400Z"
    }
   },
   "id": "dca7ccde4a7a19d4"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#run_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-19T17:40:18.932094100Z"
    }
   },
   "id": "7aac46d77e635252"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "#run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T17:40:19.027093100Z",
     "start_time": "2025-08-19T17:40:18.995094900Z"
    }
   },
   "id": "480f5d04e5aaed2a"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T17:40:19.861091900Z",
     "start_time": "2025-08-19T17:40:19.837094700Z"
    }
   },
   "id": "38cc88e726027b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training run Sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867e3f96209f238d"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# small training test\n",
    "#head, tok = train_small()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T17:40:22.084096900Z",
     "start_time": "2025-08-19T17:40:22.061099400Z"
    }
   },
   "id": "b72c8e3e4a7e3237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medium size training sweeps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d5516ca50b4309"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a9cc2ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T18:16:55.881165200Z",
     "start_time": "2025-08-19T18:16:05.627903900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n",
      "[snapshot: before train_medium] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b5de6bed97a4646990711d90d878b39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 3.7740 | lr 2.00e-05 | gates=[0.275390625, 0.271484375] | mix=hf\n",
      "[snapshot: after train_medium] alloc=9.79 GB | reserved=23.34 GB | free=0.16 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "# single medium training test/sanity check\n",
    "free_head_and_cache()\n",
    "cuda_report(\"snapshot: before train_medium\")\n",
    "head, tok = train_medium(steps=10, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "cuda_report(\"snapshot: after train_medium\")\n",
    "#Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium training sweep, testing parameter / dataset variants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81cbb7a830e97c75"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E0_baseline ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.77 GB | reserved=9.79 GB | free=14.65 GB | total=25.77 GB\n",
      "[before E0_baseline] alloc=9.77 GB | reserved=9.79 GB | free=14.65 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eabace8af14f4c37b62339b4a01c4be5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E0_baseline] alloc=19.56 GB | reserved=42.15 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.59 GB | free=3.96 GB | total=25.77 GB\n",
      "\n",
      "=== E1_memory_lean ===\n",
      "mixture=(0.4, 0.3, 0.2, 0.1), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E1_memory_lean] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "decb5484ebff4a4291c95931ee205f92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E1_memory_lean] alloc=19.56 GB | reserved=40.45 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E2_gate_reg_low ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0002, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E2_gate_reg_low] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10a2fe1167d348418cf1d2d04385ce27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E2_gate_reg_low] alloc=19.56 GB | reserved=40.85 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.59 GB | free=3.96 GB | total=25.77 GB\n",
      "\n",
      "=== E3_gate_reg_high ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0005, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E3_gate_reg_high] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8c36a72cb094e35bd8838070d613b0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E3_gate_reg_high] alloc=19.56 GB | reserved=42.10 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E4_gate_temp_0p7 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=0.7, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E4_gate_temp_0p7] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69a109786ffe4cf3b148f9cf9bd29e52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E4_gate_temp_0p7] alloc=19.56 GB | reserved=40.85 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.01 GB | total=25.77 GB\n",
      "\n",
      "=== E5a_force_g_0 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=0.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E5a_force_g_0] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ee638c623dd4eaaac3db00a752a8403"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E5a_force_g_0] alloc=19.56 GB | reserved=40.14 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E5b_force_g_1 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=1.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[before E5b_force_g_1] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e03c0b134e6844f2bd53a3ed9e301b3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after  E5b_force_g_1] alloc=19.56 GB | reserved=41.73 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.02 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compact experiment driver: E0..E5 ---\n",
    "import time, torch, gc\n",
    "\n",
    "# run set parameters\n",
    "EXP_STEPS   = 500       # try 1000–5000 for longer curves\n",
    "EXP_WARMUP  = 10       # keep ~steps/20\n",
    "BASE_MIX    = (0.4, 0.2, 0.2, 0.2)  # (hf, copy, repeat, nback)\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_isolate_test(label,\n",
    "            mixture_weights=BASE_MIX,\n",
    "            gate_reg_lambda=None,\n",
    "            gate_temp=None,\n",
    "            force_g=None,\n",
    "            steps=EXP_STEPS,\n",
    "            warmup=EXP_WARMUP):\n",
    "    \"\"\"Run a single train_medium experiment with explicit knobs.\"\"\"\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    # set experiment-specific knobs (others come from CFG defaults)\n",
    "    if gate_reg_lambda is not None: set_cfg(gate_reg_lambda=float(gate_reg_lambda))\n",
    "    if gate_temp is not None:       set_cfg(gate_temp=float(gate_temp))\n",
    "    set_cfg(force_g=force_g)  # may be None, 0.0, or 1.0\n",
    "\n",
    "    print(f\"mixture={mixture_weights}, gate_reg_lambda={getattr(CFG,'gate_reg_lambda', 0.0)}, \"\n",
    "          f\"gate_temp={getattr(CFG,'gate_temp', 1.0)}, force_g={getattr(CFG,'force_g', None)}\")\n",
    "\n",
    "    # clean slate for VRAM and allocator fragmentation between runs\n",
    "    free_head_and_cache()\n",
    "    cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # run\n",
    "    head, tok = train_medium(\n",
    "        steps=steps,\n",
    "        warmup_steps=warmup,\n",
    "        mixture_weights=mixture_weights,\n",
    "        viz_memory_after=False,   # keep quick; use visualize_memory_tb() ad-hoc\n",
    "    )\n",
    "\n",
    "    # post-run snapshot + cleanup\n",
    "    cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# === E0 baseline: identical to your sanity run ===\n",
    "run_isolate_test(\"E0_baseline\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E1 memory-leaning mix: more algorithmic exposure ===\n",
    "run_isolate_test(\"E1_memory_lean\",\n",
    "        mixture_weights=(0.4, 0.3, 0.2, 0.1),  # HF, copy, repeat, nback\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E2 gate regularizer (low) ===\n",
    "run_isolate_test(\"E2_gate_reg_low\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=2e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E3 gate regularizer (high) ===\n",
    "run_isolate_test(\"E3_gate_reg_high\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=5e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E4 sharper routing via lower gate temperature ===\n",
    "run_isolate_test(\"E4_gate_temp_0p7\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=0.7,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E5 ablations: disable/force DNC path ===\n",
    "run_isolate_test(\"E5a_force_g_0\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=0.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "run_isolate_test(\"E5b_force_g_1\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=1.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T08:50:51.919131500Z",
     "start_time": "2025-08-19T03:06:35.627725400Z"
    }
   },
   "id": "771c919035840bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T18:17:29.965132700Z",
     "start_time": "2025-08-19T18:17:29.700136Z"
    }
   },
   "id": "f07fc074b58caaa0"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E6_temp0p8_seed1337 | seed=1337 ===\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed1337] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f341a3c63f04984b12dabd6fba8d614"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.4975 | lr 2.00e-04 | gates=[0.2353515625, 0.240234375] | mix=hf\n",
      "step 20 | loss 5.7329 | lr 1.93e-04 | gates=[0.25, 0.2578125] | mix=nback\n",
      "step 30 | loss 6.3398 | lr 1.74e-04 | gates=[0.255859375, 0.26171875] | mix=copy\n",
      "step 40 | loss 3.6917 | lr 1.47e-04 | gates=[0.2158203125, 0.2275390625] | mix=hf\n",
      "step 50 | loss 2.1587 | lr 1.14e-04 | gates=[0.208984375, 0.22265625] | mix=hf\n",
      "step 60 | loss 1.9526 | lr 7.92e-05 | gates=[0.2060546875, 0.220703125] | mix=hf\n",
      "step 70 | loss 2.0035 | lr 4.70e-05 | gates=[0.201171875, 0.2158203125] | mix=hf\n",
      "step 80 | loss 5.5659 | lr 2.12e-05 | gates=[0.255859375, 0.26953125] | mix=copy\n",
      "step 90 | loss 1.3354 | lr 2.00e-05 | gates=[0.203125, 0.216796875] | mix=hf\n",
      "step 100 | loss 1.5504 | lr 2.00e-05 | gates=[0.19921875, 0.2138671875] | mix=hf\n",
      "[after  E6_temp0p8_seed1337] alloc=40.47 GB | reserved=58.81 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.71 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed2027 | seed=2027 ===\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed2027] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c29dec0b8fb44feb3a9ae941886ffe6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 7.0602 | lr 2.00e-04 | gates=[0.2412109375, 0.2470703125] | mix=nback\n",
      "step 20 | loss 1.9751 | lr 1.93e-04 | gates=[0.2333984375, 0.236328125] | mix=hf\n",
      "step 30 | loss 6.5971 | lr 1.74e-04 | gates=[0.2451171875, 0.255859375] | mix=repeat\n",
      "step 40 | loss 6.4307 | lr 1.47e-04 | gates=[0.248046875, 0.263671875] | mix=repeat\n",
      "step 50 | loss 2.2391 | lr 1.14e-04 | gates=[0.2109375, 0.220703125] | mix=hf\n",
      "step 60 | loss 6.0388 | lr 7.92e-05 | gates=[0.24609375, 0.263671875] | mix=copy\n",
      "step 70 | loss 5.7137 | lr 4.70e-05 | gates=[0.248046875, 0.267578125] | mix=copy\n",
      "step 80 | loss 1.5096 | lr 2.12e-05 | gates=[0.2041015625, 0.216796875] | mix=hf\n",
      "step 90 | loss 5.3578 | lr 2.00e-05 | gates=[0.244140625, 0.263671875] | mix=repeat\n",
      "step 100 | loss 1.5022 | lr 2.00e-05 | gates=[0.2001953125, 0.212890625] | mix=hf\n",
      "[after  E6_temp0p8_seed2027] alloc=40.47 GB | reserved=57.77 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.70 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed4242 | seed=4242 ===\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed4242] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77814a8af1b548ae8eaa05062060f662"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 8.1877 | lr 2.00e-04 | gates=[0.25, 0.251953125] | mix=repeat\n",
      "step 20 | loss 6.7026 | lr 1.93e-04 | gates=[0.255859375, 0.259765625] | mix=nback\n",
      "step 30 | loss 3.0415 | lr 1.74e-04 | gates=[0.2353515625, 0.2421875] | mix=hf\n",
      "step 40 | loss 5.4770 | lr 1.47e-04 | gates=[0.259765625, 0.267578125] | mix=repeat\n",
      "step 50 | loss 1.3972 | lr 1.14e-04 | gates=[0.2255859375, 0.234375] | mix=hf\n",
      "step 60 | loss 5.4101 | lr 7.92e-05 | gates=[0.2578125, 0.2734375] | mix=nback\n",
      "step 70 | loss 5.6242 | lr 4.70e-05 | gates=[0.26171875, 0.2734375] | mix=copy\n",
      "step 80 | loss 5.5566 | lr 2.12e-05 | gates=[0.263671875, 0.2734375] | mix=copy\n",
      "step 90 | loss 1.5476 | lr 2.00e-05 | gates=[0.216796875, 0.2265625] | mix=hf\n",
      "step 100 | loss 5.1131 | lr 2.00e-05 | gates=[0.263671875, 0.2734375] | mix=repeat\n",
      "[after  E6_temp0p8_seed4242] alloc=40.47 GB | reserved=57.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.70 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed1337 | seed=1337 ===\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed1337] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34468b3329cf4286ad27482e4d554ab4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.4303 | lr 2.00e-04 | gates=[0.24609375, 0.251953125] | mix=repeat\n",
      "step 20 | loss 2.1397 | lr 1.93e-04 | gates=[0.236328125, 0.2451171875] | mix=hf\n",
      "step 30 | loss 6.6853 | lr 1.74e-04 | gates=[0.251953125, 0.26171875] | mix=nback\n",
      "step 40 | loss 1.8675 | lr 1.47e-04 | gates=[0.22265625, 0.2333984375] | mix=hf\n",
      "step 50 | loss 1.5155 | lr 1.14e-04 | gates=[0.216796875, 0.2275390625] | mix=hf\n",
      "step 60 | loss 6.3036 | lr 7.92e-05 | gates=[0.2890625, 0.298828125] | mix=copy\n",
      "step 70 | loss 5.6683 | lr 4.70e-05 | gates=[0.287109375, 0.296875] | mix=repeat\n",
      "step 80 | loss 5.6513 | lr 2.12e-05 | gates=[0.2890625, 0.30078125] | mix=copy\n",
      "step 90 | loss 1.6827 | lr 2.00e-05 | gates=[0.2470703125, 0.255859375] | mix=hf\n",
      "step 100 | loss 5.1239 | lr 2.00e-05 | gates=[0.287109375, 0.30078125] | mix=nback\n",
      "[after  E7_warmstart_seed1337] alloc=40.47 GB | reserved=58.81 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.71 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed2027 | seed=2027 ===\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed2027] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cb51753b687427aa85ef737825d0a65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.3192 | lr 2.00e-04 | gates=[0.244140625, 0.24609375] | mix=hf\n",
      "step 20 | loss 9.0200 | lr 1.93e-04 | gates=[0.2373046875, 0.2412109375] | mix=hf\n",
      "step 30 | loss 7.7515 | lr 1.74e-04 | gates=[0.251953125, 0.26171875] | mix=nback\n",
      "step 40 | loss 6.8784 | lr 1.47e-04 | gates=[0.251953125, 0.265625] | mix=copy\n",
      "step 50 | loss 6.0626 | lr 1.14e-04 | gates=[0.25390625, 0.26953125] | mix=copy\n",
      "step 60 | loss 2.4782 | lr 7.92e-05 | gates=[0.255859375, 0.26953125] | mix=hf\n",
      "step 70 | loss 1.7376 | lr 4.70e-05 | gates=[0.25390625, 0.26953125] | mix=hf\n",
      "step 80 | loss 5.5908 | lr 2.12e-05 | gates=[0.2890625, 0.306640625] | mix=copy\n",
      "step 90 | loss 5.0990 | lr 2.00e-05 | gates=[0.287109375, 0.306640625] | mix=nback\n",
      "step 100 | loss 4.9676 | lr 2.00e-05 | gates=[0.29296875, 0.30859375] | mix=repeat\n",
      "[after  E7_warmstart_seed2027] alloc=40.47 GB | reserved=57.77 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.71 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed4242 | seed=4242 ===\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed4242] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b317bf2c43e54c3fba65bc8641cd3113"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 9.4094 | lr 2.00e-04 | gates=[0.25, 0.251953125] | mix=copy\n",
      "step 20 | loss 6.5907 | lr 1.93e-04 | gates=[0.255859375, 0.259765625] | mix=nback\n",
      "step 30 | loss 6.2954 | lr 1.74e-04 | gates=[0.2578125, 0.26171875] | mix=copy\n",
      "step 40 | loss 5.3130 | lr 1.47e-04 | gates=[0.259765625, 0.265625] | mix=repeat\n",
      "step 50 | loss 1.4202 | lr 1.14e-04 | gates=[0.2216796875, 0.23046875] | mix=hf\n",
      "step 60 | loss 5.3073 | lr 7.92e-05 | gates=[0.29296875, 0.302734375] | mix=nback\n",
      "step 70 | loss 5.5675 | lr 4.70e-05 | gates=[0.294921875, 0.3046875] | mix=copy\n",
      "step 80 | loss 5.5075 | lr 2.12e-05 | gates=[0.296875, 0.3046875] | mix=copy\n",
      "step 90 | loss 1.6206 | lr 2.00e-05 | gates=[0.251953125, 0.259765625] | mix=hf\n",
      "step 100 | loss 5.0807 | lr 2.00e-05 | gates=[0.296875, 0.3046875] | mix=repeat\n",
      "[after  E7_warmstart_seed4242] alloc=40.47 GB | reserved=57.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.70 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N64 | seed=777 ===\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E8_capacity_N64] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9d6f4ecfe5d4884a76c71bdcba2ba03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.3589 | lr 2.00e-04 | gates=[0.279296875, 0.279296875] | mix=hf\n",
      "step 20 | loss 7.0277 | lr 1.93e-04 | gates=[0.279296875, 0.2890625] | mix=copy\n",
      "step 30 | loss 6.8002 | lr 1.74e-04 | gates=[0.279296875, 0.29296875] | mix=nback\n",
      "step 40 | loss 6.3232 | lr 1.47e-04 | gates=[0.283203125, 0.296875] | mix=repeat\n",
      "step 50 | loss 6.3752 | lr 1.14e-04 | gates=[0.279296875, 0.296875] | mix=nback\n",
      "step 60 | loss 1.7769 | lr 7.92e-05 | gates=[0.2392578125, 0.2421875] | mix=hf\n",
      "step 70 | loss 1.3798 | lr 4.70e-05 | gates=[0.2353515625, 0.2392578125] | mix=hf\n",
      "step 80 | loss 5.0608 | lr 2.12e-05 | gates=[0.279296875, 0.30078125] | mix=nback\n",
      "step 90 | loss 1.8050 | lr 2.00e-05 | gates=[0.228515625, 0.232421875] | mix=hf\n",
      "step 100 | loss 5.2268 | lr 2.00e-05 | gates=[0.27734375, 0.298828125] | mix=nback\n",
      "[after  E8_capacity_N64] alloc=40.47 GB | reserved=59.13 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.71 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N128 | seed=777 ===\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E8_capacity_N128] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b7fe45221a5488786e94a8f988726f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.3579 | lr 2.00e-04 | gates=[0.279296875, 0.279296875] | mix=hf\n",
      "step 20 | loss 7.0285 | lr 1.93e-04 | gates=[0.279296875, 0.2890625] | mix=copy\n",
      "step 30 | loss 6.8010 | lr 1.74e-04 | gates=[0.279296875, 0.29296875] | mix=nback\n",
      "step 40 | loss 6.3245 | lr 1.47e-04 | gates=[0.283203125, 0.296875] | mix=repeat\n",
      "step 50 | loss 6.3738 | lr 1.14e-04 | gates=[0.279296875, 0.296875] | mix=nback\n",
      "step 60 | loss 1.7869 | lr 7.92e-05 | gates=[0.2392578125, 0.2421875] | mix=hf\n",
      "step 70 | loss 1.3772 | lr 4.70e-05 | gates=[0.2353515625, 0.2392578125] | mix=hf\n",
      "step 80 | loss 5.0625 | lr 2.12e-05 | gates=[0.279296875, 0.30078125] | mix=nback\n",
      "step 90 | loss 1.8055 | lr 2.00e-05 | gates=[0.228515625, 0.232421875] | mix=hf\n",
      "step 100 | loss 5.2289 | lr 2.00e-05 | gates=[0.27734375, 0.298828125] | mix=nback\n",
      "[after  E8_capacity_N128] alloc=40.47 GB | reserved=59.13 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.71 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E9_baseline_haystack | seed=31415 ===\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0002 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n",
      "[before E9_baseline_haystack] alloc=30.70 GB | reserved=30.93 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "236071068f94442fb97abba9ec2f5d0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 4.9431 | lr 2.00e-04 | gates=[0.2734375, 0.275390625] | mix=hf\n",
      "step 20 | loss 1.8983 | lr 1.93e-04 | gates=[0.26171875, 0.265625] | mix=hf\n",
      "step 30 | loss 5.5588 | lr 1.74e-04 | gates=[0.279296875, 0.287109375] | mix=nback\n",
      "step 40 | loss 6.8865 | lr 1.47e-04 | gates=[0.283203125, 0.291015625] | mix=copy\n",
      "step 50 | loss 2.0390 | lr 1.14e-04 | gates=[0.2314453125, 0.2412109375] | mix=hf\n",
      "step 60 | loss 2.1616 | lr 7.92e-05 | gates=[0.234375, 0.2431640625] | mix=hf\n",
      "step 70 | loss 2.0309 | lr 4.70e-05 | gates=[0.228515625, 0.23828125] | mix=hf\n",
      "step 80 | loss 1.5388 | lr 2.12e-05 | gates=[0.228515625, 0.2392578125] | mix=hf\n",
      "step 90 | loss 1.3621 | lr 2.00e-05 | gates=[0.2294921875, 0.2392578125] | mix=hf\n",
      "step 100 | loss 5.5914 | lr 2.00e-05 | gates=[0.283203125, 0.294921875] | mix=copy\n",
      "[Haystack] acc=0.001 | loss=14.843\n",
      "[after  E9_baseline_haystack] alloc=40.47 GB | reserved=56.68 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=40.47 GB | reserved=40.69 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Experiment driver: E6..E9 (E2 architecture) ---\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_one_labeled(label, steps, mixture_weights, seed=1234,\n",
    "                    mixture_schedule=None, gate_temp_schedule=None, gate_reg_schedule=None,\n",
    "                    post_haystack=False):\n",
    "    print(f\"\\n=== {label} | seed={seed} ===\")\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed) \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_cfg(force_g=None)  # ensure no ablation\n",
    "    # optional: print short config\n",
    "    print(\"CFG.gate_temp:\", getattr(CFG, \"gate_temp\", 1.0),\n",
    "          \"| CFG.gate_reg_lambda:\", getattr(CFG, \"gate_reg_lambda\", 0.0),\n",
    "          \"| mixture:\", mixture_weights)\n",
    "\n",
    "    free_head_and_cache()\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}\")\n",
    "\n",
    "    head, tok = train_medium(\n",
    "        steps=steps,\n",
    "        warmup_steps=max(10, steps//20),\n",
    "        mixture_weights=mixture_weights,\n",
    "        mixture_schedule=mixture_schedule,\n",
    "        gate_temp_schedule=gate_temp_schedule,\n",
    "        gate_reg_schedule=gate_reg_schedule,\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    if post_haystack:\n",
    "        evaluate_haystack(head, steps=50, batch=16, T=256, vocab=1024, tb_step=steps, fast=True)\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# Common settings\n",
    "EXP_STEPS = 100\n",
    "BASE_MIX  = (0.4, 0.2, 0.2, 0.2)\n",
    "SEEDS     = [1337, 2027, 4242]\n",
    "\n",
    "# === E6: gate_temp=0.8 (3 seeds), otherwise baseline ===\n",
    "set_cfg(gate_reg_lambda=getattr(CFG, \"gate_reg_lambda\", 2e-4))  # low-λ default\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E6_temp0p8_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    gate_temp_schedule=[(None, 0.8)])\n",
    "\n",
    "# === E7: memory-leaning warm-start (first 10% steps), then baseline; 3 seeds ===\n",
    "warm_steps = max(50, EXP_STEPS // 10)\n",
    "mix_warm   = (0.3, 0.3, 0.25, 0.15)  # a bit more memory-heavy than baseline\n",
    "mix_main   = BASE_MIX\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E7_warmstart_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    mixture_schedule=[(warm_steps, mix_warm), (None, mix_main)],\n",
    "                    gate_temp_schedule=[(warm_steps, 0.8), (None, 1.0)],\n",
    "                    gate_reg_schedule=[(warm_steps, max(2e-4, getattr(CFG,'gate_reg_lambda', 2e-4))), (None, getattr(CFG,'gate_reg_lambda', 2e-4))])\n",
    "\n",
    "# === E8: capacity sweep N=64 vs N=128 (1 seed each) ===\n",
    "for N_val in (64, 128):\n",
    "    set_cfg(N=N_val)  # assumes your DNC block reads CFG.N at construction\n",
    "    run_one_labeled(f\"E8_capacity_N{N_val}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=777,\n",
    "                    gate_temp_schedule=[(None, getattr(CFG, 'gate_temp', 1.0))])\n",
    "\n",
    "# Reset N if changed\n",
    "set_cfg(N=getattr(CFG, 'N', 128))\n",
    "\n",
    "# === E9: baseline with haystack eval ===\n",
    "run_one_labeled(\"E9_baseline_haystack\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                seed=31415, post_haystack=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T22:56:32.528740100Z",
     "start_time": "2025-08-19T20:16:03.709485900Z"
    }
   },
   "id": "ff510cbc0c8eb5c5"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T23:09:06.964565600Z",
     "start_time": "2025-08-19T23:09:05.911930900Z"
    }
   },
   "id": "823dd4035914de8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard log dump"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c869b656957faac2"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard version: 2.20.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "             label  run_id                                           run_file  \\\n0  E2_gate_reg_low       2  events.out.tfevents.1755631674.Persephone.1949...   \n\n                                               error  \\\n0  b'runs/dncformer-20250817-184608/events.out.tf...   \n\n                                                path        label_cat  \n0  runs/dncformer-20250817-184608/events.out.tfev...  E2_gate_reg_low  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>run_id</th>\n      <th>run_file</th>\n      <th>error</th>\n      <th>path</th>\n      <th>label_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E2_gate_reg_low</td>\n      <td>2</td>\n      <td>events.out.tfevents.1755631674.Persephone.1949...</td>\n      <td>b'runs/dncformer-20250817-184608/events.out.tf...</td>\n      <td>runs/dncformer-20250817-184608/events.out.tfev...</td>\n      <td>E2_gate_reg_low</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: analysis\\run_level_summary.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_194944\\2969953377.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m                 \u001B[0mrows\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m \u001B[1;31m# Build DataFrame and save\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 314\u001B[1;33m df_task_ts = pd.DataFrame(rows).sort_values(\n\u001B[0m\u001B[0;32m    315\u001B[0m     \u001B[1;33m[\u001B[0m\u001B[1;34m\"label\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"task\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"step\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"block\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    316\u001B[0m )\n\u001B[0;32m    317\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_task_ts\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[0m\n\u001B[0;32m   7175\u001B[0m                 \u001B[1;34mf\"Length of ascending ({len(ascending)})\"\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7176\u001B[0m                 \u001B[1;34mf\" != length of by ({len(by)})\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7177\u001B[0m             )\n\u001B[0;32m   7178\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mby\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 7179\u001B[1;33m             \u001B[0mkeys\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mby\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   7180\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7181\u001B[0m             \u001B[1;31m# need to rewrap columns in Series to apply key function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   7182\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m-> 7179\u001B[1;33m         \u001B[1;33m...\u001B[0m     \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex_natsorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"time\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1907\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mxs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mother_axes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1908\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_is_level_reference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1909\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1910\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1911\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1912\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1913\u001B[0m         \u001B[1;31m# Check for duplicates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1914\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# ===== TensorBoard event analyzer for DNCFormer runs (robust) =====\n",
    "# Parses .tfevents files -> summary tables + CSVs\n",
    "import os, math, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Optional: print TB version for debugging\n",
    "try:\n",
    "    import tensorboard as _tb\n",
    "    print(\"TensorBoard version:\", getattr(_tb, \"__version__\", \"unknown\"))\n",
    "except Exception as _e:\n",
    "    print(\"TensorBoard import note:\", _e)\n",
    "\n",
    "# EventAccumulator + module-level constants (version-agnostic handling)\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed in this environment. \"\n",
    "                       \"Install via: pip install tensorboard\") from e\n",
    "\n",
    "# --- Point explicitly to log paths ---\n",
    "EVENT_FILES = [\n",
    "    r\"runs/dncformer-20250817-184608/events.out.tfevents.1755631674.Persephone.194944.2\",\n",
    "]\n",
    "# If you prefer to auto-discover:\n",
    "if not EVENT_FILES:\n",
    "    EVENT_FILES = sorted([str(p) for p in Path(\".\").glob(\"**/events.out.tfevents.*\")])\n",
    "\n",
    "assert EVENT_FILES, \"No event files found. Fill EVENT_FILES or adjust the glob.\"\n",
    "\n",
    "# --------- helpers ---------\n",
    "def _size_guidance_version_safe():\n",
    "    \"\"\"\n",
    "    Build size_guidance dict that works across TB versions:\n",
    "    older/newer TB put constants at module level, not on the class.\n",
    "    \"\"\"\n",
    "    sg = {}\n",
    "    # Try module-level numeric constants first; otherwise fall back to lowercase keys\n",
    "    keys = [\"SCALARS\", \"HISTOGRAMS\", \"IMAGES\", \"COMPRESSED_HISTOGRAMS\", \"AUDIO\"]\n",
    "    for k in keys:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            # event_accumulator also accepts string keys like \"scalars\"\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "def load_scalars(ev_path: str):\n",
    "    acc = EventAccumulator(ev_path, size_guidance=_size_guidance_version_safe())\n",
    "    acc.Reload()\n",
    "    # acc.Tags() returns dict with lowercase keys in modern TB\n",
    "    tags = acc.Tags()\n",
    "    scalar_tags = tags.get('scalars', [])\n",
    "    out = {}\n",
    "    for tag in scalar_tags:\n",
    "        vals = acc.Scalars(tag)\n",
    "        out[tag] = [(x.step, float(x.value)) for x in vals]\n",
    "    return out\n",
    "\n",
    "def s_last(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[-1])\n",
    "    return float(np.nanmean(arr[-k:]))\n",
    "\n",
    "def s_first(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[0])\n",
    "    return float(np.nanmean(arr[:k]))\n",
    "\n",
    "def s_mean(vals):\n",
    "    if not vals: return np.nan\n",
    "    return float(np.nanmean([v for _, v in vals]))\n",
    "\n",
    "def s_count(vals):\n",
    "    return len(vals) if vals else 0\n",
    "\n",
    "TASKS  = [\"hf\", \"copy\", \"repeat\", \"nback\"]\n",
    "BLOCKS = [0, 1]\n",
    "\n",
    "def summarize_run(ev_path: str):\n",
    "    scal = load_scalars(ev_path)\n",
    "\n",
    "    # basics\n",
    "    loss = scal.get(\"train/loss\", [])\n",
    "    lr   = scal.get(\"train/lr\", [])\n",
    "    steps_logged = max([s for s, _ in loss], default=np.nan) if loss else np.nan\n",
    "    loss0 = s_first(loss, k=5)\n",
    "    lossT = s_last(loss,  k=10)\n",
    "    loss_delta = (loss0 - lossT) if not any(map(math.isnan, [loss0, lossT])) else np.nan\n",
    "    lr_last = s_last(lr, k=1)\n",
    "\n",
    "    # gates (global)\n",
    "    g_means, g_entropy = {}, {}\n",
    "    g_frac_avg = {}\n",
    "    for b in BLOCKS:\n",
    "        g_means[b]   = s_last(scal.get(f\"gates/block_{b}_mean\", []), k=10)\n",
    "        g_entropy[b] = s_last(scal.get(f\"gates/block_{b}_entropy\", []), k=10)\n",
    "        fracs = []\n",
    "        for t in TASKS:\n",
    "            tag = f\"gates_by_task/block_{b}_frac>0.5/{t}\"\n",
    "            if tag in scal:\n",
    "                fracs.append(s_last(scal[tag], k=10))\n",
    "        g_frac_avg[b] = float(np.nanmean(fracs)) if fracs else np.nan\n",
    "\n",
    "    # per-task losses (mean of last quarter)\n",
    "    task_loss_last, task_counts = {}, {}\n",
    "    for t in TASKS:\n",
    "        ts = scal.get(f\"loss_by_task/{t}\", [])\n",
    "        task_counts[t] = s_count(ts)\n",
    "        if ts:\n",
    "            k = max(1, len(ts)//4)\n",
    "            task_loss_last[t] = s_last(ts, k=k)\n",
    "        else:\n",
    "            task_loss_last[t] = np.nan\n",
    "\n",
    "    # per-task gate means (avg last quarter)\n",
    "    task_gmeans = {t: {} for t in TASKS}\n",
    "    for t in TASKS:\n",
    "        for b in BLOCKS:\n",
    "            ts = scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])\n",
    "            if ts:\n",
    "                k = max(1, len(ts)//4)\n",
    "                task_gmeans[t][b] = s_last(ts, k=k)\n",
    "            else:\n",
    "                task_gmeans[t][b] = np.nan\n",
    "\n",
    "    # quartiles (block0 Q1..Q4) averaged across tasks if present\n",
    "    q_means = {}\n",
    "    for qi in range(1,5):\n",
    "        vals = []\n",
    "        for t in TASKS:\n",
    "            tagt = f\"gates/block0_q{qi}_mean/{t}\"\n",
    "            if tagt in scal:\n",
    "                vals.append(s_last(scal[tagt], k=10))\n",
    "        q_means[qi] = float(np.nanmean(vals)) if vals else np.nan\n",
    "\n",
    "    # Try to infer forced gating\n",
    "    forced_guess = None\n",
    "    gm_all = [g_means[b] for b in BLOCKS if not math.isnan(g_means[b])]\n",
    "    if gm_all:\n",
    "        m = float(np.nanmean(gm_all))\n",
    "        if m < 0.02:  forced_guess = \"force_g=0\"\n",
    "        elif m > 0.98: forced_guess = \"force_g=1\"\n",
    "\n",
    "    # run_id default (may be overridden in assembly)\n",
    "    suffix = Path(ev_path).suffix.lstrip(\".\")\n",
    "    run_id = suffix if suffix.isdigit() else Path(ev_path).name\n",
    "\n",
    "    summary = {\n",
    "        \"run_file\": Path(ev_path).name,\n",
    "        \"run_id\": run_id,\n",
    "        \"steps_logged\": steps_logged,\n",
    "        \"loss_start~5\": loss0,\n",
    "        \"loss_end~10\": lossT,\n",
    "        \"loss_delta\": loss_delta,\n",
    "        \"lr_last\": lr_last,\n",
    "        \"g_mean_b0\": g_means.get(0, np.nan),\n",
    "        \"g_mean_b1\": g_means.get(1, np.nan),\n",
    "        \"g_frac>0.5_avg\": float(np.nanmean([g_frac_avg.get(0, np.nan), g_frac_avg.get(1, np.nan)])),\n",
    "        \"g_entropy_b0\": g_entropy.get(0, np.nan),\n",
    "        \"g_entropy_b1\": g_entropy.get(1, np.nan),\n",
    "        \"forced_guess\": forced_guess,\n",
    "        **{f\"loss_{t}_last\": task_loss_last[t] for t in TASKS},\n",
    "        **{f\"gmean_b0_{t}\": task_gmeans[t][0] for t in TASKS},\n",
    "        **{f\"gmean_b1_{t}\": task_gmeans[t][1] for t in TASKS},\n",
    "        **{f\"g_b0_Q{qi}_mean\": q_means[qi] for qi in range(1,5)},\n",
    "        **{f\"count_{t}\": task_counts[t] for t in TASKS},\n",
    "    }\n",
    "    return summary, scal\n",
    "\n",
    "def parse_suffix_as_int(ev_path: str):\n",
    "    suf = Path(ev_path).suffix.lstrip(\".\")\n",
    "    return int(suf) if suf.isdigit() else None\n",
    "\n",
    "# Map numeric suffix -> experiment label\n",
    "RUN_LABELS = {\n",
    "    0: \"E0_baseline\",\n",
    "    1: \"E1_memory_lean\",\n",
    "    2: \"E2_gate_reg_low\",\n",
    "    3: \"E3_gate_reg_high\",\n",
    "    4: \"E4_gate_temp_0p7\",\n",
    "    5: \"E5a_force_g_0\",\n",
    "    6: \"E5b_force_g_1\",\n",
    "}\n",
    "\n",
    "# --------- robust assembly & labeling ----------\n",
    "summaries = []\n",
    "all_scalars = {}\n",
    "for p in EVENT_FILES:\n",
    "    fn = Path(p).name\n",
    "    try:\n",
    "        s, scal = summarize_run(p)\n",
    "        if \"run_id\" not in s or (s.get(\"run_id\") in (None, \"\", np.nan)):\n",
    "            suf_i = parse_suffix_as_int(p)\n",
    "            s[\"run_id\"] = suf_i if suf_i is not None else fn\n",
    "        suf_i = parse_suffix_as_int(p)\n",
    "        s[\"label\"] = RUN_LABELS.get(suf_i, s[\"run_id\"])\n",
    "        s[\"path\"] = str(p)\n",
    "        summaries.append(s)\n",
    "        all_scalars[fn] = scal\n",
    "    except Exception as e:\n",
    "        suf_i = parse_suffix_as_int(p)\n",
    "        summaries.append({\n",
    "            \"run_file\": fn,\n",
    "            \"run_id\": suf_i if suf_i is not None else fn,\n",
    "            \"label\": RUN_LABELS.get(suf_i, f\"unparsed_{fn}\"),\n",
    "            \"error\": str(e),\n",
    "            \"path\": str(p),\n",
    "        })\n",
    "        all_scalars[fn] = {}\n",
    "\n",
    "df_runs = pd.DataFrame(summaries)\n",
    "\n",
    "# Order by planned experiment sequence\n",
    "if \"label\" in df_runs.columns:\n",
    "    order = [\"E0_baseline\",\"E1_memory_lean\",\"E2_gate_reg_low\",\"E3_gate_reg_high\",\n",
    "             \"E4_gate_temp_0p7\",\"E5a_force_g_0\",\"E5b_force_g_1\"]\n",
    "    df_runs[\"label_cat\"] = pd.Categorical(df_runs[\"label\"], categories=order, ordered=True)\n",
    "    df_runs = df_runs.sort_values([\"label_cat\",\"run_id\"], ignore_index=True)\n",
    "elif \"run_id\" in df_runs.columns:\n",
    "    df_runs = df_runs.sort_values(\"run_id\", key=lambda s: s.astype(str), ignore_index=True)\n",
    "\n",
    "# Arrange columns with most relevant up front\n",
    "front_cols = [c for c in [\n",
    "    \"label\", \"run_id\", \"run_file\", \"steps_logged\",\n",
    "    \"loss_start~5\",\"loss_end~10\",\"loss_delta\",\"lr_last\",\n",
    "    \"g_mean_b0\",\"g_mean_b1\",\"g_frac>0.5_avg\",\"g_entropy_b0\",\"g_entropy_b1\",\n",
    "    \"loss_hf_last\",\"loss_copy_last\",\"loss_repeat_last\",\"loss_nback_last\",\n",
    "    \"gmean_b0_hf\",\"gmean_b1_hf\",\"gmean_b0_copy\",\"gmean_b1_copy\",\n",
    "    \"gmean_b0_repeat\",\"gmean_b1_repeat\",\"gmean_b0_nback\",\"gmean_b1_nback\",\n",
    "    \"g_b0_Q1_mean\",\"g_b0_Q2_mean\",\"g_b0_Q3_mean\",\"g_b0_Q4_mean\",\n",
    "    \"forced_guess\",\"error\",\"path\"\n",
    "] if c in df_runs.columns]\n",
    "df_runs = df_runs[[*front_cols, *[c for c in df_runs.columns if c not in front_cols]]]\n",
    "\n",
    "display(df_runs)\n",
    "\n",
    "# Save CSV\n",
    "out_dir = Path(\"./analysis\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "runs_csv = out_dir / \"run_level_summary.csv\"\n",
    "df_runs.to_csv(runs_csv, index=False)\n",
    "print(\"Saved:\", runs_csv)\n",
    "\n",
    "# --------- per‑task time series (granular export; replaces tidy summary block) ----------\n",
    "rows = []\n",
    "for ev, scal in all_scalars.items():\n",
    "    # derive run label from suffix (0..6) using our RUN_LABELS map\n",
    "    suf_i = parse_suffix_as_int(ev)\n",
    "    run_label = RUN_LABELS.get(suf_i, ev)\n",
    "    run_id_str = RUN_LABELS.get(suf_i, str(suf_i) if suf_i is not None else ev)\n",
    "\n",
    "    # join LR by global training step (helps analyze scheduler effects)\n",
    "    lr_dict = {int(s): float(v) for s, v in scal.get(\"train/lr\", [])}\n",
    "\n",
    "    for t in TASKS:\n",
    "        # per-task loss series: list[(step, value)], logged at your log_every cadence\n",
    "        loss_series = scal.get(f\"loss_by_task/{t}\", [])\n",
    "        if not loss_series:\n",
    "            continue\n",
    "\n",
    "        # per-task gate metrics per block, keyed by step\n",
    "        gmean_dict = {\n",
    "            b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])}\n",
    "            for b in BLOCKS\n",
    "        }\n",
    "        gfrac_dict = {\n",
    "            b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_frac>0.5/{t}\", [])}\n",
    "            for b in BLOCKS\n",
    "        }\n",
    "\n",
    "        # optional quartile gate means for block 0 (if logged)\n",
    "        q_dict = {\n",
    "            qi: {int(s): float(v) for s, v in scal.get(f\"gates/block0_q{qi}_mean/{t}\", [])}\n",
    "            for qi in (1, 2, 3, 4)\n",
    "        }\n",
    "\n",
    "        for step, loss_val in loss_series:\n",
    "            step = int(step)\n",
    "            loss_val = float(loss_val)\n",
    "\n",
    "            for b in BLOCKS:\n",
    "                g_mean = gmean_dict[b].get(step, np.nan)\n",
    "                g_frac = gfrac_dict[b].get(step, np.nan)\n",
    "                row = {\n",
    "                    \"label\": run_label,\n",
    "                    \"run_id\": run_id_str,\n",
    "                    \"step\": step,\n",
    "                    \"task\": t,\n",
    "                    \"block\": b,\n",
    "                    \"loss\": loss_val,\n",
    "                    \"g_mean\": g_mean,\n",
    "                    \"g_frac>0.5\": g_frac,\n",
    "                    \"lr\": lr_dict.get(step, np.nan),\n",
    "                }\n",
    "                # add quartile gating only for block 0; keep NaN for block 1\n",
    "                if b == 0:\n",
    "                    for qi in (1, 2, 3, 4):\n",
    "                        row[f\"g_b0_Q{qi}\"] = q_dict[qi].get(step, np.nan)\n",
    "                else:\n",
    "                    for qi in (1, 2, 3, 4):\n",
    "                        row[f\"g_b0_Q{qi}\"] = np.nan\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "# Build DataFrame and save\n",
    "df_task_ts = pd.DataFrame(rows).sort_values(\n",
    "    [\"label\", \"task\", \"step\", \"block\"], ignore_index=True\n",
    ")\n",
    "display(df_task_ts.head(20))\n",
    "\n",
    "# Backwards-compat alias (if you referenced df_task elsewhere)\n",
    "df_task = df_task_ts.copy()\n",
    "\n",
    "tasks_csv = out_dir / \"per_task_metrics.csv\"\n",
    "df_task_ts.to_csv(tasks_csv, index=False)\n",
    "print(\"Saved (granular):\", tasks_csv)\n",
    "\n",
    "\n",
    "df_task = pd.DataFrame(rows)\n",
    "if not df_task.empty:\n",
    "    df_task = df_task.sort_values([\"label\",\"task\",\"block\"], ignore_index=True)\n",
    "display(df_task)\n",
    "\n",
    "tasks_csv = out_dir / \"per_task_metrics.csv\"\n",
    "df_task.to_csv(tasks_csv, index=False)\n",
    "print(\"Saved:\", tasks_csv)\n",
    "\n",
    "print(\"Analyzed files:\")\n",
    "for p in EVENT_FILES:\n",
    "    print(\" -\", p)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-19T23:07:33.137811200Z",
     "start_time": "2025-08-19T23:07:32.984812200Z"
    }
   },
   "id": "f354871dcf359754"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 12. Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask. A step-wise cached mode can be added for streaming scenarios.\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks. You can plug in larger corpora or evaluation suites later.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984435d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-17T19:09:55.631811500Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "\n",
    "print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "_run(\"conda env export --from-history > environment.yml\")\n",
    "_run(\"conda env export > environment.lock.yml\")\n",
    "_run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
