{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "This notebook implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:38.809430300Z",
     "start_time": "2025-08-17T07:27:38.803430200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n",
    "\n",
    "# SDPA configured via SDPA_CTX() above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7891e",
   "metadata": {},
   "source": [
    "## 1. Config & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:39.218432500Z",
     "start_time": "2025-08-17T07:27:39.198432100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n",
      "AMP dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "import os, math, random, torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "print(\"AMP dtype:\", amp_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:39.403430900Z",
     "start_time": "2025-08-17T07:27:39.394430500Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- SDPA selection (prefer PyTorch SDPA; avoid flash-attn) ---\n",
    "import contextlib\n",
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:31:00.364547100Z",
     "start_time": "2025-08-17T07:31:00.355547100Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n",
    "\n",
    "def get_or_build_model_and_tokenizer():\n",
    "    mid = CFG.base_model_id\n",
    "    if _GLOBAL.get(\"head\") is not None and _GLOBAL.get(\"mid\") == mid:\n",
    "        # Reuse the already-loaded GPU model\n",
    "        return _GLOBAL[\"tok\"], _GLOBAL[\"head\"]\n",
    "    # Otherwise build fresh\n",
    "    tok, base = load_base_model(mid)             # your existing GPU-only loader\n",
    "    head = DNCFormerHead(base, CFG).to(device)   # enrichment head on CUDA\n",
    "    _GLOBAL.update({\"tok\": tok, \"head\": head, \"mid\": mid})\n",
    "    return tok, head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 3. DNC Memory (compact reference implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:40.221430200Z",
     "start_time": "2025-08-17T07:27:40.188429900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe({\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1),  # (B,)\n",
    "            })\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "## 4. Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:40.628431300Z",
     "start_time": "2025-08-17T07:27:40.606432300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "## 5. DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:41.033432600Z",
     "start_time": "2025-08-17T07:27:41.017430300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "## 6. Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:41.455432700Z",
     "start_time": "2025-08-17T07:27:41.447431500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        return out, dnc_state, g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "## 7. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:41.876432500Z",
     "start_time": "2025-08-17T07:27:41.864433Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def spda_ctx():\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception as e:\n",
    "        return f\"error {e} encountered ->\\n{contextlib.nullcontext()}\"\n",
    "\n",
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with spda_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 8. Data: Synthetic tasks + simple instruction-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:42.294431900Z",
     "start_time": "2025-08-17T07:27:42.286431900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0322be9",
   "metadata": {},
   "source": [
    "## 9. Training loop (lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:49:06.413790200Z",
     "start_time": "2025-08-17T07:49:06.386791200Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Synthetic generators + HF dataset integration + MixtureSampler ---\n",
    "import random, torch\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    For each row, sample a short sequence (length ~ T//2), sample a scalar repeat factor r in [repeat_min, repeat_max],\n",
    "    and repeat every token in that row by r. Truncate/pad to length T with pad_id.\n",
    "    \"\"\"\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))  # per-row scalar\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n",
    "\n",
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception as e:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\")\n",
    "        out = ex.get(o_field, \"\")\n",
    "        if instr and out:\n",
    "            pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items:\n",
    "            break\n",
    "    random.shuffle(pairs)\n",
    "    return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]\n",
    "        out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n",
    "\n",
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float]):\n",
    "        self.gens = gens\n",
    "        import torch\n",
    "        self.p = torch.tensor(weights, dtype=torch.float)\n",
    "        self.p /= self.p.sum()\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch\n",
    "        idx = torch.multinomial(self.p, 1).item()\n",
    "        return self.gens[idx](batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6be4ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:43.750467600Z",
     "start_time": "2025-08-17T07:27:43.740467800Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44d73c717b8b58c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:44.944504700Z",
     "start_time": "2025-08-17T07:27:44.765503500Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- TensorBoard Logger (lightweight) ---\n",
    "import os, time\n",
    "from typing import List, Optional\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        if not self.enabled:\n",
    "            self.writer = None\n",
    "            return\n",
    "        if logdir is None:\n",
    "            logdir = \"./runs\"\n",
    "        if run_name is None:\n",
    "            run_name = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not self.enabled: return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not self.enabled: return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a70a7bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:45.967503500Z",
     "start_time": "2025-08-17T07:27:45.960504700Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(head, tok):\n",
    "    head.eval()\n",
    "    prompt = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\"\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(enc.input_ids)\n",
    "    print(\"Gates (mean):\", [g.mean().item() for g in gates])\n",
    "    return gates\n",
    "\n",
    "def train_small():\n",
    "    tok, head = get_or_build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, CFG.warmup_steps, CFG.train_steps, min_lr_ratio=0.10)\n",
    "    head.train()\n",
    "\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    for step in range(1, CFG.train_steps+1):\n",
    "        in_ids, lab_ids = tokenize_instruction_pairs(tok, INSTR_PAIRS, max_len=min(CFG.max_seq_len, 256))\n",
    "        in_ids = in_ids.to(device)\n",
    "        \n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            logits, gates = head(in_ids)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step()\n",
    "            \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % CFG.log_every == 0:\n",
    "            gm = [g.mean().item() for g in gates]\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e} | gates={gm}\")\n",
    "            tb.log_scalars(step, float(loss.item()), float(lr), gm)\n",
    "            \n",
    "    evaluate_simple(head, tok)\n",
    "    return head, tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9ee90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 10. Eval harness (needle-in-haystack, copy, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028e39a3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:27:47.271531900Z",
     "start_time": "2025-08-17T07:27:47.263534800Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Memory tracer & TensorBoard visualizer ---\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU allocation sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd62ef5c5785017b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# --- GPU VRAM diagnostics ---\n",
    "import torch, gc, contextlib\n",
    "\n",
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "\n",
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-17T07:27:48.535533300Z"
    }
   },
   "id": "2af2ed601f9c7606"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n",
      "[liveness] DNCFormerHead instances alive: 0\n",
      "[liveness] no global 'head'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_116152\\2359170240.py:15: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n"
     ]
    }
   ],
   "source": [
    "cuda_report(\"before\")\n",
    "list_head_refs()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-17T07:27:49.738580900Z"
    }
   },
   "id": "9dc850ece5144e62"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# generator smoke test\u001B[39;00m\n\u001B[0;32m      2\u001B[0m mx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(CFG, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_seq_len\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m256\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m tok, _base \u001B[38;5;241m=\u001B[39m tok \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtok\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m() \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m _pad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(tok, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpad_token_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m tok \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      6\u001B[0m x \u001B[38;5;241m=\u001B[39m make_repeat_copy(batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, T\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmin\u001B[39m(mx, \u001B[38;5;241m128\u001B[39m), vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, pad_id\u001B[38;5;241m=\u001B[39m_pad)\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# generator smoke test\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:54:35.967338400Z",
     "start_time": "2025-08-17T07:54:35.938339400Z"
    }
   },
   "id": "be0b78c71554906c"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c91532fe61141fe95a4b6c998302d0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=17.43 GB | reserved=17.46 GB | free=6.91 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:54:53.081010500Z",
     "start_time": "2025-08-17T07:54:48.882010900Z"
    }
   },
   "id": "4f0be150819cf745"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medium-size training loop test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597e27677e1220b4"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000):\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "    \n",
    "    pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\",\"output\"), max_items=hf_max_items)\n",
    "    gens = []; wts = []\n",
    "    if pairs:\n",
    "        def gen_hf(b): return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "        gens.append(gen_hf); wts.append(weights[0])\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        print(\"HF dataset unavailable or empty; using synthetic only.\")\n",
    "        s_w = [1.0, 1.0, 1.0]\n",
    "        \n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "    \n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback]); wts.extend(s_w)\n",
    "    return MixtureSampler(gens, wts)\n",
    "\n",
    "def train_medium(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.10,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 2000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = True,\n",
    "    viz_prompt: str = \"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "):\n",
    "    cfg = CFG\n",
    "    clip = float(getattr(CFG, \"grad_clip\", 1.0))\n",
    "    steps = int(steps if steps is not None else getattr(cfg, \"train_steps\", 200))\n",
    "    batch_size = int(batch_size if batch_size is not None else getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "\n",
    "    tok, head = get_or_build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "    head.train()\n",
    "\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "    use_scaler = (amp_dtype == torch.float16)\n",
    "    scaler = GradScaler('cuda', enabled=use_scaler)\n",
    "\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    for step in range(1, steps+1):\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            logits, gates = head(in_ids)\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), clip)\n",
    "            scaler.step(optim); scaler.update(); scheduler.step()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), clip)\n",
    "            optim.step(); scheduler.step()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            gm = [g.mean().item() for g in gates]\n",
    "            lr = optim.param_groups[0]['lr']\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e} | gates={gm}\")\n",
    "            tb.log_scalars(step, float(loss.item()), float(lr), gm)\n",
    "\n",
    "    if viz_memory_after and TB_AVAILABLE:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "\n",
    "    tb.flush(); tb.close()\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:54:57.025117100Z",
     "start_time": "2025-08-17T07:54:57.007117600Z"
    }
   },
   "id": "d629080a"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T07:59:36.633649Z",
     "start_time": "2025-08-17T07:59:36.127649300Z"
    }
   },
   "id": "a83b2d08c4a137e7"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80e53881",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T08:02:13.030232Z",
     "start_time": "2025-08-17T08:02:13.020234700Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db24ba31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:02:15.488286700Z",
     "start_time": "2025-08-17T08:02:15.464287400Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Evaluator unit tests (no heavy model) ---\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3036ac",
   "metadata": {},
   "source": [
    "## 11. Unit-like tests (sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cb70f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:02:25.536025200Z",
     "start_time": "2025-08-17T08:02:25.524026500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24f2641cb05940",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Unit tests/basic sanity checks\n",
    "currently all passing, but self-reminder here to comment out if architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d842ad22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:02:30.311131800Z",
     "start_time": "2025-08-17T08:02:30.297130900Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Smoke tests ---\n",
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aac46d77e635252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.089300Z",
     "start_time": "2025-08-12T07:20:32.028131900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run_unit_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "480f5d04e5aaed2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:20:32.090300500Z",
     "start_time": "2025-08-12T07:20:32.043300800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy acc: 0.0 gates: [0.495686799287796, 0.508457362651825]\n",
      "reverse acc: 0.0 gates: [0.5013509392738342, 0.5040897130966187]\n",
      "needle acc: 0.0 gates: [0.49810758233070374, 0.4982784688472748]\n",
      "Evaluator unit tests passed.\n"
     ]
    }
   ],
   "source": [
    "#run_eval_unit_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc88e726027b34",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e3f96209f238d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training run Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b72c8e3e4a7e3237",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-17T08:03:35.832511800Z",
     "start_time": "2025-08-17T08:03:35.817513400Z"
    }
   },
   "outputs": [],
   "source": [
    "# small training test\n",
    "#head, tok = train_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "[snapshot: before train_medium] alloc=9.79 GB | reserved=9.82 GB | free=14.55 GB | total=25.77 GB\n",
      "step 10 | loss 21.3759 | lr 2.00e-04 | gates=[0.3125, 0.30078125]\n",
      "step 20 | loss 4.3655 | lr 1.93e-04 | gates=[0.166015625, 0.23828125]\n",
      "step 30 | loss 6.1520 | lr 1.74e-04 | gates=[0.140625, 0.2216796875]\n",
      "step 40 | loss 10.5862 | lr 1.47e-04 | gates=[0.2119140625, 0.34375]\n",
      "step 50 | loss 11.6569 | lr 1.14e-04 | gates=[0.2158203125, 0.384765625]\n",
      "step 60 | loss 2.0442 | lr 7.92e-05 | gates=[0.06298828125, 0.2216796875]\n",
      "step 70 | loss 8.4067 | lr 4.70e-05 | gates=[0.1298828125, 0.376953125]\n",
      "step 80 | loss 2.0727 | lr 2.12e-05 | gates=[0.0576171875, 0.21484375]\n",
      "step 90 | loss 1.8196 | lr 2.00e-05 | gates=[0.0498046875, 0.2158203125]\n",
      "step 100 | loss 6.1017 | lr 2.00e-05 | gates=[0.1435546875, 0.42578125]\n"
     ]
    }
   ],
   "source": [
    "# medium training test\n",
    "free_head_and_cache()\n",
    "cuda_report(\"snapshot: before train_medium\")\n",
    "head, tok = train_medium(steps=100, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "# Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-17T07:54:57.954117200Z"
    }
   },
   "id": "7a9cc2ad"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 12. Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask. A step-wise cached mode can be added for streaming scenarios.\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks. You can plug in larger corpora or evaluation suites later.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984435d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "\n",
    "print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "_run(\"conda env export --from-history > environment.yml\")\n",
    "_run(\"conda env export > environment.lock.yml\")\n",
    "_run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
