{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "Implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, config, and environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f6d41fd5a7206b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.533514700Z",
     "start_time": "2025-08-20T16:32:38.873515800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "import os, math, random, time, numpy as np\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import contextlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.550515200Z",
     "start_time": "2025-08-20T16:32:38.882515800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n",
      "AMP dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "print(\"AMP dtype:\", amp_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# --- Config patch toggles ---\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class _Tmp: pass\n",
    "    CFG = _Tmp()\n",
    "# safe defaults if not already present\n",
    "if not hasattr(CFG, 'batch_size'): CFG.batch_size = 8\n",
    "if not hasattr(CFG, 'gate_reg_lambda'): CFG.gate_reg_lambda = 0.0   # only applied on memory-tagged batches\n",
    "if not hasattr(CFG, 'hist_every'): CFG.hist_every = 200             # histogram cadence\n",
    "if not hasattr(CFG, 'force_g'): CFG.force_g = None                  # None, or 0.0 or 1.0\n",
    "if not hasattr(CFG, 'gate_temp'): CFG.gate_temp = 1.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.569515600Z",
     "start_time": "2025-08-20T16:32:38.900517700Z"
    }
   },
   "id": "682d4e57"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.584514800Z",
     "start_time": "2025-08-20T16:32:38.915519900Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- SDPA selection (prefer PyTorch SDPA; avoid flash-attn) ---\n",
    "\n",
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 1. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Model Information and factory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a83e8350cc6d34f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.590514800Z",
     "start_time": "2025-08-20T16:32:38.933515200Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Gate metrics utils (mean, frac>0.5, entropy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f19b2569a5560d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def _reduce_gate_tensor(g: torch.Tensor) -> torch.Tensor:\n",
    "    # g: (B,T,*) -> (B,T)\n",
    "    if g.dim() == 3:\n",
    "        return g.mean(dim=-1)\n",
    "    return g\n",
    "\n",
    "@torch.no_grad()\n",
    "def _gate_metrics(g: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns scalar (mean, frac>0.5, entropy)\n",
    "    \"\"\"\n",
    "    g2 = _reduce_gate_tensor(g.detach())\n",
    "    mean_val = float(g2.mean().item())\n",
    "    frac = float((g2 > 0.5).float().mean().item())\n",
    "    eps = 1e-6\n",
    "    p = g2.clamp(eps, 1 - eps)\n",
    "    # binary entropy (natural log base)\n",
    "    ent = float((-(p * (p + eps).log() + (1 - p) * (1 - p + eps).log())).mean().item())\n",
    "    return mean_val, frac, ent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.608516500Z",
     "start_time": "2025-08-20T16:32:38.947522400Z"
    }
   },
   "id": "dad9cdfcce225bcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Memory Freeing/handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfb25b111aaa708"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.611514600Z",
     "start_time": "2025-08-20T16:32:38.963518900Z"
    }
   },
   "id": "228ec3411b7b0b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Environment Export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f294b5269755df"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "        \n",
    "def export_current_env():\n",
    "    print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "    _run(\"conda env export --from-history > environment.yml\")\n",
    "    _run(\"conda env export > environment.lock.yml\")\n",
    "    _run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "    print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.614515200Z",
     "start_time": "2025-08-20T16:32:38.978519700Z"
    }
   },
   "id": "f14fc6a99cc0d7e8"
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1: DNC Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61be1839c9ebb66f"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.629514700Z",
     "start_time": "2025-08-20T16:32:38.995517200Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        # aggregated lightweight stats (per-step)\n",
    "        try:\n",
    "            _stats = {}\n",
    "            with torch.no_grad():\n",
    "                _stats[\"u_mean\"] = state[\"u\"].mean().detach()\n",
    "                try:\n",
    "                    _stats[\"M_norm_mean\"] = state[\"M\"].norm(dim=-1).mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"rw_max_mean\"] = rw.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"ww_max_mean\"] = ww.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            state[\"stats\"] = _stats\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe(\n",
    "                {\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1), # (B,)\n",
    "                }\n",
    "            )\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "#### 2.2 Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.632514800Z",
     "start_time": "2025-08-20T16:32:39.011516100Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "#### 2.3 DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.634514700Z",
     "start_time": "2025-08-20T16:32:39.026516500Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "#### 2.4 Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.649515100Z",
     "start_time": "2025-08-20T16:32:39.043516300Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None, gate_override: None = None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None, gate_override: None = None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        \n",
    "        # collect per-block metrics if requested\n",
    "        if self.collect_metrics:\n",
    "            try:\n",
    "                import math, torch as _t\n",
    "                eps = 1e-6\n",
    "                g_clamp = g.clamp(min=eps, max=1-eps)\n",
    "                g_entropy = (-(g_clamp*_t.log(g_clamp) + (1-g_clamp)*_t.log(1-g_clamp))).mean()\n",
    "                vt_norm = vt.norm(dim=-1).mean()\n",
    "                dt_norm = dt.norm(dim=-1).mean()\n",
    "                _stats = dnc_state.get(\"stats\", {}) if isinstance(dnc_state, dict) else {}\n",
    "                # ensure CPU-detached tiny tensors\n",
    "                self.last_metrics = {\n",
    "                    \"g_mean\": g.mean().detach(),\n",
    "                    \"g_entropy\": g_entropy.detach(),\n",
    "                    \"vt_norm\": vt_norm.detach(),\n",
    "                    \"dt_norm\": dt_norm.detach(),\n",
    "                    **_stats\n",
    "                }\n",
    "            except Exception:\n",
    "                self.last_metrics = None\n",
    "        else:\n",
    "            self.last_metrics = None\n",
    "            return out, dnc_state, g \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "#### 2.5. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.649515100Z",
     "start_time": "2025-08-20T16:32:39.058515100Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n",
    "\n",
    "\n",
    "\n",
    "    def forward_with_metrics(self, input_ids: torch.Tensor, attention_mask: \"Optional[torch.Tensor]\" = None,\n",
    "                             gate_override: \"Optional[float]\" = None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1].to(device)  # ensure CUDA\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates_det = []\n",
    "        gates_raw = []\n",
    "        per_block = []\n",
    "        # enable metrics collection per block\n",
    "        for blk in self.blocks:\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = True\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i], gate_override=gate_override)\n",
    "            gates_raw.append(g)\n",
    "            gates_det.append(g.detach())\n",
    "            if hasattr(blk, \"last_metrics\") and blk.last_metrics is not None:\n",
    "                per_block.append(blk.last_metrics)\n",
    "            else:\n",
    "                per_block.append({})\n",
    "            # reset flag to avoid overhead elsewhere\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = False\n",
    "        # lm head on its own device, then back to CUDA\n",
    "        lm_dev = self.base.lm_head.weight.device\n",
    "        y = self.proj_out(h).to(lm_dev, dtype=self.base.lm_head.weight.dtype)\n",
    "        logits = self.base.lm_head(y).to(device)\n",
    "        aux = {\"per_block\": per_block, \"gates_raw\": gates_raw, \"gates_detached\": gates_det}\n",
    "        aux[\"g_entropy_block\"] = []\n",
    "        for g in gates_det:\n",
    "            _, _, ent = _gate_metrics(g)\n",
    "            aux[\"g_entropy_block\"].append(ent)\n",
    "        return logits, gates_det, aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 3. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Synthetic tasks + simple instruction-following"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372040027d86e088"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.652514700Z",
     "start_time": "2025-08-20T16:32:39.074514900Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 HF dataset integration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eaf6ff06d518fec"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.666516400Z",
     "start_time": "2025-08-20T16:32:39.090518400Z"
    }
   },
   "outputs": [],
   "source": [
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\"); out = ex.get(o_field, \"\")\n",
    "        if instr and out: pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items: break\n",
    "    random.shuffle(pairs); return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]; out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Haystack batch creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20de8c6a7c630b74"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# --- Haystack (needle) eval: long-span retrieval of a key's value ---\n",
    "def make_haystack_batch(batch: int, T: int = 256, vocab: int = 1024, sentinel: int = 3):\n",
    "    \"\"\"\n",
    "    Build sequences: ... K V ... K ?  (next token should be V)\n",
    "    Returns: input_ids [B,T], answer_ids [B], query_pos [B] (position of '?')\n",
    "    \"\"\"\n",
    "    assert T >= 12, \"T too small for haystack layout\"\n",
    "    x = torch.randint(5, vocab, (batch, T), dtype=torch.long)\n",
    "    K = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "    V = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "\n",
    "    # Place (K,V) in the first half\n",
    "    p1 = torch.randint(low=T//8, high=T//2 - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p1] = K\n",
    "    x[torch.arange(batch), p1 + 1] = V\n",
    "\n",
    "    # Place (K, sentinel) in the last quarter\n",
    "    p2 = torch.randint(low=3*T//4, high=T - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p2] = K\n",
    "    x[torch.arange(batch), p2 + 1] = sentinel  # '?'\n",
    "    query_pos = p2 + 1  # position of '?'; we will look at logits at this position\n",
    "\n",
    "    return x, V, query_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.680514900Z",
     "start_time": "2025-08-20T16:32:39.105517100Z"
    }
   },
   "id": "3ef03e6eaa8a61bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4 MixtureSampler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b4f90ad9bf9b7"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float], names: Optional[List[str]] = None):\n",
    "        self.gens = gens\n",
    "        import torch as _t\n",
    "        self.weights = list(map(float, weights))\n",
    "        self.p = _t.tensor(self.weights, dtype=_t.float32)  # CPU is fine for multinomial\n",
    "        self.p /= (self.p.sum() + 1e-8)\n",
    "        self.names = names if names is not None else [f\"g{i}\" for i in range(len(gens))]\n",
    "        self.last_name = None\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch as _t\n",
    "        idx = _t.multinomial(self.p, 1).item()\n",
    "        self.last_name = self.names[idx]\n",
    "        return self.gens[idx](batch)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Update mixture probabilities at runtime (used by schedules).\"\"\"\n",
    "        import torch as _t\n",
    "        ws = list(map(float, weights))\n",
    "        t = _t.tensor(ws, dtype=_t.float32, device=self.p.device)\n",
    "        s = float(t.sum().item())\n",
    "        if s <= 0:\n",
    "            raise ValueError(\"Mixture weights must sum to > 0\")\n",
    "        self.p = t / s\n",
    "        self.weights = ws\n",
    "        # Optional sanity: warn if names length mismatches weights\n",
    "        if hasattr(self, \"names\") and len(self.names) != len(ws):\n",
    "            print(f\"[MixtureSampler] Warning: len(names)={len(self.names)} != len(weights)={len(ws)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.686514600Z",
     "start_time": "2025-08-20T16:32:39.123516600Z"
    }
   },
   "id": "c47d40c7b9fdc5ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.5 Mixture Builder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6db5be99034904"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000) -> MixtureSampler:\n",
    "    \"\"\"\n",
    "    Build a mixture of generators: [HF, copy, repeat, nback].\n",
    "    - If hf_dataset is Alpaca-like (has instruction/output), we use hf_instruction_loader + make_hf_batch.\n",
    "    - Else we treat it as text-only (e.g., roneneldan/TinyStories), tokenizing and making windowed sequences.\n",
    "    - If HF fails/empty, we drop it and renormalize over synthetics.\n",
    "    \"\"\"\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "\n",
    "    gens, wts, names = [], [], []\n",
    "\n",
    "    hf_ok = False\n",
    "    hf_reason = \"\"\n",
    "    gen_hf = None\n",
    "\n",
    "    if hf_dataset:\n",
    "        try:\n",
    "            # Heuristic: use instruction loader for alpaca-like IDs\n",
    "            if \"alpaca\" in hf_dataset.lower():\n",
    "                pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\", \"output\"),\n",
    "                                              max_items=hf_max_items)\n",
    "                if pairs:\n",
    "                    def gen_hf(b): \n",
    "                        return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "                    hf_ok = True\n",
    "                else:\n",
    "                    hf_reason = \"hf_instruction_loader returned 0 pairs\"\n",
    "            else:\n",
    "                # Text-only fallback (e.g., roneneldan/TinyStories)\n",
    "                from datasets import load_dataset\n",
    "                # Try streaming first, then non-streaming\n",
    "                try:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\", streaming=True)\n",
    "                except Exception:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\")\n",
    "\n",
    "                # Find a usable text key\n",
    "                text_key = None\n",
    "                common_keys = (\"text\", \"content\", \"story\", \"document\", \"body\", \"article\")\n",
    "\n",
    "                feats = getattr(ds, \"features\", None)\n",
    "                if feats:\n",
    "                    for k in common_keys:\n",
    "                        if k in feats:\n",
    "                            text_key = k\n",
    "                            break\n",
    "\n",
    "                if text_key is None:\n",
    "                    # Probe first example (works for streaming iterable)\n",
    "                    try:\n",
    "                        first_ex = next(iter(ds))\n",
    "                        for k in common_keys:\n",
    "                            if k in first_ex:\n",
    "                                text_key = k\n",
    "                                break\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "\n",
    "                if text_key is None:\n",
    "                    hf_reason = \"no usable text field (tried: %s)\" % \",\".join(common_keys)\n",
    "                else:\n",
    "                    import random as _rnd\n",
    "                    import torch\n",
    "                    samples = []\n",
    "                    for ex in ds:\n",
    "                        txt = ex.get(text_key, None)\n",
    "                        if not txt:\n",
    "                            continue\n",
    "                        ids = tok(txt, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "                        if ids.numel() < 8:\n",
    "                            continue\n",
    "                        samples.append(ids.cpu())\n",
    "                        if len(samples) >= int(hf_max_items):\n",
    "                            break\n",
    "\n",
    "                    if len(samples) == 0:\n",
    "                        hf_reason = \"collected 0 tokenized samples\"\n",
    "                    else:\n",
    "                        def gen_hf(b: int) -> torch.Tensor:\n",
    "                            out = []\n",
    "                            for _ in range(b):\n",
    "                                ids = _rnd.choice(samples)\n",
    "                                n = ids.numel()\n",
    "                                if n >= mx:\n",
    "                                    s = _rnd.randint(0, n - mx)\n",
    "                                    seq = ids[s:s+mx]\n",
    "                                else:\n",
    "                                    pad = torch.full((mx - n,), pad_id, dtype=torch.long)\n",
    "                                    seq = torch.cat([ids, pad], dim=0)\n",
    "                                out.append(seq.unsqueeze(0))\n",
    "                            return torch.cat(out, dim=0)\n",
    "                        hf_ok = True\n",
    "        except Exception as e:\n",
    "            hf_reason = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "    # Assemble mixture in the agreed order (hf, copy, repeat, nback)\n",
    "    if hf_ok and gen_hf is not None:\n",
    "        gens.append(gen_hf); wts.append(weights[0]); names.append(\"hf\")\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        if hf_dataset:\n",
    "            print(f\"HF dataset unavailable or empty; using synthetic only. reason={hf_reason}\")\n",
    "        s_w = list(weights[1:])  # keep caller's relative weights for synthetics\n",
    "\n",
    "    # Synthetic tasks (your existing closures)\n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100, pad_id=pad_id, device=\"cpu\")\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "\n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback])\n",
    "    wts.extend(s_w)\n",
    "    names.extend([\"copy\", \"repeat\", \"nback\"])\n",
    "\n",
    "    return MixtureSampler(gens, wts, names=names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.703514600Z",
     "start_time": "2025-08-20T16:32:39.137516Z"
    }
   },
   "id": "d629080a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Logging utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a8d8e756c4c8a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 TensorBoard Logger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "189691151b04221f"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        self.writer = None\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        logdir = logdir or \"./runs\"\n",
    "        run_name = run_name or time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def add_histogram(self, tag: str, values: \"torch.Tensor\", step: int, bins: int = 50):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        v = values\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.detach()\n",
    "            if v.device.type != \"cpu\": v = v.cpu()\n",
    "            v = v.reshape(-1).float()\n",
    "        self.writer.add_histogram(tag, v, global_step=step, bins=bins)\n",
    "\n",
    "    def add_text(self, tag: str, text: str, step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_text(tag, text, step)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.848514800Z",
     "start_time": "2025-08-20T16:32:39.152516Z"
    }
   },
   "id": "b44d73c717b8b58c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Memory tracer & TensorBoard memory visualizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6636493a9a0eb2"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.849514400Z",
     "start_time": "2025-08-20T16:32:39.344515500Z"
    }
   },
   "id": "028e39a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0322be9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1 Schedulers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eac969641de0772d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.850514300Z",
     "start_time": "2025-08-20T16:32:39.355514700Z"
    }
   },
   "id": "b6be4ba6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2 Training Utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7a398e8eff544"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    #print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.850514300Z",
     "start_time": "2025-08-20T16:32:39.371516300Z"
    }
   },
   "id": "a70a7bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.3 Experimental training loop, stage 1 experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597e27677e1220b4"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train_experiment(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 5000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = False,\n",
    "    viz_prompt: str = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "    # NEW: optional schedules (each a list of (until_step, value))\n",
    "    mixture_schedule=None,         # e.g., [(100, (0.3,0.3,0.2,0.2)), (None, (0.4,0.2,0.2,0.2))]\n",
    "    gate_temp_schedule=None,       # e.g., [(100, 0.8), (None, 1.0)]\n",
    "    gate_reg_schedule=None,        # e.g., [(100, 3e-4), (None, 2e-4)]\n",
    "):\n",
    "    cfg = CFG\n",
    "    steps = int(steps or cfg.train_steps)\n",
    "    batch_size = int(batch_size or getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "\n",
    "    # Model & tokenizer\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "\n",
    "    # Sampler (and allow schedules to change weights)\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    def _apply_schedules(step: int, mix_name_hint=None):\n",
    "        # mixture schedule\n",
    "        if mixture_schedule:\n",
    "            for until, ws in mixture_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    try:\n",
    "                        mixer.set_weights(ws)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "        # gate temp schedule\n",
    "        if gate_temp_schedule:\n",
    "            for until, temp in gate_temp_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_temp\", float(temp))\n",
    "                    break\n",
    "        # gate reg schedule\n",
    "        if gate_reg_schedule:\n",
    "            for until, lam in gate_reg_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_reg_lambda\", float(lam))\n",
    "                    break\n",
    "\n",
    "    # TB setup\n",
    "    global tb\n",
    "    if TB_AVAILABLE:\n",
    "        try:\n",
    "            tb  # NameError if not defined\n",
    "        except NameError:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        if not isinstance(tb, TBLogger) or getattr(tb, \"writer\", None) is None:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        tblog = True\n",
    "        tb.add_text(\"run/config\", json.dumps({\n",
    "            \"steps\": steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    else:\n",
    "        tblog = False\n",
    "\n",
    "    head.train()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(amp_dtype in (torch.float16, torch.bfloat16)))\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        _apply_schedules(step)\n",
    "\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "        with torch.autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype != torch.float32)):\n",
    "            logits, gates, aux = head.forward_with_metrics(in_ids, gate_override=getattr(CFG, \"force_g\", None))\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "            # Optional: mild gate-usage regularizer (same as before)\n",
    "            lam = float(getattr(CFG, \"gate_reg_lambda\", 0.0))\n",
    "            if lam > 0 and isinstance(gates, (list, tuple)) and len(gates) > 0:\n",
    "                reg = 0.0\n",
    "                for g in gates:\n",
    "                    g2 = _reduce_gate_tensor(g)\n",
    "                    # Encourage decisive routing on memory batches only? For now, uniform\n",
    "                    reg = reg + (g2.mean() * 0.0 + (g2 * (1 - g2)).mean())  # small entropy-like penalty\n",
    "                loss = loss + lam * reg\n",
    "\n",
    "        use_scaler = (amp_dtype in (torch.float16, torch.bfloat16))\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step(); optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # --- Logging (TB) ---\n",
    "        if step % log_every == 0 and tblog:\n",
    "            # global scalars\n",
    "            tb.writer.add_scalar(\"train/loss\", float(loss.item()), step)\n",
    "            tb.writer.add_scalar(\"train/lr\", float(scheduler.get_last_lr()[0]), step)\n",
    "\n",
    "            # gate metrics per block (global)\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_mean\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_frac>0.5\", g_frac, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_entropy\", g_ent, step)\n",
    "\n",
    "                # per-task metrics (using the sampler's last batch type)\n",
    "                mix_name = getattr(mixer, \"last_name\", None) or \"unknown\"\n",
    "                tb.writer.add_scalar(f\"loss_by_task/{mix_name}\", float(loss.item()), step)\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_mean/{mix_name}\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_frac>0.5/{mix_name}\", g_frac, step)\n",
    "\n",
    "                # optional: quartiles (block 0)\n",
    "                if len(gates) > 0:\n",
    "                    g0 = _reduce_gate_tensor(gates[0].detach())\n",
    "                    T = g0.size(1); q = max(1, T // 4)\n",
    "                    slices = [(0, q), (q, 2*q), (2*q, 3*q), (3*q, T)]\n",
    "                    for qi, (s, e) in enumerate(slices, start=1):\n",
    "                        tb.writer.add_scalar(f\"gates/block0_q{qi}_mean/{mix_name}\", float(g0[:, s:e].mean().item()), step)\n",
    "\n",
    "            tb.flush()\n",
    "\n",
    "        # --- Console echo (always) ---\n",
    "        if step % log_every == 0:\n",
    "            g_means_print = []\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for g in gates:\n",
    "                    g_means_print.append(float(_reduce_gate_tensor(g).mean().item()))\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {scheduler.get_last_lr()[0]:.2e} | \"\n",
    "                  f\"gates={g_means_print} | mix={getattr(mixer,'last_name','?')}\")\n",
    "\n",
    "    # Optional memory viz\n",
    "    if viz_memory_after:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "    \n",
    "    # flush log      \n",
    "    if tblog:\n",
    "        tb.flush()\n",
    "\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.850514300Z",
     "start_time": "2025-08-20T16:32:39.386517600Z"
    }
   },
   "id": "f8879a898faf706a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.4 Robust forward for ParallelEnrichmentBlock\n",
    "TODO: this is a monkeypatch, non-critical but fold in properly when I have time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569f513137ac4911"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# # --- Patch: robust forward for ParallelEnrichmentBlock ---\n",
    "def _peb_forward(self, x, dnc_state=None, gate_override=None):\n",
    "    \"\"\"\n",
    "    Returns: (out, dnc_state, g)\n",
    "      out: (B,T,D), mixed from vanilla (vt) and DNC (dt)\n",
    "      dnc_state: updated state dict from DNC path\n",
    "      g: (B,T,D) or (B,T,1) gate values in [0,1]\n",
    "    \"\"\"\n",
    "    # 1) Vanilla transformer path\n",
    "    T = x.size(1)\n",
    "    mask = causal_mask(T, device=x.device)  # (T,T) causal attn mask\n",
    "    x_cast = x.to(self.vanilla.ln1.weight.dtype)\n",
    "    vt = self.vanilla(x_cast, attn_mask=mask)    # (B,T,D)\n",
    "\n",
    "    # 2) DNC path (controller+memory)\n",
    "    dt, dnc_state = self.dncblock(x, state=dnc_state)  # dt: (B,T,D)\n",
    "\n",
    "    # 3) Gate computation (optionally LN before gate)\n",
    "    import torch as _t\n",
    "    z = _t.cat([vt, dt], dim=-1)  # (B,T,2D)\n",
    "    h = self.pre_gate_ln(z) if hasattr(self, \"pre_gate_ln\") and self.pre_gate_ln is not None else z\n",
    "    g_pre = self.gate(h)                 # typically (B,T,D) or (B,T,1)\n",
    "    tau = float(getattr(CFG, 'gate_temp', 1.0))\n",
    "    g = torch.sigmoid(g_pre / max(tau, 1e-6))\n",
    "\n",
    "    # Optional ablation override: force g to 0.0 (vanilla) or 1.0 (DNC)\n",
    "    if gate_override is not None:\n",
    "        g = _t.full_like(g, float(gate_override))\n",
    "\n",
    "    # 4) Blend paths\n",
    "    out = g * dt + (1.0 - g) * vt        # (B,T,D)\n",
    "\n",
    "    # 5) Lightweight metrics (only if requested)\n",
    "    self.last_metrics = None\n",
    "    if getattr(self, \"collect_metrics\", False):\n",
    "        try:\n",
    "            eps = 1e-6\n",
    "            gc = g.clamp(min=eps, max=1.0 - eps)\n",
    "            g_entropy = (-(gc * gc.log() + (1 - gc) * (1 - gc).log())).mean()\n",
    "            vt_norm = vt.norm(dim=-1).mean()\n",
    "            dt_norm = dt.norm(dim=-1).mean()\n",
    "            mstats = {}\n",
    "            # Pull aggregated memory stats if present\n",
    "            if isinstance(dnc_state, dict) and isinstance(dnc_state.get(\"stats\", None), dict):\n",
    "                for k in (\"u_mean\", \"rw_max_mean\", \"ww_max_mean\", \"M_norm_mean\"):\n",
    "                    if k in dnc_state[\"stats\"]:\n",
    "                        mstats[k] = dnc_state[\"stats\"][k]\n",
    "            self.last_metrics = {\n",
    "                \"g_mean\": g.mean().detach(),\n",
    "                \"g_entropy\": g_entropy.detach(),\n",
    "                \"vt_norm\": vt_norm.detach(),\n",
    "                \"dt_norm\": dt_norm.detach(),\n",
    "                **mstats,\n",
    "            }\n",
    "        except Exception:\n",
    "            self.last_metrics = None\n",
    "\n",
    "    return out, dnc_state, g\n",
    "\n",
    "# Apply the patch\n",
    "ParallelEnrichmentBlock.forward = _peb_forward\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.850514300Z",
     "start_time": "2025-08-20T16:32:39.501517500Z"
    }
   },
   "id": "3eabc7991c7e743d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Eval harnesses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9ee90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1 copy, reverse, needle-in-haystack"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73218bd54c4f2bc"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad() # WARNING - planned for depreciation\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_haystack(head, steps: int = 50, batch: int = 16, T: int = 256, vocab: int = 1024,\n",
    "                      tb_step: int = None, fast: bool = False):\n",
    "    \"\"\"\n",
    "    Long-span retrieval probe: ... K V ... K ? => predict V at '?'.\n",
    "    fast=True uses inference_mode() and smaller defaults to speed up sweeps.\n",
    "    \"\"\"\n",
    "    from torch.amp import autocast\n",
    "    head.eval()\n",
    "\n",
    "    # Fast path defaults (can still be overridden by explicit args)\n",
    "    if fast:\n",
    "        steps = min(steps, 10)\n",
    "        batch = min(batch, 8)\n",
    "        T = min(T, 128)\n",
    "\n",
    "    device_ = next(head.parameters()).device\n",
    "    use_amp = (amp_dtype in (torch.float16, torch.bfloat16)) and torch.cuda.is_available()\n",
    "\n",
    "    accs, losses = [], []\n",
    "    ctx = torch.inference_mode() if fast else torch.no_grad()\n",
    "    with ctx:\n",
    "        for _ in range(steps):\n",
    "            x, V, qpos = make_haystack_batch(batch, T=T, vocab=vocab)\n",
    "            x = x.to(device_); V = V.to(device_); qpos = qpos.to(device_)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    logits, _g = head(x)\n",
    "            else:\n",
    "                logits, _g = head(x)\n",
    "\n",
    "            idx = torch.arange(x.size(0), device=device_)\n",
    "            logits_q = logits[idx, qpos, :].float()\n",
    "            loss = F.cross_entropy(logits_q, V)\n",
    "            pred = logits_q.argmax(dim=-1)\n",
    "\n",
    "            accs.append((pred == V).float().mean().item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    acc_m = float(np.mean(accs)); loss_m = float(np.mean(losses))\n",
    "\n",
    "    if TB_AVAILABLE and ('tb' in globals()):\n",
    "        tb.writer.add_scalar(\"eval/haystack_acc\", acc_m, tb_step if tb_step is not None else 0)\n",
    "        tb.writer.add_scalar(\"eval/haystack_loss\", loss_m, tb_step if tb_step is not None else 0)\n",
    "        tb.flush()\n",
    "\n",
    "    head.train()\n",
    "    print(f\"[Haystack] acc={acc_m:.3f} | loss={loss_m:.3f} | fast={fast}\")\n",
    "    return acc_m, loss_m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.863514500Z",
     "start_time": "2025-08-20T16:32:39.518515100Z"
    }
   },
   "id": "efb2dc08fe1942ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Unit tests, smoke tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3036ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.1 basic unit test, model integrity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba6259d196dd13cc"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def run_basic_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.865514400Z",
     "start_time": "2025-08-20T16:32:39.527518Z"
    }
   },
   "id": "2cb70f70"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "#run_basic_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:39.991514400Z",
     "start_time": "2025-08-20T16:32:39.543515Z"
    }
   },
   "id": "f9f2d4e1ea8a1a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.2 Evaluator unit tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b399df35806fe6f5"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:40.005515Z",
     "start_time": "2025-08-20T16:32:39.558514700Z"
    }
   },
   "id": "db24ba31"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "#run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:40.006514300Z",
     "start_time": "2025-08-20T16:32:39.573514900Z"
    }
   },
   "id": "4c45b9e3f62413ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.3 GPU VRAM diagnostics + cuda allocation smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683887a5f3501496"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=0.00 GB | reserved=0.00 GB | free=24.44 GB | total=25.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "424d21b2074c4cefa828ceddeaaa517d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=7.64 GB | reserved=7.64 GB | free=16.79 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, contextlib\n",
    "\n",
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "        \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.446756900Z",
     "start_time": "2025-08-20T16:32:39.588515Z"
    }
   },
   "id": "2af2ed601f9c7606"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.4 TB logger test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d48becf8a7f50a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB active: True | logdir: ./runs\\dncformer-20250820-093245\n"
     ]
    }
   ],
   "source": [
    "tb = TBLogger(logdir=\"./runs\")\n",
    "try:\n",
    "    tb\n",
    "except NameError:\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "print(\"TB active:\", TB_AVAILABLE, \"| logdir:\", getattr(tb, \"path\", None))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.493751100Z",
     "start_time": "2025-08-20T16:32:45.447754300Z"
    }
   },
   "id": "bf494415ac5e8547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.5 Memory tracer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1d5de08007861f"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.506751300Z",
     "start_time": "2025-08-20T16:32:45.464753500Z"
    }
   },
   "id": "d842ad22"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.507751500Z",
     "start_time": "2025-08-20T16:32:45.479752400Z"
    }
   },
   "id": "86a52d1c90f05c57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.6 Data generator smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6097ee7f06d9e87c"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_copy batch shape: torch.Size([3, 128]) | dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.520751100Z",
     "start_time": "2025-08-20T16:32:45.495751500Z"
    }
   },
   "id": "be0b78c71554906c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.7 Mixture sampler smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92b9b09dbe07c2b"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "p1: [0.30000001192092896, 0.30000001192092896, 0.25, 0.15000000596046448]\n"
     ]
    }
   ],
   "source": [
    "ms = MixtureSampler(gens=[lambda b: None, lambda b: None, lambda b: None, lambda b: None],\n",
    "                    weights=[0.4,0.2,0.2,0.2],\n",
    "                    names=[\"hf\",\"copy\",\"repeat\",\"nback\"])\n",
    "print(\"p0:\", ms.p.tolist())    # ~[0.4,0.2,0.2,0.2]\n",
    "ms.set_weights([0.3,0.3,0.25,0.15])\n",
    "print(\"p1:\", ms.p.tolist())    # ~[0.3,0.3,0.25,0.15]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:45.570752900Z",
     "start_time": "2025-08-20T16:32:45.510766200Z"
    }
   },
   "id": "635b3129c4abe2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.8 Build mixer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc74e56a0b98ce26"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d58c1a3baeb480f961cd4183e4510cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n"
     ]
    }
   ],
   "source": [
    "# 1) With Alpaca (instruction/output)\n",
    "tok, _ = build_model_and_tokenizer()\n",
    "m1 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=500)\n",
    "print(\"names:\", m1.names, \"| p:\", m1.p.tolist())  # expect 'hf' present\n",
    "\n",
    "# 2) With TinyStories (text)\n",
    "m2 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"roneneldan/TinyStories\", hf_max_items=500)\n",
    "print(\"names:\", m2.names, \"| p:\", m2.p.tolist())  # expect 'hf' present (text path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:32:58.406780300Z",
     "start_time": "2025-08-20T16:32:45.527751700Z"
    }
   },
   "id": "92366ea6aeb2c846"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.9 haystack smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63121cfeae026f7c"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbc66d1c471546749149f3e7e88e328d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Haystack] acc=0.000 | loss=10.112 | fast=False\n",
      "Haystack smoke: 0.0 10.111836910247803\n"
     ]
    }
   ],
   "source": [
    "tok, head = build_model_and_tokenizer()\n",
    "acc, loss = evaluate_haystack(head, steps=2, batch=4, T=64, vocab=512, tb_step=0)\n",
    "print(\"Haystack smoke:\", acc, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:33:05.492780700Z",
     "start_time": "2025-08-20T16:32:58.402780200Z"
    }
   },
   "id": "dca7ccde4a7a19d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.10 Training run Sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867e3f96209f238d"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a9cc2ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:33:53.325297200Z",
     "start_time": "2025-08-20T16:33:05.491780800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=9.78 GB | reserved=9.81 GB | free=14.57 GB | total=25.77 GB\n",
      "[snapshot: before train_experiment] alloc=9.78 GB | reserved=9.81 GB | free=14.57 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45f2c694e8964977911aeeacd7f7807d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 3.5799 | lr 2.00e-05 | gates=[0.283203125, 0.275390625] | mix=hf\n",
      "[snapshot: after train_experiment] alloc=19.56 GB | reserved=33.25 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()\n",
    "cuda_report(\"snapshot: before train_experiment\")\n",
    "head, tok = train_experiment(steps=10, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "cuda_report(\"snapshot: after train_experiment\")\n",
    "#Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Experiments - Stage 1 - basic architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5c8e4aa3034c4a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.1 Experiment Set 1 - Medium training sweep, testing parameter / dataset variants\n",
    " - E0: Baseline\n",
    " - E1: memory/algorithmic slanted data mix\n",
    " - E2: gate regularizer (low)\n",
    " - E3: gate regularizer (high)\n",
    " - E4: sharper routing via lower gate temperature\n",
    " - E5: ablations: disable/force DNC path\n",
    "    - 5a: disable DNC path (DNC path disabled)\n",
    "    - 5b: force DNC path (transformer path disabled) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81cbb7a830e97c75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E0_baseline ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E0_baseline] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "978f5412cfc4409d8fe23562b6b085d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time, torch, gc\n",
    "\n",
    "# run set parameters\n",
    "EXP_STEPS   = 500       # try 1000–5000 for longer curves\n",
    "EXP_WARMUP  = 10       # keep ~steps/20\n",
    "BASE_MIX    = (0.4, 0.2, 0.2, 0.2)  # (hf, copy, repeat, nback)\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_isolate_test(label,\n",
    "            mixture_weights=BASE_MIX,\n",
    "            gate_reg_lambda=None,\n",
    "            gate_temp=None,\n",
    "            force_g=None,\n",
    "            steps=EXP_STEPS,\n",
    "            warmup=EXP_WARMUP):\n",
    "    \"\"\"Run a single train_experiment experiment with explicit knobs.\"\"\"\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    # set experiment-specific knobs (others come from CFG defaults)\n",
    "    if gate_reg_lambda is not None: set_cfg(gate_reg_lambda=float(gate_reg_lambda))\n",
    "    if gate_temp is not None:       set_cfg(gate_temp=float(gate_temp))\n",
    "    set_cfg(force_g=force_g)  # may be None, 0.0, or 1.0\n",
    "\n",
    "    print(f\"mixture={mixture_weights}, gate_reg_lambda={getattr(CFG,'gate_reg_lambda', 0.0)}, \"\n",
    "          f\"gate_temp={getattr(CFG,'gate_temp', 1.0)}, force_g={getattr(CFG,'force_g', None)}\")\n",
    "\n",
    "    # clean slate for VRAM and allocator fragmentation between runs\n",
    "    free_head_and_cache()\n",
    "    cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # run\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=warmup,\n",
    "        mixture_weights=mixture_weights,\n",
    "        viz_memory_after=False,   # keep quick; use visualize_memory_tb() ad-hoc\n",
    "    )\n",
    "\n",
    "    # post-run snapshot + cleanup\n",
    "    cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# === E0 baseline: identical to your sanity run ===\n",
    "run_isolate_test(\"E0_baseline\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E1 memory-leaning mix: more algorithmic exposure ===\n",
    "run_isolate_test(\"E1_memory_lean\",\n",
    "        mixture_weights=(0.4, 0.3, 0.2, 0.1),  # HF, copy, repeat, nback\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E2 gate regularizer (low) ===\n",
    "run_isolate_test(\"E2_gate_reg_low\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=2e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E3 gate regularizer (high) ===\n",
    "run_isolate_test(\"E3_gate_reg_high\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=5e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E4 sharper routing via lower gate temperature ===\n",
    "run_isolate_test(\"E4_gate_temp_0p7\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=0.7,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E5 ablations: disable/force DNC path ===\n",
    "run_isolate_test(\"E5a_force_g_0\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=0.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "run_isolate_test(\"E5b_force_g_1\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=1.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-08-20T16:33:53.323296700Z"
    }
   },
   "id": "771c919035840bc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f07fc074b58caaa0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.2 Experiment Set 2 - parameter sweeps, based on E2 params from set 1 above\n",
    " - E6: higher gate temp\n",
    " - E7: memory-leaning warm-start\n",
    " - E8: capacity sweep\n",
    " - E9: baseline training, haystack eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a269fb24f3b9075"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re, time\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "    \n",
    "def start_tb_run(label: str = None, logdir: str = \"./runs\"):\n",
    "    \"\"\"Close any existing TB writer and open a fresh run dir with timestamp + optional label.\"\"\"\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TensorBoard not available; skipping start_tb_run.\")\n",
    "        return False\n",
    "    global tb\n",
    "    # Close/flush a previous writer if present\n",
    "    try:\n",
    "        if 'tb' in globals() and isinstance(tb, TBLogger) and getattr(tb, \"writer\", None):\n",
    "            tb.flush(); tb.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ts = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "    run_name = ts\n",
    "    if label:\n",
    "        safe = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(label))\n",
    "        run_name = f\"{ts}-{safe}\"\n",
    "\n",
    "    tb = TBLogger(logdir=logdir, run_name=run_name)\n",
    "    tb.add_text(\"run/label\", str(label or \"unlabeled\"), 0)\n",
    "    print(\"TB run started:\", getattr(tb, \"path\", None))\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_one_labeled(label, steps, mixture_weights, seed=1234,\n",
    "                    mixture_schedule=None, gate_temp_schedule=None, gate_reg_schedule=None,\n",
    "                    post_haystack=False):\n",
    "    print(f\"\\n=== {label} | seed={seed} ===\")\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed) \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_cfg(force_g=None)  # ensure no ablation\n",
    "    # unique TB run for each experiment label\n",
    "    start_tb_run(label)\n",
    "\n",
    "    # echo run metadata\n",
    "    if TB_AVAILABLE and 'tb' in globals():\n",
    "        import json\n",
    "        tb.add_text(\"run/meta\", json.dumps({\n",
    "            \"label\": label,\n",
    "            \"steps\": steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    \n",
    "    # print config stub\n",
    "    print(\"CFG.gate_temp:\", getattr(CFG, \"gate_temp\", 1.0),\n",
    "          \"| CFG.gate_reg_lambda:\", getattr(CFG, \"gate_reg_lambda\", 0.0),\n",
    "          \"| mixture:\", mixture_weights)\n",
    "\n",
    "    free_head_and_cache()\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}\")\n",
    "\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=max(10, steps//20),\n",
    "        mixture_weights=mixture_weights,\n",
    "        mixture_schedule=mixture_schedule,\n",
    "        gate_temp_schedule=gate_temp_schedule,\n",
    "        gate_reg_schedule=gate_reg_schedule,\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    if post_haystack:\n",
    "        evaluate_haystack(head, steps=50, batch=16, T=256, vocab=1024, tb_step=steps, fast=True)\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# Common settings\n",
    "EXP_STEPS = 500\n",
    "BASE_MIX  = (0.4, 0.2, 0.2, 0.2)\n",
    "SEEDS     = [1337, 2027, 4242]\n",
    "\n",
    "# === E6: gate_temp=0.8 (3 seeds), otherwise baseline ===\n",
    "set_cfg(gate_reg_lambda=getattr(CFG, \"gate_reg_lambda\", 2e-4))  # low-λ default\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E6_temp0p8_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    gate_temp_schedule=[(None, 0.8)])\n",
    "\n",
    "# === E7: memory-leaning warm-start (first 10% steps), then baseline; 3 seeds ===\n",
    "warm_steps = max(50, EXP_STEPS // 10)\n",
    "mix_warm   = (0.3, 0.3, 0.25, 0.15)  # a bit more memory-heavy than baseline\n",
    "mix_main   = BASE_MIX\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E7_warmstart_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    mixture_schedule=[(warm_steps, mix_warm), (None, mix_main)],\n",
    "                    gate_temp_schedule=[(warm_steps, 0.8), (None, 1.0)],\n",
    "                    gate_reg_schedule=[(warm_steps, max(2e-4, getattr(CFG,'gate_reg_lambda', 2e-4))), (None, getattr(CFG,'gate_reg_lambda', 2e-4))])\n",
    "\n",
    "# === E8: capacity sweep N=64 vs N=128 (1 seed each) ===\n",
    "for N_val in (64, 128):\n",
    "    set_cfg(N=N_val)  # assumes your DNC block reads CFG.N at construction\n",
    "    run_one_labeled(f\"E8_capacity_N{N_val}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=777,\n",
    "                    gate_temp_schedule=[(None, getattr(CFG, 'gate_temp', 1.0))])\n",
    "\n",
    "# Reset N if changed\n",
    "set_cfg(N=getattr(CFG, 'N', 128))\n",
    "\n",
    "# === E9: baseline with haystack eval ===\n",
    "run_one_labeled(\"E9_baseline_haystack\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                seed=31415, post_haystack=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ff510cbc0c8eb5c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "823dd4035914de8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Experiments - Stage 2 - tiered/parallel memory systems"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e4b05cfd4e94265"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "da253ebbc27abf4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 00. Misc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a94ff038fb1b7b65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensorboard log dump"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c869b656957faac2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ===== TensorBoard event analyzer for DNCFormer runs (robust) =====\n",
    "# Discovers TB runs, merges multiple event files per run, summarizes, and exports granular CSVs.\n",
    "\n",
    "import os, re, math, json, time, glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- TensorBoard imports (version-agnostic handling) ---\n",
    "try:\n",
    "    import tensorboard as _tb\n",
    "    print(\"TensorBoard version:\", getattr(_tb, \"__version__\", \"unknown\"))\n",
    "except Exception as _e:\n",
    "    print(\"TensorBoard import note:\", _e)\n",
    "\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. Install via: pip install tensorboard\") from e\n",
    "\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _size_guidance_version_safe():\n",
    "    \"\"\"Build size_guidance dict usable across TB versions.\"\"\"\n",
    "    sg = {}\n",
    "    keys = [\"SCALARS\", \"HISTOGRAMS\", \"IMAGES\", \"COMPRESSED_HISTOGRAMS\", \"AUDIO\", \"TENSORS\"]\n",
    "    for k in keys:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "\n",
    "def _infer_label_from_run_dir(run_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Expect 'dncformer-YYYYMMDD-HHMMSS-<LABEL>' or just 'dncformer-YYYYMMDD-HHMMSS'.\n",
    "    Returns <LABEL> if present, else the directory name.\n",
    "    \"\"\"\n",
    "    name = run_dir.name\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return name\n",
    "\n",
    "\n",
    "def _load_scalars_from_event_file(ev_path: str) -> dict:\n",
    "    \"\"\"Load scalars from a single event file: tag -> list[(step, value)].\"\"\"\n",
    "    acc = EventAccumulator(ev_path, size_guidance=_size_guidance_version_safe())\n",
    "    acc.Reload()\n",
    "    tags = acc.Tags().get('scalars', []) or []\n",
    "    out = {}\n",
    "    for tag in tags:\n",
    "        vals = acc.Scalars(tag)\n",
    "        out[tag] = [(int(x.step), float(x.value)) for x in vals]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _merge_scalar_dicts(list_of_scalar_dicts):\n",
    "    \"\"\"\n",
    "    Merge multiple event files for the same run.\n",
    "    For each tag: keep the last value seen per (step), then return sorted by step.\n",
    "    \"\"\"\n",
    "    merged = defaultdict(dict)  # tag -> {step: value}\n",
    "    for scal in list_of_scalar_dicts:\n",
    "        for tag, series in scal.items():\n",
    "            d = merged[tag]\n",
    "            for step, val in series:\n",
    "                d[step] = val  # 'last wins' is fine; event files are append-only per run\n",
    "    # Convert to tag -> sorted list[(step, value)]\n",
    "    out = {}\n",
    "    for tag, d in merged.items():\n",
    "        steps_sorted = sorted(d.keys())\n",
    "        out[tag] = [(s, d[s]) for s in steps_sorted]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _detect_tasks_and_blocks(scalars: dict):\n",
    "    \"\"\"Detect task names and block ids from present tags. Returns (sorted_tasks, sorted_blocks).\"\"\"\n",
    "    tasks = set()\n",
    "    blocks = set()\n",
    "\n",
    "    # tasks from \"loss_by_task/<task>\"\n",
    "    for tag in scalars.keys():\n",
    "        if tag.startswith(\"loss_by_task/\"):\n",
    "            tasks.add(tag.split(\"/\", 1)[1])\n",
    "\n",
    "    # blocks from \"gates_by_task/block_<b>_mean/<task>\" or \"gates/block_<b>_mean\"\n",
    "    for tag in scalars.keys():\n",
    "        m = re.match(r\"gates_by_task/block_(\\d+)_\", tag)\n",
    "        if m:\n",
    "            blocks.add(int(m.group(1)))\n",
    "        m2 = re.match(r\"gates/block_(\\d+)_mean$\", tag)\n",
    "        if m2:\n",
    "            blocks.add(int(m2.group(1)))\n",
    "\n",
    "    # sensible defaults if nothing is found\n",
    "    if not tasks:\n",
    "        tasks = {\"hf\", \"copy\", \"repeat\", \"nback\"}\n",
    "    if not blocks:\n",
    "        blocks = {0, 1}\n",
    "\n",
    "    return sorted(tasks), sorted(blocks)\n",
    "\n",
    "\n",
    "def s_last(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[-1])\n",
    "    return float(np.nanmean(arr[-k:]))\n",
    "\n",
    "\n",
    "def s_first(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[0])\n",
    "    return float(np.nanmean(arr[:k]))\n",
    "\n",
    "\n",
    "def s_mean(vals):\n",
    "    if not vals: return np.nan\n",
    "    return float(np.nanmean([v for _, v in vals]))\n",
    "\n",
    "\n",
    "def s_count(vals):\n",
    "    return len(vals) if vals else 0\n",
    "\n",
    "\n",
    "# ---------------- discover runs ----------------\n",
    "# Option 1: auto-discover all event files and group by their parent directory\n",
    "FOUND_EVENTS = sorted(glob.glob(\"runs/**/events.out.tfevents.*\", recursive=True))\n",
    "\n",
    "# Option 2: pin a subset manually\n",
    "# FOUND_EVENTS = [\n",
    "#     r\"runs/dncformer-20250819-122754-E6_gate_temp_0p8/events.out.tfevents....\",\n",
    "# ]\n",
    "\n",
    "assert FOUND_EVENTS, \"No event files found under ./runs. Have you executed any experiments?\"\n",
    "\n",
    "# Group event files by run directory\n",
    "run_groups = defaultdict(list)   # run_dir_path -> [event_file_paths]\n",
    "for p in FOUND_EVENTS:\n",
    "    run_groups[str(Path(p).parent)].append(p)\n",
    "\n",
    "print(f\"Discovered {len(run_groups)} run(s).\")\n",
    "\n",
    "\n",
    "# ---------------- summarize each run ----------------\n",
    "run_summaries = []\n",
    "per_run_scalars = {}   # run_dir_name -> merged scalars dict\n",
    "\n",
    "for run_dir, files in sorted(run_groups.items()):\n",
    "    run_dir_path = Path(run_dir)\n",
    "    label = _infer_label_from_run_dir(run_dir_path)\n",
    "    run_id = run_dir_path.name\n",
    "\n",
    "    try:\n",
    "        # load and merge all files for this run\n",
    "        scal_dicts = [_load_scalars_from_event_file(f) for f in sorted(files)]\n",
    "        scal = _merge_scalar_dicts(scal_dicts)\n",
    "        per_run_scalars[run_dir_path.name] = scal\n",
    "\n",
    "        # detect tasks and blocks present\n",
    "        TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "        # basics\n",
    "        loss_series = scal.get(\"train/loss\", [])\n",
    "        lr_series   = scal.get(\"train/lr\", [])\n",
    "        steps_logged = max([s for s, _ in loss_series], default=np.nan) if loss_series else np.nan\n",
    "        loss0   = s_first(loss_series, k=5)\n",
    "        lossT   = s_last(loss_series,  k=10)\n",
    "        ldelta  = (loss0 - lossT) if not any(map(math.isnan, [loss0, lossT])) else np.nan\n",
    "        lr_last = s_last(lr_series, k=1)\n",
    "\n",
    "        # gates (global)\n",
    "        g_means, g_entropy, g_frac_avg = {}, {}, {}\n",
    "        for b in BLOCKS:\n",
    "            g_means[b]   = s_last(scal.get(f\"gates/block_{b}_mean\", []), k=10)\n",
    "            g_entropy[b] = s_last(scal.get(f\"gates/block_{b}_entropy\", []), k=10)\n",
    "            fracs = []\n",
    "            for t in TASKS:\n",
    "                tag = f\"gates_by_task/block_{b}_frac>0.5/{t}\"\n",
    "                if tag in scal:\n",
    "                    fracs.append(s_last(scal[tag], k=10))\n",
    "            g_frac_avg[b] = float(np.nanmean(fracs)) if fracs else np.nan\n",
    "\n",
    "        # per-task losses (mean of last quarter of points for that task)\n",
    "        task_loss_last, task_counts = {}, {}\n",
    "        for t in TASKS:\n",
    "            ts = scal.get(f\"loss_by_task/{t}\", [])\n",
    "            task_counts[t] = s_count(ts)\n",
    "            if ts:\n",
    "                k = max(1, len(ts)//4)\n",
    "                task_loss_last[t] = s_last(ts, k=k)\n",
    "            else:\n",
    "                task_loss_last[t] = np.nan\n",
    "\n",
    "        # per-task gate means (avg last quarter)\n",
    "        task_gmeans = {t: {} for t in TASKS}\n",
    "        for t in TASKS:\n",
    "            for b in BLOCKS:\n",
    "                ts = scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])\n",
    "                if ts:\n",
    "                    k = max(1, len(ts)//4)\n",
    "                    task_gmeans[t][b] = s_last(ts, k=k)\n",
    "                else:\n",
    "                    task_gmeans[t][b] = np.nan\n",
    "\n",
    "        # quartiles (block0) if present\n",
    "        q_means = {}\n",
    "        for qi in range(1, 5):\n",
    "            vals = []\n",
    "            # May be logged globally or by task; check both\n",
    "            tag_global = f\"gates/block0_q{qi}_mean\"\n",
    "            if tag_global in scal:\n",
    "                vals.append(s_last(scal[tag_global], k=10))\n",
    "            else:\n",
    "                for t in TASKS:\n",
    "                    tag_task = f\"gates/block0_q{qi}_mean/{t}\"\n",
    "                    if tag_task in scal:\n",
    "                        vals.append(s_last(scal[tag_task], k=10))\n",
    "            q_means[qi] = float(np.nanmean(vals)) if vals else np.nan\n",
    "\n",
    "        # haystack eval if present\n",
    "        hay_acc  = s_last(scal.get(\"eval/haystack_acc\",  []), k=1)\n",
    "        hay_loss = s_last(scal.get(\"eval/haystack_loss\", []), k=1)\n",
    "\n",
    "        # forced-g guess heuristic\n",
    "        forced_guess = None\n",
    "        gm_all = [g_means[b] for b in g_means if not math.isnan(g_means[b])]\n",
    "        if gm_all:\n",
    "            m = float(np.nanmean(gm_all))\n",
    "            if m < 0.02:   forced_guess = \"force_g=0\"\n",
    "            elif m > 0.98: forced_guess = \"force_g=1\"\n",
    "\n",
    "        summary = {\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"run_dir\": str(run_dir_path),\n",
    "            \"n_event_files\": len(files),\n",
    "            \"steps_logged\": steps_logged,\n",
    "            \"loss_start~5\": loss0,\n",
    "            \"loss_end~10\":  lossT,\n",
    "            \"loss_delta\":   ldelta,\n",
    "            \"lr_last\":      lr_last,\n",
    "            \"haystack_acc_last\":  hay_acc,\n",
    "            \"haystack_loss_last\": hay_loss,\n",
    "            \"forced_guess\": forced_guess,\n",
    "        }\n",
    "\n",
    "        # flatten gate summaries\n",
    "        for b in BLOCKS:\n",
    "            summary[f\"g_mean_b{b}\"]     = g_means.get(b, np.nan)\n",
    "            summary[f\"g_entropy_b{b}\"]  = g_entropy.get(b, np.nan)\n",
    "            summary[f\"g_frac>0.5_b{b}\"] = g_frac_avg.get(b, np.nan)\n",
    "\n",
    "        # flatten per-task last losses and per-task mean gates\n",
    "        for t in TASKS:\n",
    "            summary[f\"loss_{t}_last\"] = task_loss_last[t]\n",
    "            for b in BLOCKS:\n",
    "                summary[f\"gmean_b{b}_{t}\"] = task_gmeans[t][b]\n",
    "\n",
    "        # quartiles\n",
    "        for qi in range(1, 5):\n",
    "            summary[f\"g_b0_Q{qi}_mean\"] = q_means[qi]\n",
    "\n",
    "        run_summaries.append(summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[analyzer] Skipped run {run_dir}: {e}\")\n",
    "\n",
    "# -------- assemble run-level summary --------\n",
    "df_runs = pd.DataFrame(run_summaries)\n",
    "if df_runs.empty:\n",
    "    print(\"No runs summarized.\")\n",
    "else:\n",
    "    # Sort by timestamp embedded in run_id if possible\n",
    "    def _ts_key(name: str):\n",
    "        m = re.search(r\"(\\d{8})-(\\d{6})\", name or \"\")\n",
    "        return (m.group(1), m.group(2)) if m else (\"\", \"\")\n",
    "    df_runs = df_runs.sort_values(by=[\"run_id\"], key=lambda s: s.map(_ts_key), ignore_index=True)\n",
    "\n",
    "    # Arrange prominent columns\n",
    "    front_cols = [c for c in [\n",
    "        \"label\",\"run_id\",\"n_event_files\",\"steps_logged\",\n",
    "        \"loss_start~5\",\"loss_end~10\",\"loss_delta\",\"lr_last\",\n",
    "        \"haystack_acc_last\",\"haystack_loss_last\",\"forced_guess\",\n",
    "        \"g_mean_b0\",\"g_mean_b1\",\"g_entropy_b0\",\"g_entropy_b1\",\n",
    "        \"g_frac>0.5_b0\",\"g_frac>0.5_b1\",\n",
    "        \"loss_hf_last\",\"loss_copy_last\",\"loss_repeat_last\",\"loss_nback_last\",\n",
    "        \"gmean_b0_hf\",\"gmean_b1_hf\",\"gmean_b0_copy\",\"gmean_b1_copy\",\n",
    "        \"gmean_b0_repeat\",\"gmean_b1_repeat\",\"gmean_b0_nback\",\"gmean_b1_nback\",\n",
    "        \"g_b0_Q1_mean\",\"g_b0_Q2_mean\",\"g_b0_Q3_mean\",\"g_b0_Q4_mean\",\n",
    "        \"run_dir\"\n",
    "    ] if c in df_runs.columns]\n",
    "    df_runs = df_runs[[*front_cols, *[c for c in df_runs.columns if c not in front_cols]]]\n",
    "\n",
    "    display(df_runs)\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_runs.to_csv(out_dir / \"run_level_summary.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"run_level_summary.csv\")\n",
    "\n",
    "\n",
    "# -------- granular per-task time series export --------\n",
    "rows = []\n",
    "for run_dir_name, scal in per_run_scalars.items():\n",
    "    label = _infer_label_from_run_dir(Path(run_dir_name))\n",
    "    # detect tasks/blocks present in this run\n",
    "    TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "    # pick up LR series for convenient join\n",
    "    lr_by_step = {int(s): float(v) for s, v in scal.get(\"train/lr\", [])}\n",
    "\n",
    "    # per-task loss and per-task gate metrics\n",
    "    for t in TASKS:\n",
    "        loss_series = scal.get(f\"loss_by_task/{t}\", [])\n",
    "        if not loss_series:\n",
    "            continue\n",
    "\n",
    "        gmean_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "        gfrac_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_frac>0.5/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "\n",
    "        # Optional quartile logs (block 0)\n",
    "        q_by_step = {qi: {int(s): float(v) for s, v in scal.get(f\"gates/block0_q{qi}_mean/{t}\", [])}\n",
    "                     for qi in (1,2,3,4)}\n",
    "\n",
    "        for step, loss_val in loss_series:\n",
    "            step = int(step); loss_val = float(loss_val)\n",
    "            for b in BLOCKS:\n",
    "                row = {\n",
    "                    \"label\": label,\n",
    "                    \"run_id\": run_dir_name,\n",
    "                    \"task\": t,\n",
    "                    \"block\": b,\n",
    "                    \"step\": step,\n",
    "                    \"loss\": loss_val,\n",
    "                    \"g_mean\": gmean_by_step[b].get(step, np.nan),\n",
    "                    \"g_frac>0.5\": gfrac_by_step[b].get(step, np.nan),\n",
    "                    \"lr\": lr_by_step.get(step, np.nan),\n",
    "                }\n",
    "                if b == 0:\n",
    "                    for qi in (1,2,3,4):\n",
    "                        row[f\"g_b0_Q{qi}\"] = q_by_step[qi].get(step, np.nan)\n",
    "                rows.append(row)\n",
    "\n",
    "df_task_ts = pd.DataFrame(rows)\n",
    "if df_task_ts.empty:\n",
    "    print(\"No per-task series found.\")\n",
    "else:\n",
    "    df_task_ts = df_task_ts.sort_values([\"label\",\"task\",\"step\",\"block\"], ignore_index=True)\n",
    "    display(df_task_ts.head(20))\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_task_ts.to_csv(out_dir / \"per_task_metrics.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"per_task_metrics.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f354871dcf359754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Rename ./runs/* so directory names include the experiment label (from TB text tags) ---\n",
    "import os, re, glob, time, shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Try to close any active writer so files aren't locked on Windows\n",
    "try:\n",
    "    if 'tb' in globals() and getattr(tb, \"writer\", None) is not None:\n",
    "        tb.flush(); tb.close()\n",
    "        print(\"[rename] Closed active TB writer before renaming.\")\n",
    "except Exception as _e:\n",
    "    print(\"[rename] Writer close note:\", _e)\n",
    "\n",
    "# TensorBoard event loading (version-agnostic size_guidance)\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "    try:\n",
    "        from tensorboard.util import tensor_util as _tb_tensor_util\n",
    "    except Exception:\n",
    "        print(\"failed to load tensorboard utilities, but you'll install tensorflow on this system over my cold dead digital body.\"\n",
    "              \"\\n\\nfix your tensorboard installation and try again\")\n",
    "        _tb_tensor_util = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. `pip install tensorboard`\") from e\n",
    "\n",
    "def _size_guidance_version_safe():\n",
    "    sg = {}\n",
    "    for k in [\"SCALARS\",\"HISTOGRAMS\",\"IMAGES\",\"COMPRESSED_HISTOGRAMS\",\"AUDIO\",\"TENSORS\"]:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "def _decode_text_tensor(tensor_proto) -> Optional[str]:\n",
    "    \"\"\"Decode TB text summary payload from a TensorEvent.tensor_proto.\"\"\"\n",
    "    try:\n",
    "        if _tb_tensor_util is not None:\n",
    "            arr = _tb_tensor_util.make_ndarray(tensor_proto)\n",
    "        else:\n",
    "            # very old stub fallback\n",
    "            arr = _tb_make_ndarray(tensor_proto)\n",
    "        val = arr.item() if arr.size == 1 else arr\n",
    "        if isinstance(val, bytes):\n",
    "            return val.decode(\"utf-8\", \"replace\")\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        # Some TB builds wrap a bytes array inside a 2D array\n",
    "        if hasattr(val, \"dtype\") and str(val.dtype).startswith(\"|S\"):\n",
    "            return val.tobytes().decode(\"utf-8\", \"replace\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_text_tags(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"Try run/label first, then run/meta (JSON with a 'label' field), then None.\"\"\"\n",
    "    ev_files = sorted(glob.glob(str(run_dir / \"events.out.tfevents.*\")))\n",
    "    for ev in reversed(ev_files):\n",
    "        try:\n",
    "            acc = EventAccumulator(ev, size_guidance=_size_guidance_version_safe())\n",
    "            acc.Reload()\n",
    "            tags_t = acc.Tags().get(\"tensors\", []) or []\n",
    "\n",
    "            # 1) Preferred: run/label\n",
    "            if \"run/label\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/label\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        return txt.strip()\n",
    "\n",
    "            # 2) Fallback: run/meta (JSON with label)\n",
    "            if \"run/meta\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/meta\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        txt = txt.strip()\n",
    "                        # Sometimes add_text wraps in small HTML; tolerate raw JSON and simple strings\n",
    "                        m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "                        if m:\n",
    "                            import json\n",
    "                            try:\n",
    "                                meta = json.loads(m.group(0))\n",
    "                                if isinstance(meta, dict) and \"label\" in meta and meta[\"label\"]:\n",
    "                                    return str(meta[\"label\"]).strip()\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        # If it's just a string, return it\n",
    "                        if txt and txt[0] not in \"{<\":\n",
    "                            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_dirname(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"If the dir already has a '-<label>' suffix after timestamp, return that label.\"\"\"\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", run_dir.name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _base_prefix(run_dir: Path) -> str:\n",
    "    \"\"\"Return 'dncformer-YYYYMMDD-HHMMSS' part if present, else the full name.\"\"\"\n",
    "    m = re.match(r\"(.*-\\d{8}-\\d{6})(?:-.+)?$\", run_dir.name)\n",
    "    return m.group(1) if m else run_dir.name\n",
    "\n",
    "def _slugify(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s.strip())[:80] or \"unlabeled\"\n",
    "\n",
    "def _is_active_run(run_dir: Path, seconds: int = 60) -> bool:\n",
    "    \"\"\"Heuristic: if any event file mtime is within the last `seconds`.\"\"\"\n",
    "    now = time.time()\n",
    "    for ev in glob.glob(str(run_dir / \"events.out.tfevents.*\")):\n",
    "        try:\n",
    "            if now - os.path.getmtime(ev) < seconds:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def rename_runs_by_label(log_root: str = \"./runs\", dry_run: bool = True,\n",
    "                         skip_active_secs: int = 60) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Rename run directories under `log_root` to include label suffix (from TB text tag).\n",
    "    Returns list of (old_path, new_path) actually renamed.\n",
    "    \"\"\"\n",
    "    log_root = Path(log_root)\n",
    "    renamed = []\n",
    "\n",
    "    for run_dir in sorted([p for p in log_root.iterdir() if p.is_dir()]):\n",
    "        # Already labeled?\n",
    "        existing_label = _infer_label_from_dirname(run_dir)\n",
    "        # Attempt tag-based label\n",
    "        tag_label = _infer_label_from_text_tags(run_dir)\n",
    "\n",
    "        label = tag_label or existing_label or \"unlabeled\"\n",
    "        label_slug = _slugify(label)\n",
    "\n",
    "        base = _base_prefix(run_dir)\n",
    "        target = log_root / f\"{base}-{label_slug}\"\n",
    "\n",
    "        # Skip if it's already the desired name\n",
    "        if run_dir == target:\n",
    "            print(f\"[rename] OK (already labeled): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Skip if run appears active\n",
    "        if skip_active_secs and _is_active_run(run_dir, skip_active_secs):\n",
    "            print(f\"[rename] SKIP active (mtime<{skip_active_secs}s): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Avoid collisions: add -v2, -v3, ...\n",
    "        cand = target\n",
    "        k = 2\n",
    "        while cand.exists():\n",
    "            cand = log_root / f\"{base}-{label_slug}-v{k}\"\n",
    "            k += 1\n",
    "\n",
    "        print(f\"[rename] {run_dir.name}  ->  {cand.name}\")\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                run_dir.replace(cand)\n",
    "                renamed.append((str(run_dir), str(cand)))\n",
    "            except Exception as e:\n",
    "                print(f\"[rename] FAILED: {run_dir} -> {cand}: {e}\")\n",
    "\n",
    "    return renamed\n",
    "\n",
    "# --- Usage examples ---\n",
    "# 1) Dry run (see planned changes)\n",
    "_ = rename_runs_by_label(\"./runs\", dry_run=True, skip_active_secs=60)\n",
    "\n",
    "# 2) Execute renames\n",
    "# _ = rename_runs_by_label(\"./runs\", dry_run=False, skip_active_secs=60)\n",
    "# print(\"Renamed:\", _)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b12a3d8ecf86f82e"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
