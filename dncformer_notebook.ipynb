{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55276763",
   "metadata": {},
   "source": [
    "\n",
    "# DNCformer: Parallel-Enrichment Transformer–DNC (Notebook Prototype)\n",
    "\n",
    "Implements a **parallel enrichment** architecture that adds a **Transformer-style DNC block** alongside a standard Transformer block, with a learned **gating** to mix their outputs. It wires on top of a **frozen ~4B LLM** (Phi-3-mini-4k-instruct by default), and provides **lightweight train/eval** loops and **unit-like tests**.\n",
    "\n",
    "**Hardware:** designed for a single GPU (e.g., RTX 3090 24GB) using AMP (`bf16` if available, otherwise `fp16`).  \n",
    "**Structure:** Config → Utils → DNC Memory → Transformer Controller → DNCformer Block → Parallel Enrichment → Frozen Base + N Blocks → Data → Train → Eval → Tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Imports, config, and environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f6d41fd5a7206b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.1 Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5acb18ebaa7c254"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abd20f0434c5a8c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T17:59:54.174937400Z",
     "start_time": "2025-08-21T17:59:54.148411500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python exe: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\python.exe\n",
      "torch file: C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\__init__.py\n",
      "torch ver: 2.5.1 | torch.version.cuda: 12.6\n",
      "cuda available: True | device count: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "import os, math, random, time, numpy as np\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import contextlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch ver:\", torch.__version__, \"| torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available(), \"| device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.2 Configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8fc3ee8a8a120f"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd9a9680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:59:55.272852100Z",
     "start_time": "2025-08-21T17:59:55.204866600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda CUDA: True\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_id: str = \"microsoft/Phi-3-mini-4k-instruct\"  # ~3.8B\n",
    "    d_model: Optional[int] = None          # If None, infer from base model hidden size\n",
    "    n_blocks: int = 2                      # number of parallel enrichment blocks\n",
    "    attn_heads: int = 8                    # heads in DNC controller\n",
    "    attn_dropout: float = 0.1\n",
    "    ffn_mult: float = 4.0\n",
    "    dnc_read_heads: int = 2\n",
    "    dnc_cell_size: int = 64                # memory slot width\n",
    "    dnc_nr_cells: int = 256                # number of memory slots\n",
    "    gate_bias_init: float = -1.0           # bias to prefer transformer at init\n",
    "    lr: float = 2e-4                       \n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_len: int = 1024                # training seq length\n",
    "    train_steps: int = 200                 # small sanity pass\n",
    "    warmup_steps: int = 20\n",
    "    grad_clip: float = 1.0\n",
    "    precision: str = \"bf16\"                # \"bf16\" | \"fp16\" | \"fp32\"\n",
    "    use_torch_compile: bool = False\n",
    "    device: str = \"cuda\"\n",
    "    log_every: int = 10\n",
    "    batch_size: int = 8\n",
    "    seed: int = 42\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "device = torch.device(CFG.device if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "amp_dtype = None\n",
    "if CFG.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "elif CFG.precision == \"fp16\" and torch.cuda.is_available():\n",
    "    amp_dtype = torch.float16\n",
    "else:\n",
    "    amp_dtype = torch.float32\n",
    "\n",
    "def cfg_to_json(cfg=None) -> str:\n",
    "    cfg = cfg or CFG\n",
    "    d = {}\n",
    "    for k,v in getattr(cfg, \"__dict__\", {}).items():\n",
    "        if not k.startswith(\"_\"):\n",
    "            try:\n",
    "                json.dumps(v); d[k]=v\n",
    "            except TypeError:\n",
    "                d[k]=str(v)\n",
    "    return json.dumps(d, indent=2)\n",
    "\n",
    "def echo_cfg(to_console: bool = True, to_tb: bool = True, tag: str = \"cfg/json\"):\n",
    "    s = cfg_to_json(CFG)\n",
    "    if to_console:\n",
    "        print(\"=== CFG ===\")\n",
    "        print(s)\n",
    "    if to_tb and 'tb' in globals() and getattr(tb, \"writer\", None):\n",
    "        with contextlib.suppress(Exception):\n",
    "            tb.writer.add_text(tag, s, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.3 Config patch toggles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "926f2d40052c24aa"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class _Tmp: pass\n",
    "    CFG = _Tmp()\n",
    "# safe defaults if not already present\n",
    "if not hasattr(CFG, 'batch_size'): CFG.batch_size = 8\n",
    "if not hasattr(CFG, 'gate_reg_lambda'): CFG.gate_reg_lambda = 0.0   # only applied on memory-tagged batches\n",
    "if not hasattr(CFG, 'hist_every'): CFG.hist_every = 200             # histogram cadence\n",
    "if not hasattr(CFG, 'force_g'): CFG.force_g = None                  # None, or 0.0 or 1.0\n",
    "if not hasattr(CFG, 'gate_temp'): CFG.gate_temp = 1.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T17:59:56.345361400Z",
     "start_time": "2025-08-21T17:59:56.319361200Z"
    }
   },
   "id": "682d4e57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.4 SDPA selection\n",
    "- prefer PyTorch SDPA\n",
    "- avoid flash-attn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc51d39d0304bef"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "945aac73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T17:59:57.359979500Z",
     "start_time": "2025-08-21T17:59:57.318978800Z"
    }
   },
   "outputs": [],
   "source": [
    "def sdpa_ctx():\n",
    "    \"\"\"Return a fresh attention-kernel selection context each time it's called.\n",
    "    Uses PyTorch SDPA (math + mem-efficient) and disables flash-attn to avoid warnings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from torch.backends.cuda import sdp_kernel  # callable context manager in PyTorch 2.x\n",
    "        return sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0.5 Determinism and seeds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f23192fc1053686"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import os, random, numpy as _np, torch, contextlib\n",
    "\n",
    "def set_determinism(seed: int = 42, deterministic: bool = True, cudnn_benchmark: bool = False):\n",
    "    \"\"\"\n",
    "    Set seeds across Python/NumPy/Torch and optionally toggle deterministic algorithms.\n",
    "    deterministic=True may slow kernels; Warn_only to avoid hard errors\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    _np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # cuDNN settings\n",
    "    torch.backends.cudnn.benchmark = bool(cudnn_benchmark)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # Help cublas determinism for some kernels; warn only failure mode\n",
    "        os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "        with contextlib.suppress(Exception):\n",
    "            torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Convenience alias.\"\"\"\n",
    "    set_determinism(seed=seed, deterministic=False, cudnn_benchmark=False)\n",
    "\n",
    "# set CFG seed\n",
    "with contextlib.suppress(Exception):\n",
    "    if hasattr(CFG, \"seed\"):\n",
    "        set_seed(int(CFG.seed))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T17:59:58.827596400Z",
     "start_time": "2025-08-21T17:59:58.800597300Z"
    }
   },
   "id": "4f82989f7aae0850"
  },
  {
   "cell_type": "markdown",
   "id": "2bc75597",
   "metadata": {},
   "source": [
    "## 1. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Model Information and factory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a83e8350cc6d34f"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e47e1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:01.057848400Z",
     "start_time": "2025-08-21T18:00:01.039131400Z"
    }
   },
   "outputs": [],
   "source": [
    "_GLOBAL = {}\n",
    "\n",
    "def causal_mask(sz: int, device=None):\n",
    "    return torch.full((sz, sz), float(\"-inf\"), device=device).triu(1)\n",
    "\n",
    "def count_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def requires_grad_(module: nn.Module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "    return module\n",
    "\n",
    "def print_shapes(**tensors):\n",
    "    for k, v in tensors.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            print(k, [tuple(x.shape) for x in v])\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Gate metrics utils (mean, frac>0.5, entropy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f19b2569a5560d"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def _reduce_gate_tensor(g: torch.Tensor) -> torch.Tensor:\n",
    "    # g: (B,T,*) -> (B,T)\n",
    "    if g.dim() == 3:\n",
    "        return g.mean(dim=-1)\n",
    "    return g\n",
    "\n",
    "@torch.no_grad()\n",
    "def _gate_metrics(g: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns scalar (mean, frac>0.5, entropy)\n",
    "    \"\"\"\n",
    "    g2 = _reduce_gate_tensor(g.detach())\n",
    "    mean_val = float(g2.mean().item())\n",
    "    frac = float((g2 > 0.5).float().mean().item())\n",
    "    eps = 1e-6\n",
    "    p = g2.clamp(eps, 1 - eps)\n",
    "    # binary entropy (natural log base)\n",
    "    ent = float((-(p * (p + eps).log() + (1 - p) * (1 - p + eps).log())).mean().item())\n",
    "    return mean_val, frac, ent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:01.479978900Z",
     "start_time": "2025-08-21T18:00:01.461980600Z"
    }
   },
   "id": "dad9cdfcce225bcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Memory Freeing/handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfb25b111aaa708"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def free_head_and_cache():\n",
    "    # delete typical globals and clear allocator\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['head']\n",
    "    with contextlib.suppress(Exception):\n",
    "        del globals()['tok']\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    cuda_report(\"after free\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:01.889059Z",
     "start_time": "2025-08-21T18:00:01.870059200Z"
    }
   },
   "id": "228ec3411b7b0b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Environment Export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f294b5269755df"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# --- Optional: export environment specs (run in the dncformer conda env) ---\n",
    "import shutil, subprocess, sys\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        print(\">\", cmd); subprocess.run(cmd, shell=True, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Command failed (may be expected on some setups):\", e)\n",
    "        \n",
    "def export_current_env():\n",
    "    print(\"Exporting conda environment and pip freeze to current working directory...\")\n",
    "    _run(\"conda env export --from-history > environment.yml\")\n",
    "    _run(\"conda env export > environment.lock.yml\")\n",
    "    _run(\"python -m pip list --format=freeze > requirements-pip.txt\")\n",
    "    print(\"Done. If any commands failed, run them in your terminal inside the active conda env.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:02.432211100Z",
     "start_time": "2025-08-21T18:00:02.413210700Z"
    }
   },
   "id": "f14fc6a99cc0d7e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 Checkpoint helpers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f221f816cf7b6d63"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "import json, os, contextlib, torch\n",
    "from pathlib import Path\n",
    "\n",
    "def _cfg_to_dict(cfg) -> dict:\n",
    "    out = {}\n",
    "    for k, v in getattr(cfg, \"__dict__\", {}).items():\n",
    "        if not k.startswith(\"_\"):\n",
    "            try:\n",
    "                json.dumps(v)  # test serializability\n",
    "                out[k] = v\n",
    "            except TypeError:\n",
    "                out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def save_head(head: torch.nn.Module, out_dir: str, cfg=None, run_label: str = None):\n",
    "    \"\"\"\n",
    "    Saves only the trainable head parameters (state_dict) to out_dir/{run_label or 'head'}.pt,\n",
    "    plus a metadata JSON with config and light model info.\n",
    "    \"\"\"\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    tag = run_label or \"head\"\n",
    "    ckpt_path = Path(out_dir) / f\"{tag}.pt\"\n",
    "    meta_path = Path(out_dir) / f\"{tag}.meta.json\"\n",
    "\n",
    "    # Only head params\n",
    "    sd = head.state_dict()\n",
    "\n",
    "    # Try to capture minimal head info\n",
    "    info = {\"type\": head.__class__.__name__}\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"blocks\"] = len(getattr(head, \"blocks\", []))\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"d_model\"] = int(getattr(CFG, \"d_model\"))\n",
    "    with contextlib.suppress(Exception):\n",
    "        info[\"base_model_id\"] = getattr(CFG, \"base_model_id\", None)\n",
    "\n",
    "    meta = {\"config\": _cfg_to_dict(cfg or CFG), \"head_info\": info}\n",
    "    torch.save(sd, ckpt_path)\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[save_head] wrote {ckpt_path} and {meta_path}\")\n",
    "\n",
    "def load_head(head: torch.nn.Module, in_path: str, strict: bool = False, map_location=None):\n",
    "    \"\"\"\n",
    "    Loads a head checkpoint (state_dict) into an existing head instance.\n",
    "    \"\"\"\n",
    "    in_path = str(in_path)\n",
    "    sd = torch.load(in_path, map_location=map_location or \"cpu\")\n",
    "    missing, unexpected = head.load_state_dict(sd, strict=strict)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[load_head] missing={missing} unexpected={unexpected}\")\n",
    "    else:\n",
    "        print(\"[load_head] restored successfully.\")\n",
    "    return head"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:02.960787900Z",
     "start_time": "2025-08-21T18:00:02.943787900Z"
    }
   },
   "id": "e31ad229b8ebdd8c"
  },
  {
   "cell_type": "markdown",
   "id": "11924669",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1: DNC Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61be1839c9ebb66f"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13794579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:04.096665100Z",
     "start_time": "2025-08-21T18:00:04.046896500Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DNC memory:\n",
    "    - memory M: (B, N, W)\n",
    "    - usage u: (B, N)\n",
    "    - link L: (B, N, N) temporal links\n",
    "    - precedence p: (B, N)\n",
    "    - read weights rw: (B, R, N)\n",
    "    - write weights ww: (B, N)\n",
    "    - read vectors r: (B, R, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_cells: int, cell_size: int, read_heads: int):\n",
    "        super().__init__()\n",
    "        self.probe = None  # optional callable to record per-step state\n",
    "        self.N = nr_cells\n",
    "        self.W = cell_size\n",
    "        self.R = read_heads\n",
    "\n",
    "    def reset(self, B: int, device=None):\n",
    "        device = device or next(self.parameters(), torch.empty(0, device=\"cpu\")).device\n",
    "        M = torch.zeros(B, self.N, self.W, device=device)\n",
    "        u = torch.zeros(B, self.N, device=device)\n",
    "        L = torch.zeros(B, self.N, self.N, device=device)\n",
    "        p = torch.zeros(B, self.N, device=device)\n",
    "        rw = F.one_hot(torch.zeros(B, self.R, dtype=torch.long, device=device), num_classes=self.N).float()\n",
    "        r = torch.zeros(B, self.R, self.W, device=device)\n",
    "        return {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_sim(M: torch.Tensor, k: torch.Tensor, eps=1e-6):\n",
    "        # M: (B, N, W), k: (B, W) or (B, R, W)\n",
    "        if k.dim() == 2:\n",
    "            k = k.unsqueeze(1)  # (B,1,W)\n",
    "        B, R, W = k.shape\n",
    "        Mnorm = F.normalize(M, p=2, dim=-1)\n",
    "        knorm = F.normalize(k, p=2, dim=-1)\n",
    "        sim = torch.einsum(\"bnw,brw->brn\", Mnorm, knorm)  # (B, R, N)\n",
    "        return sim\n",
    "\n",
    "    def _allocation(self, u: torch.Tensor):\n",
    "        # u: (B, N) in [0,1]\n",
    "        δ = 1e-6\n",
    "        u = δ + (1 - δ) * u                 # avoid tiny values before cumprod\n",
    "        B, N = u.shape\n",
    "        # sort ascending usage -> free list φ\n",
    "        sorted_u, phi = torch.sort(u, dim=-1, descending=False)  # (B,N)\n",
    "        # exclusive cumprod of sorted_u\n",
    "        ones = torch.ones(B, 1, device=u.device, dtype=u.dtype)\n",
    "        prod_excl = torch.cumprod(torch.cat([ones, sorted_u], dim=1), dim=1)[:, :-1]  # (B,N)\n",
    "        a_sorted = (1 - sorted_u) * prod_excl                                        # (B,N)\n",
    "        # invert the sort to original order\n",
    "        inv_phi = torch.argsort(phi, dim=-1)\n",
    "        a = a_sorted.gather(1, inv_phi)                                              # (B,N)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def forward(self, x_if: dict, state: dict):\n",
    "        \"\"\"\n",
    "        x_if (interface dict) must contain:\n",
    "        - k_read: (B,R,W), beta_read: (B,R,1)\n",
    "        - k_write: (B,W), beta_write:(B,1)\n",
    "        - erase: (B,W) in (0,1), write_vec: (B,W)\n",
    "        - free_gates: (B,R,1) in (0,1), alloc_gate:(B,1), write_gate:(B,1) in (0,1)\n",
    "        - read_mode: (B,R,3) softmax over {backward, content, forward}\n",
    "        \"\"\"\n",
    "        M, u, L, p, rw, r = state[\"M\"], state[\"u\"], state[\"L\"], state[\"p\"], state[\"rw\"], state[\"r\"]\n",
    "        B, N, W = M.shape\n",
    "\n",
    "        # --- Usage update (faithful, stable) ---\n",
    "        # previous write weights; keep as (B,1,N) for easy broadcasting\n",
    "        ww_prev = state.get(\"ww\", torch.zeros(M.size(0), 1, self.N, device=M.device, dtype=M.dtype))\n",
    "        # writes increase usage\n",
    "        u = u + (1 - u) * (1 - torch.prod(1 - ww_prev, dim=1))   # -> (B,N)\n",
    "        # free gates release usage at read locations (per-location retention)\n",
    "        psi = torch.prod(1 - x_if[\"free_gates\"] * rw, dim=1)     # (B,N), since free_gates:(B,R,1), rw:(B,R,N)\n",
    "        u = torch.clamp(u * psi, 0, 1)\n",
    "\n",
    "\n",
    "        # 2.1) Write weighting (robust broadcasting) ---\n",
    "        # sim_w: (B,1,N) if k_write=(B,W); (B,R,N) if k_write=(B,R,W)\n",
    "        sim_w = self._cosine_sim(M, x_if[\"k_write\"])\n",
    "        # beta_w: expect (B,1) or (B,R,1); make sure it has the trailing head axis\n",
    "        beta_w = x_if[\"beta_write\"]\n",
    "        if beta_w.dim() == 2:           # (B,1) or (B,R) -> add trailing axis\n",
    "            beta_w = beta_w.unsqueeze(-1)  # -> (B,1,1) or (B,R,1)\n",
    "        # content weights over memory locations\n",
    "        cw = F.softmax(sim_w * beta_w, dim=-1)  # (B,1,N) or (B,R,N)\n",
    "        # canonical DNC: single write head; if multiple heads exist, reduce over heads\n",
    "        if cw.size(1) > 1:\n",
    "            cw = cw.mean(dim=1)                # -> (B,N)  (alternatives: sum or a learned reduce)\n",
    "        else:\n",
    "            cw = cw.squeeze(1)                 # -> (B,N)\n",
    "        # allocation weights from usage (B,N)\n",
    "        a = self._allocation(u)                # (B,N)\n",
    "        # interpolate content vs allocation via alloc_gate, then apply write_gate\n",
    "        alloc = x_if[\"alloc_gate\"]             # (B,1)\n",
    "        write_gate = x_if[\"write_gate\"]        # (B,1)\n",
    "        # Broadcast (B,1) over N\n",
    "        ww = write_gate * (alloc * a + (1.0 - alloc) * cw)  # -> (B,N) via broadcasting\n",
    "        state[\"ww\"] = ww\n",
    "        \n",
    "        # 2.2) save current ww as \"previous write weights\" for t+1\n",
    "        state[\"ww_prev\"] = ww.unsqueeze(1)   # keep grads for BPTT; use .detach() only if you explicitly want to stop gradients across steps\n",
    "\n",
    "        # 3) Memory write\n",
    "        erase = x_if[\"erase\"].unsqueeze(1)  # (B,1,W)\n",
    "        write_vec = x_if[\"write_vec\"].unsqueeze(1)  # (B,1,W)\n",
    "        M = M * (1 - ww.unsqueeze(-1) * erase) + ww.unsqueeze(-1) * write_vec\n",
    "\n",
    "        # 4) Temporal link\n",
    "        prev_p = p\n",
    "        p = (1 - ww.sum(dim=-1, keepdim=True)) * p + ww  # precedence\n",
    "        L = (1 - ww.unsqueeze(2) - ww.unsqueeze(1)) * L + torch.einsum(\"bn,bm->bnm\", prev_p, ww)\n",
    "        L = L * (1 - torch.eye(N, device=M.device).unsqueeze(0))\n",
    "\n",
    "        # 5) Read weighting\n",
    "        cr = F.softmax(self._cosine_sim(M, x_if[\"k_read\"]) * x_if[\"beta_read\"], dim=-1)  # (B,R,N)\n",
    "        fwd = torch.einsum(\"brn,bnm->brm\", rw, L)       # (B,R,N) forward\n",
    "        bwd = torch.einsum(\"brn,bmn->brm\", rw, L)       # (B,R,N) backward\n",
    "        read_mode = F.softmax(x_if[\"read_mode\"], dim=-1)  # (B,R,3)\n",
    "        rw = read_mode[:,:,0:1]*bwd + read_mode[:,:,1:2]*cr + read_mode[:,:,2:3]*fwd\n",
    "        r = torch.einsum(\"brn,bnw->brw\", rw, M)  # (B,R,W)\n",
    "\n",
    "        state = {\"M\": M, \"u\": u, \"L\": L, \"p\": p, \"rw\": rw, \"r\": r}\n",
    "        # aggregated lightweight stats (per-step)\n",
    "        try:\n",
    "            _stats = {}\n",
    "            with torch.no_grad():\n",
    "                _stats[\"u_mean\"] = state[\"u\"].mean().detach()\n",
    "                try:\n",
    "                    _stats[\"M_norm_mean\"] = state[\"M\"].norm(dim=-1).mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"rw_max_mean\"] = rw.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    _stats[\"ww_max_mean\"] = ww.max(dim=-1).values.mean().detach()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            state[\"stats\"] = _stats\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if self.probe is not None:\n",
    "            M = state[\"M\"].detach().float().cpu()\n",
    "            u = state[\"u\"].detach().float().cpu()\n",
    "            L = state[\"L\"].detach().float().cpu()\n",
    "            rw_cpu = rw.detach().float().cpu()\n",
    "            ww_cpu = state.get(\"ww\", torch.zeros_like(state[\"u\"])).detach().float().cpu()\n",
    "            self.probe(\n",
    "                {\n",
    "                \"u\": u,                         # (B,N)\n",
    "                \"ww\": ww_cpu,                   # (B,N)\n",
    "                \"rw\": rw_cpu,                   # (B,R,N)\n",
    "                \"M_norm\": M.norm(dim=-1),       # (B,N)\n",
    "                \"L_diag_mean\": torch.diagonal(L, dim1=-2, dim2=-1).mean(dim=-1), # (B,)\n",
    "                }\n",
    "            )\n",
    "        return r, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c34574",
   "metadata": {},
   "source": [
    "#### 2.2 Transformer-style Controller (sequence mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "415e9bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:04.483651Z",
     "start_time": "2025-08-21T18:00:04.463652400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerController(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer encoder producing the DNC interface vector.\n",
    "    Inputs: X (B, T, d_in); prev_reads typically concatenated to X before calling.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(d_in, d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        h = self.proj_in(x)\n",
    "        h = self.ln1(h)\n",
    "        attn_out, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = h + self.dropout(attn_out)\n",
    "        h = self.ln2(h)\n",
    "        h2 = self.ff(h)\n",
    "        h = h + self.dropout(h2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732c0b7",
   "metadata": {},
   "source": [
    "#### 2.3 DNCformer Block (controller → interface → memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb79598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:05.411804300Z",
     "start_time": "2025-08-21T18:00:05.389804600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNCInterfaceHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects controller hidden to DNC interface:\n",
    "    read keys (R*W), read strengths (R), write key (W), write strength (1),\n",
    "    erase (W in (0,1)), write vector (W), free_gates (R in (0,1)),\n",
    "    alloc_gate (1), write_gate (1), read_mode (R*3 softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, R: int, W: int):\n",
    "        super().__init__()\n",
    "        self.R, self.W = R, W\n",
    "        out = R*W + R + W + 1 + W + W + R + 1 + 1 + R*3\n",
    "        self.proj = nn.Linear(d_model, out)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        B, T, D = h.shape\n",
    "        v = self.proj(h)  # (B,T,out)\n",
    "        idx = 0\n",
    "        def take(sz): \n",
    "            nonlocal idx\n",
    "            part = v[..., idx:idx+sz]; idx += sz; return part\n",
    "        R, W = self.R, self.W\n",
    "        k_read = take(R*W).view(B,T,R,W)\n",
    "        beta_read = F.softplus(take(R)).view(B,T,R,1)\n",
    "        k_write = take(W).view(B,T,W)\n",
    "        beta_write = F.softplus(take(1)).view(B,T,1)\n",
    "        erase = torch.sigmoid(take(W)).view(B,T,W)\n",
    "        write_vec = take(W).view(B,T,W)\n",
    "        free_gates = torch.sigmoid(take(R)).view(B,T,R,1)\n",
    "        alloc_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        write_gate = torch.sigmoid(take(1)).view(B,T,1)\n",
    "        read_mode = take(R*3).view(B,T,R,3)\n",
    "        return {\n",
    "            \"k_read\": k_read, \"beta_read\": beta_read,\n",
    "            \"k_write\": k_write, \"beta_write\": beta_write,\n",
    "            \"erase\": erase, \"write_vec\": write_vec,\n",
    "            \"free_gates\": free_gates, \"alloc_gate\": alloc_gate,\n",
    "            \"write_gate\": write_gate, \"read_mode\": read_mode\n",
    "        }\n",
    "\n",
    "class DNCformerBlock(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float):\n",
    "        super().__init__()\n",
    "        self.R, self.W, self.N = R, W, N\n",
    "        self.ctrl = TransformerController(d_in + R*W, d_model, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.if_head = DNCInterfaceHead(d_model, R=R, W=W)\n",
    "        self.mem = DNCMemory(nr_cells=N, cell_size=W, read_heads=R)\n",
    "        self.out_proj = nn.Linear(d_model + R*W, d_model)  # fuse controller + reads\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Optional[dict]=None):\n",
    "        # x: (B,T,d_in); state carries memory fields; if None -> reset\n",
    "        B, T, D = x.shape\n",
    "        if state is None:\n",
    "            state = self.mem.reset(B, device=x.device)\n",
    "        reads = state[\"r\"].reshape(B, self.R*self.W)  # (B,RW)\n",
    "        reads_seq = reads.unsqueeze(1).expand(B, T, self.R*self.W)\n",
    "        ctrl_in = torch.cat([x, reads_seq], dim=-1)  # concat\n",
    "        h = self.ctrl(ctrl_in, attn_mask=causal_mask(T, device=x.device))  # (B,T,d_model)\n",
    "\n",
    "        # step over time for memory I/O\n",
    "        r_list = []\n",
    "        new_state = state\n",
    "        iface = self.if_head(h)\n",
    "        for t in range(T):\n",
    "            x_if = {k: v[:,t] for k,v in iface.items()}\n",
    "            r_t, new_state = self.mem(x_if, new_state)\n",
    "            r_list.append(r_t)\n",
    "        Rseq = torch.stack(r_list, dim=1)  # (B,T,R,W)\n",
    "        reads_flat = Rseq.reshape(B,T,self.R*self.W)\n",
    "        fused = torch.cat([h, reads_flat], dim=-1)\n",
    "        y = self.out_proj(fused)  # (B,T,d_model)\n",
    "        return y, new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3dae",
   "metadata": {},
   "source": [
    "#### 2.4 Parallel Enrichment Block (Transformer path ‖ DNCformer path + gating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "325e1f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:06.248174Z",
     "start_time": "2025-08-21T18:00:06.221652Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float=0.1, ffn_mult: float=4.0):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, int(d_model*ffn_mult)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(d_model*ffn_mult), d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]=None, gate_override: None = None):\n",
    "        h = self.ln1(x)\n",
    "        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n",
    "        h = x + self.dropout(a)\n",
    "        z = self.ln2(h)\n",
    "        z2 = self.ff(z)\n",
    "        return h + self.dropout(z2)\n",
    "\n",
    "class ParallelEnrichmentBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_in: int, R: int, W: int, N: int, heads: int, dropout: float, ffn_mult: float, gate_bias_init: float):\n",
    "        super().__init__()\n",
    "        self.collect_metrics = False\n",
    "        self.vanilla = VanillaTransformerBlock(d_model, heads, dropout, ffn_mult)\n",
    "        self.dncblock = DNCformerBlock(d_in=d_in, d_model=d_model, R=R, W=W, N=N, heads=heads, dropout=dropout, ffn_mult=ffn_mult)\n",
    "        self.pre_gate_ln = nn.LayerNorm(2*d_model)\n",
    "        self.gate = nn.Linear(2*d_model, d_model)\n",
    "        nn.init.constant_(self.gate.bias, gate_bias_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dnc_state=None, gate_override: None = None):\n",
    "        # Both branches consume the same x (B,T,d_model) and produce (B,T,d_model)\n",
    "        T = x.size(1)\n",
    "        mask = causal_mask(T, device=x.device)\n",
    "        vt = self.vanilla(x, attn_mask=mask)\n",
    "        dt, dnc_state = self.dncblock(x, state=dnc_state)\n",
    "        z = torch.cat([vt, dt], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(self.pre_gate_ln(z)))\n",
    "        out = g*dt + (1-g)*vt\n",
    "        \n",
    "        # collect per-block metrics if requested\n",
    "        if self.collect_metrics:\n",
    "            try:\n",
    "                import math, torch as _t\n",
    "                eps = 1e-6\n",
    "                g_clamp = g.clamp(min=eps, max=1-eps)\n",
    "                g_entropy = (-(g_clamp*_t.log(g_clamp) + (1-g_clamp)*_t.log(1-g_clamp))).mean()\n",
    "                vt_norm = vt.norm(dim=-1).mean()\n",
    "                dt_norm = dt.norm(dim=-1).mean()\n",
    "                _stats = dnc_state.get(\"stats\", {}) if isinstance(dnc_state, dict) else {}\n",
    "                # ensure CPU-detached tiny tensors\n",
    "                self.last_metrics = {\n",
    "                    \"g_mean\": g.mean().detach(),\n",
    "                    \"g_entropy\": g_entropy.detach(),\n",
    "                    \"vt_norm\": vt_norm.detach(),\n",
    "                    \"dt_norm\": dt_norm.detach(),\n",
    "                    **_stats\n",
    "                }\n",
    "            except Exception:\n",
    "                self.last_metrics = None\n",
    "        else:\n",
    "            self.last_metrics = None\n",
    "            return out, dnc_state, g \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153a50b",
   "metadata": {},
   "source": [
    "#### 2.5. Frozen Base LLM + N Enrichment Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73c62ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:06.821817200Z",
     "start_time": "2025-08-21T18:00:06.802817600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16) if amp_dtype!=torch.float32 else None,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    requires_grad_(model, False)\n",
    "    model.config.output_hidden_states = True\n",
    "    model.config.use_cache = False\n",
    "    return tok, model\n",
    "\n",
    "class DNCFormerHead(nn.Module):\n",
    "    def __init__(self, base: AutoModelForCausalLM, cfg):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        d_model = base.config.hidden_size if cfg.d_model is None else cfg.d_model\n",
    "        self.d_model = d_model\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ParallelEnrichmentBlock(\n",
    "                d_model=d_model, d_in=d_model,\n",
    "                R=cfg.dnc_read_heads, W=cfg.dnc_cell_size, N=cfg.dnc_nr_cells,\n",
    "                heads=cfg.attn_heads, dropout=cfg.attn_dropout,\n",
    "                ffn_mult=cfg.ffn_mult, gate_bias_init=cfg.gate_bias_init\n",
    "            ) for _ in range(cfg.n_blocks)\n",
    "        ])\n",
    "        self.proj_out = nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1]  # (B,T,d_model)\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i])\n",
    "            gates.append(g.detach())\n",
    "        logits = self.base.lm_head(self.proj_out(h).to(self.base.lm_head.weight.dtype))\n",
    "        return logits, gates\n",
    "\n",
    "\n",
    "\n",
    "    def forward_with_metrics(self, input_ids: torch.Tensor, attention_mask: \"Optional[torch.Tensor]\" = None,\n",
    "                             gate_override: \"Optional[float]\" = None):\n",
    "        with torch.no_grad():\n",
    "            with sdpa_ctx():\n",
    "                out = self.base(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "        h = out.hidden_states[-1].to(device)  # ensure CUDA\n",
    "        dnc_states = [None]*len(self.blocks)\n",
    "        gates_det = []\n",
    "        gates_raw = []\n",
    "        per_block = []\n",
    "        # enable metrics collection per block\n",
    "        for blk in self.blocks:\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = True\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            h, dnc_states[i], g = blk(h, dnc_state=dnc_states[i], gate_override=gate_override)\n",
    "            gates_raw.append(g)\n",
    "            gates_det.append(g.detach())\n",
    "            if hasattr(blk, \"last_metrics\") and blk.last_metrics is not None:\n",
    "                per_block.append(blk.last_metrics)\n",
    "            else:\n",
    "                per_block.append({})\n",
    "            # reset flag to avoid overhead elsewhere\n",
    "            if hasattr(blk, \"collect_metrics\"):\n",
    "                blk.collect_metrics = False\n",
    "        # lm head on its own device, then back to CUDA\n",
    "        lm_dev = self.base.lm_head.weight.device\n",
    "        y = self.proj_out(h).to(lm_dev, dtype=self.base.lm_head.weight.dtype)\n",
    "        logits = self.base.lm_head(y).to(device)\n",
    "        aux = {\"per_block\": per_block, \"gates_raw\": gates_raw, \"gates_detached\": gates_det}\n",
    "        aux[\"g_entropy_block\"] = []\n",
    "        for g in gates_det:\n",
    "            _, _, ent = _gate_metrics(g)\n",
    "            aux[\"g_entropy_block\"].append(ent)\n",
    "        return logits, gates_det, aux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3a6f9",
   "metadata": {},
   "source": [
    "## 3. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Synthetic tasks + simple instruction-following"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372040027d86e088"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "775f2c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:07.684816300Z",
     "start_time": "2025-08-21T18:00:07.650817100Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_copy_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    return x\n",
    "\n",
    "def make_reverse_task(batch, T, vocab=50):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    y = torch.flip(x, dims=[1])\n",
    "    return x, y\n",
    "\n",
    "def make_needle_task(batch, T, needle_len=5, vocab=100):\n",
    "    x = torch.randint(5, vocab, (batch, T))\n",
    "    for b in range(batch):\n",
    "        start = random.randint(0, T-needle_len-5)\n",
    "        needle = torch.randint(5, vocab, (needle_len,))\n",
    "        x[b, start:start+needle_len] = needle\n",
    "        x[b, -1] = needle[0]\n",
    "    return x\n",
    "\n",
    "INSTR_PAIRS = [\n",
    "    (\"Reverse the string: abcd\", \"dcba\"),\n",
    "    (\"Add two numbers: 7 + 12\", \"19\"),\n",
    "    (\"Instruction: say hello\", \"hello\"),\n",
    "    (\"Uppercase this: cat\", \"CAT\"),\n",
    "]\n",
    "\n",
    "def tokenize_instruction_pairs(tok, pairs, max_len):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" for p,_ in pairs]\n",
    "    labels = [ans for _,ans in pairs]\n",
    "    input_ids = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    label_ids = tok(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    # naive pack: just use inputs; real packing/labels can be elaborated\n",
    "    return input_ids, label_ids\n",
    "\n",
    "def make_repeat_copy(batch: int, T: int, repeat_min=2, repeat_max=4, vocab=100, pad_id: int = 0, device: str = \"cpu\") -> torch.Tensor:\n",
    "    L = max(1, T // 2)\n",
    "    x = torch.randint(1, vocab, (batch, L), device=device, dtype=torch.long)\n",
    "    r = torch.randint(repeat_min, repeat_max + 1, (batch,), device=device)\n",
    "    out = torch.full((batch, T), pad_id, dtype=torch.long, device=device)\n",
    "    for i in range(batch):\n",
    "        seq = x[i].repeat_interleave(int(r[i].item()))\n",
    "        out[i, :min(T, seq.numel())] = seq[:T]\n",
    "    return out\n",
    "\n",
    "def make_n_back(batch: int, T: int, n: int = 3, vocab=50) -> torch.Tensor:\n",
    "    return torch.randint(1, vocab, (batch, T))\n",
    "\n",
    "def format_instruction(tok, instr: str, resp: str, max_len=256) -> torch.Tensor:\n",
    "    prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{resp}\"\n",
    "    return tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 HF dataset integration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eaf6ff06d518fec"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb23da95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:08.406058400Z",
     "start_time": "2025-08-21T18:00:08.358946500Z"
    }
   },
   "outputs": [],
   "source": [
    "def hf_instruction_loader(dataset_name=\"tatsu-lab/alpaca\", split=\"train\", text_field=(\"instruction\",\"output\"), max_items=5000):\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception:\n",
    "        print(\"Install 'datasets' to enable HF loading: pip install datasets -q\")\n",
    "        return []\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    pairs = []\n",
    "    i_field, o_field = text_field\n",
    "    for ex in ds:\n",
    "        instr = ex.get(i_field, \"\"); out = ex.get(o_field, \"\")\n",
    "        if instr and out: pairs.append((instr, out))\n",
    "        if len(pairs) >= max_items: break\n",
    "    random.shuffle(pairs); return pairs\n",
    "\n",
    "def make_hf_batch(tok, pairs: List[Tuple[str,str]], batch: int, max_len=256) -> torch.Tensor:\n",
    "    if not pairs:\n",
    "        return torch.full((batch, max_len), tok.pad_token_id, dtype=torch.long)\n",
    "    batch_ids = []\n",
    "    for _ in range(batch):\n",
    "        instr, out = random.choice(pairs)\n",
    "        ids = format_instruction(tok, instr, out, max_len=max_len)\n",
    "        batch_ids.append(ids)\n",
    "    maxL = min(max(x.size(0) for x in batch_ids), max_len)\n",
    "    out_ids = torch.full((batch, maxL), tok.pad_token_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        ids = ids[:maxL]; out_ids[i, :ids.size(0)] = ids\n",
    "    return out_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Haystack batch creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20de8c6a7c630b74"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# --- Haystack (needle) eval: long-span retrieval of a key's value ---\n",
    "def make_haystack_batch(batch: int, T: int = 256, vocab: int = 1024, sentinel: int = 3):\n",
    "    \"\"\"\n",
    "    Build sequences: ... K V ... K ?  (next token should be V)\n",
    "    Returns: input_ids [B,T], answer_ids [B], query_pos [B] (position of '?')\n",
    "    \"\"\"\n",
    "    assert T >= 12, \"T too small for haystack layout\"\n",
    "    x = torch.randint(5, vocab, (batch, T), dtype=torch.long)\n",
    "    K = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "    V = torch.randint(5, vocab, (batch,), dtype=torch.long)\n",
    "\n",
    "    # Place (K,V) in the first half\n",
    "    p1 = torch.randint(low=T//8, high=T//2 - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p1] = K\n",
    "    x[torch.arange(batch), p1 + 1] = V\n",
    "\n",
    "    # Place (K, sentinel) in the last quarter\n",
    "    p2 = torch.randint(low=3*T//4, high=T - 2, size=(batch,))\n",
    "    x[torch.arange(batch), p2] = K\n",
    "    x[torch.arange(batch), p2 + 1] = sentinel  # '?'\n",
    "    query_pos = p2 + 1  # position of '?'; we will look at logits at this position\n",
    "\n",
    "    return x, V, query_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:09.102764400Z",
     "start_time": "2025-08-21T18:00:09.085766900Z"
    }
   },
   "id": "3ef03e6eaa8a61bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4 MixtureSampler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b4f90ad9bf9b7"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class MixtureSampler:\n",
    "    def __init__(self, gens: List, weights: List[float], names: Optional[List[str]] = None):\n",
    "        self.gens = gens\n",
    "        import torch as _t\n",
    "        self.weights = list(map(float, weights))\n",
    "        self.p = _t.tensor(self.weights, dtype=_t.float32)  # CPU is fine for multinomial\n",
    "        self.p /= (self.p.sum() + 1e-8)\n",
    "        self.names = names if names is not None else [f\"g{i}\" for i in range(len(gens))]\n",
    "        self.last_name = None\n",
    "\n",
    "    def __call__(self, batch: int) -> torch.Tensor:\n",
    "        import torch as _t\n",
    "        idx = _t.multinomial(self.p, 1).item()\n",
    "        self.last_name = self.names[idx]\n",
    "        return self.gens[idx](batch)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Update mixture probabilities at runtime (used by schedules).\"\"\"\n",
    "        import torch as _t\n",
    "        ws = list(map(float, weights))\n",
    "        t = _t.tensor(ws, dtype=_t.float32, device=self.p.device)\n",
    "        s = float(t.sum().item())\n",
    "        if s <= 0:\n",
    "            raise ValueError(\"Mixture weights must sum to > 0\")\n",
    "        self.p = t / s\n",
    "        self.weights = ws\n",
    "        # Optional sanity: warn if names length mismatches weights\n",
    "        if hasattr(self, \"names\") and len(self.names) != len(ws):\n",
    "            print(f\"[MixtureSampler] Warning: len(names)={len(self.names)} != len(weights)={len(ws)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:09.785155500Z",
     "start_time": "2025-08-21T18:00:09.767157800Z"
    }
   },
   "id": "c47d40c7b9fdc5ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.5 Mixture Builder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6db5be99034904"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def _build_mixer(tok, weights, hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=2000) -> MixtureSampler:\n",
    "    \"\"\"\n",
    "    Build a mixture of generators: [HF, copy, repeat, nback].\n",
    "    - If hf_dataset is Alpaca-like (has instruction/output), we use hf_instruction_loader + make_hf_batch.\n",
    "    - Else we treat it as text-only (e.g., roneneldan/TinyStories), tokenizing and making windowed sequences.\n",
    "    - If HF fails/empty, we drop it and renormalize over synthetics.\n",
    "    \"\"\"\n",
    "    mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "    pad_id = getattr(tok, \"pad_token_id\", 0) or 0\n",
    "\n",
    "    gens, wts, names = [], [], []\n",
    "\n",
    "    hf_ok = False\n",
    "    hf_reason = \"\"\n",
    "    gen_hf = None\n",
    "\n",
    "    if hf_dataset:\n",
    "        try:\n",
    "            # Heuristic: use instruction loader for alpaca-like IDs\n",
    "            if \"alpaca\" in hf_dataset.lower():\n",
    "                pairs = hf_instruction_loader(hf_dataset, \"train\", (\"instruction\", \"output\"),\n",
    "                                              max_items=hf_max_items)\n",
    "                if pairs:\n",
    "                    def gen_hf(b): \n",
    "                        return make_hf_batch(tok, pairs, b, max_len=mx)\n",
    "                    hf_ok = True\n",
    "                else:\n",
    "                    hf_reason = \"hf_instruction_loader returned 0 pairs\"\n",
    "            else:\n",
    "                # Text-only fallback (e.g., roneneldan/TinyStories)\n",
    "                from datasets import load_dataset\n",
    "                # Try streaming first, then non-streaming\n",
    "                try:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\", streaming=True)\n",
    "                except Exception:\n",
    "                    ds = load_dataset(hf_dataset, split=\"train\")\n",
    "\n",
    "                # Find a usable text key\n",
    "                text_key = None\n",
    "                common_keys = (\"text\", \"content\", \"story\", \"document\", \"body\", \"article\")\n",
    "\n",
    "                feats = getattr(ds, \"features\", None)\n",
    "                if feats:\n",
    "                    for k in common_keys:\n",
    "                        if k in feats:\n",
    "                            text_key = k\n",
    "                            break\n",
    "\n",
    "                if text_key is None:\n",
    "                    # Probe first example (works for streaming iterable)\n",
    "                    try:\n",
    "                        first_ex = next(iter(ds))\n",
    "                        for k in common_keys:\n",
    "                            if k in first_ex:\n",
    "                                text_key = k\n",
    "                                break\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "\n",
    "                if text_key is None:\n",
    "                    hf_reason = \"no usable text field (tried: %s)\" % \",\".join(common_keys)\n",
    "                else:\n",
    "                    import random as _rnd\n",
    "                    import torch\n",
    "                    samples = []\n",
    "                    for ex in ds:\n",
    "                        txt = ex.get(text_key, None)\n",
    "                        if not txt:\n",
    "                            continue\n",
    "                        ids = tok(txt, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "                        if ids.numel() < 8:\n",
    "                            continue\n",
    "                        samples.append(ids.cpu())\n",
    "                        if len(samples) >= int(hf_max_items):\n",
    "                            break\n",
    "\n",
    "                    if len(samples) == 0:\n",
    "                        hf_reason = \"collected 0 tokenized samples\"\n",
    "                    else:\n",
    "                        def gen_hf(b: int) -> torch.Tensor:\n",
    "                            out = []\n",
    "                            for _ in range(b):\n",
    "                                ids = _rnd.choice(samples)\n",
    "                                n = ids.numel()\n",
    "                                if n >= mx:\n",
    "                                    s = _rnd.randint(0, n - mx)\n",
    "                                    seq = ids[s:s+mx]\n",
    "                                else:\n",
    "                                    pad = torch.full((mx - n,), pad_id, dtype=torch.long)\n",
    "                                    seq = torch.cat([ids, pad], dim=0)\n",
    "                                out.append(seq.unsqueeze(0))\n",
    "                            return torch.cat(out, dim=0)\n",
    "                        hf_ok = True\n",
    "        except Exception as e:\n",
    "            hf_reason = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "    # Assemble mixture in the agreed order (hf, copy, repeat, nback)\n",
    "    if hf_ok and gen_hf is not None:\n",
    "        gens.append(gen_hf); wts.append(weights[0]); names.append(\"hf\")\n",
    "        s_w = list(weights[1:])\n",
    "    else:\n",
    "        if hf_dataset:\n",
    "            print(f\"HF dataset unavailable or empty; using synthetic only. reason={hf_reason}\")\n",
    "        s_w = list(weights[1:])  # keep caller's relative weights for synthetics\n",
    "\n",
    "    # Synthetic tasks (your existing closures)\n",
    "    def gen_copy(b):   return make_copy_task(b, T=min(mx, 128), vocab=100)\n",
    "    def gen_repeat(b): return make_repeat_copy(b, T=min(mx, 128), vocab=100, pad_id=pad_id, device=\"cpu\")\n",
    "    def gen_nback(b):  return make_n_back(b, T=min(mx, 128), n=5, vocab=50)\n",
    "\n",
    "    gens.extend([gen_copy, gen_repeat, gen_nback])\n",
    "    wts.extend(s_w)\n",
    "    names.extend([\"copy\", \"repeat\", \"nback\"])\n",
    "\n",
    "    return MixtureSampler(gens, wts, names=names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:10.437776600Z",
     "start_time": "2025-08-21T18:00:10.383777700Z"
    }
   },
   "id": "d629080a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Logging utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a8d8e756c4c8a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 TensorBoard Logger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "189691151b04221f"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    TB_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"TensorBoard not available:\", e)\n",
    "    TB_AVAILABLE = False\n",
    "\n",
    "class TBLogger:\n",
    "    def __init__(self, logdir: Optional[str] = None, run_name: Optional[str] = None):\n",
    "        self.enabled = TB_AVAILABLE\n",
    "        self.writer = None\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        logdir = logdir or \"./runs\"\n",
    "        run_name = run_name or time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "        self.path = os.path.join(logdir, run_name)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.path)\n",
    "\n",
    "    def log_scalars(self, step: int, loss: float, lr: float, gate_means: List[float]):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_scalar(\"train/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"train/lr\", lr, step)\n",
    "        for i, gm in enumerate(gate_means):\n",
    "            self.writer.add_scalar(f\"gates/block_{i}_mean\", gm, step)\n",
    "\n",
    "    def add_image_hw(self, tag: str, img_hw: \"torch.Tensor\", step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        x = img_hw\n",
    "        if x.device.type != \"cpu\": x = x.cpu()\n",
    "        x = x.float()\n",
    "        if x.numel() > 0:\n",
    "            m, M = x.min(), x.max()\n",
    "            x = (x - m) / (M - m + 1e-8)\n",
    "        x = x.unsqueeze(0)  # [1,H,W]\n",
    "        self.writer.add_image(tag, x, step, dataformats='CHW')\n",
    "\n",
    "    def add_histogram(self, tag: str, values: \"torch.Tensor\", step: int, bins: int = 50):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        import torch\n",
    "        v = values\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.detach()\n",
    "            if v.device.type != \"cpu\": v = v.cpu()\n",
    "            v = v.reshape(-1).float()\n",
    "        self.writer.add_histogram(tag, v, global_step=step, bins=bins)\n",
    "\n",
    "    def add_text(self, tag: str, text: str, step: int):\n",
    "        if not (self.enabled and self.writer): return\n",
    "        self.writer.add_text(tag, text, step)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.enabled and self.writer: self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.enabled and self.writer: self.writer.close()\n",
    "        \n",
    "        \n",
    "def start_tb_run(label: str = None, logdir: str = \"./runs\"):\n",
    "    \"\"\"Close any existing TB writer and open a fresh run dir with timestamp + optional label.\"\"\"\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TensorBoard not available; skipping start_tb_run.\")\n",
    "        return False\n",
    "    global tb\n",
    "    # Close/flush a previous writer if present\n",
    "    try:\n",
    "        if 'tb' in globals() and isinstance(tb, TBLogger) and getattr(tb, \"writer\", None):\n",
    "            tb.flush(); tb.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ts = time.strftime(\"dncformer-%Y%m%d-%H%M%S\")\n",
    "    run_name = ts\n",
    "    if label:\n",
    "        safe = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(label))\n",
    "        run_name = f\"{ts}-{safe}\"\n",
    "\n",
    "    tb = TBLogger(logdir=logdir, run_name=run_name)\n",
    "    tb.add_text(\"run/label\", str(label or \"unlabeled\"), 0)\n",
    "    print(\"TB run started:\", getattr(tb, \"path\", None))\n",
    "    return True\n",
    "\n",
    "# TODO: wire this into start_tb_run above at some stage\n",
    "# try:\n",
    "#     _orig_start_tb_run = start_tb_run\n",
    "#     def start_tb_run(label: str = None):\n",
    "#         tb_obj = _orig_start_tb_run(label)\n",
    "#         echo_cfg(to_console=False, to_tb=True, tag=\"cfg/json\")\n",
    "#         return tb_obj\n",
    "# except Exception:\n",
    "#     pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:11.577826300Z",
     "start_time": "2025-08-21T18:00:11.556807300Z"
    }
   },
   "id": "b44d73c717b8b58c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Memory tracer & TensorBoard memory visualizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6636493a9a0eb2"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracer:\n",
    "    def __init__(self): self.frames = []\n",
    "    def __call__(self, frame): self.frames.append(frame)\n",
    "    def stack(self, key, bidx: int = 0):\n",
    "        import torch\n",
    "        return torch.stack([f[key][bidx] for f in self.frames], dim=0)  # [T, ...]\n",
    "\n",
    "@contextmanager\n",
    "def trace_memory(module):\n",
    "    tracer = MemoryTracer()\n",
    "    memories = [m for m in module.modules() if m.__class__.__name__ == \"DNCMemory\"]\n",
    "    for m in memories: m.probe = tracer\n",
    "    try:\n",
    "        yield tracer\n",
    "    finally:\n",
    "        for m in memories: m.probe = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_memory_tb(head, tok, writer, global_step: int, prompt=\"### Instruction:\\nRemember A then B; later return A.\\n\\n### Response:\\n\", max_T=64):\n",
    "    if writer is None: return\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    enc.input_ids = enc.input_ids[:, :max_T]\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(enc.input_ids)\n",
    "\n",
    "    u = tracer.stack(\"u\")           # [T, N]\n",
    "    Mnorm = tracer.stack(\"M_norm\")  # [T, N]\n",
    "    rw = tracer.stack(\"rw\")         # [T, R, N]\n",
    "\n",
    "    # Log images\n",
    "    writer.add_image(\"memory/u_TxN\", (u.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "    writer.add_image(\"memory/Mnorm_TxN\", (Mnorm.T).unsqueeze(0), global_step, dataformats='CHW')\n",
    "\n",
    "    top_read = rw.argmax(dim=-1).float()  # [T,R]\n",
    "    top_read_img = (top_read / max(1, rw.size(-1)-1)).T  # [R,T]\n",
    "    writer.add_image(\"memory/top_read_RxT\", top_read_img.unsqueeze(0), global_step, dataformats='CHW')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:12.272817Z",
     "start_time": "2025-08-21T18:00:12.241733500Z"
    }
   },
   "id": "028e39a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0322be9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1 Schedulers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eac969641de0772d"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# --- LR Scheduler: linear warmup -> cosine decay (nonzero start) ---\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, warmup_steps: int, total_steps: int, min_lr_ratio: float = 0.10):\n",
    "    \"\"\"\n",
    "    Warms up linearly from 0->1 over warmup_steps; cosine decays from 1->min_lr_ratio for the remainder.\n",
    "    Uses step_idx+1 to avoid zero LR at the start.\n",
    "    \"\"\"\n",
    "    warmup_steps = max(1, int(warmup_steps))\n",
    "    total_steps = max(warmup_steps + 1, int(total_steps))\n",
    "\n",
    "    def lr_lambda(step_idx: int):\n",
    "        s = step_idx + 1\n",
    "        if s <= warmup_steps:\n",
    "            return s / float(warmup_steps)\n",
    "        progress = (s - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:17.031577800Z",
     "start_time": "2025-08-21T18:00:17.005579200Z"
    }
   },
   "id": "b6be4ba6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2 Training Utilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7a398e8eff544"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    tok, base = load_base_model(CFG.base_model_id)\n",
    "    if CFG.d_model is None:\n",
    "        CFG.d_model = base.config.hidden_size\n",
    "    head = DNCFormerHead(base, CFG).to(device)\n",
    "    if CFG.use_torch_compile and hasattr(torch, 'compile'):\n",
    "        head = torch.compile(head)\n",
    "    #print(\"Trainable params in head:\", count_params(head))\n",
    "    return tok, head\n",
    "\n",
    "def make_optimizer(model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.AdamW(params, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "def lm_shift_labels(input_ids, logits, tok):\n",
    "    labels = input_ids[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=tok.pad_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:17.990270800Z",
     "start_time": "2025-08-21T18:00:17.968272300Z"
    }
   },
   "id": "a70a7bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.3 Experimental training loop, stage 1 experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597e27677e1220b4"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train_experiment(\n",
    "    steps: int = None,\n",
    "    batch_size: int = None,\n",
    "    warmup_steps: int = None,\n",
    "    min_lr_ratio: float = 0.1,\n",
    "    mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "    hf_dataset: str = \"tatsu-lab/alpaca\",\n",
    "    hf_max_items: int = 5000,\n",
    "    log_every: int = None,\n",
    "    viz_memory_after: bool = False,\n",
    "    viz_prompt: str = \"### Instruction:\\nSay hello in one word\\n\\n### Response:\\n\",\n",
    "    viz_max_T: int = 64,\n",
    "    # NEW: optional schedules (each a list of (until_step, value))\n",
    "    mixture_schedule=None,         # e.g., [(100, (0.3,0.3,0.2,0.2)), (None, (0.4,0.2,0.2,0.2))]\n",
    "    gate_temp_schedule=None,       # e.g., [(100, 0.8), (None, 1.0)]\n",
    "    gate_reg_schedule=None,        # e.g., [(100, 3e-4), (None, 2e-4)]\n",
    "):\n",
    "    cfg = CFG\n",
    "    steps = int(steps or cfg.train_steps)\n",
    "    batch_size = int(batch_size or getattr(cfg, \"batch_size\", 8))\n",
    "    warmup_steps = int(warmup_steps if warmup_steps is not None else getattr(cfg, \"warmup_steps\", max(10, steps // 20)))\n",
    "    log_every = int(log_every if log_every is not None else getattr(cfg, \"log_every\", 10))\n",
    "\n",
    "    # Model & tokenizer\n",
    "    tok, head = build_model_and_tokenizer()\n",
    "    optim = make_optimizer(head)\n",
    "    scheduler = make_warmup_cosine_scheduler(optim, warmup_steps, steps, min_lr_ratio=min_lr_ratio)\n",
    "\n",
    "    # Sampler (and allow schedules to change weights)\n",
    "    mixer = _build_mixer(tok, mixture_weights, hf_dataset=hf_dataset, hf_max_items=hf_max_items)\n",
    "\n",
    "    def _apply_schedules(step: int, mix_name_hint=None):\n",
    "        # mixture schedule\n",
    "        if mixture_schedule:\n",
    "            for until, ws in mixture_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    try:\n",
    "                        mixer.set_weights(ws)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "        # gate temp schedule\n",
    "        if gate_temp_schedule:\n",
    "            for until, temp in gate_temp_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_temp\", float(temp))\n",
    "                    break\n",
    "        # gate reg schedule\n",
    "        if gate_reg_schedule:\n",
    "            for until, lam in gate_reg_schedule:\n",
    "                if until is None or step <= int(until):\n",
    "                    setattr(CFG, \"gate_reg_lambda\", float(lam))\n",
    "                    break\n",
    "\n",
    "    # TB setup\n",
    "    global tb\n",
    "    if TB_AVAILABLE:\n",
    "        try:\n",
    "            tb  # NameError if not defined\n",
    "        except NameError:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        if not isinstance(tb, TBLogger) or getattr(tb, \"writer\", None) is None:\n",
    "            tb = TBLogger(logdir=\"./runs\")\n",
    "        tblog = True\n",
    "        tb.add_text(\"run/config\", json.dumps({\n",
    "            \"steps\": steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"warmup_steps\": warmup_steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    else:\n",
    "        tblog = False\n",
    "\n",
    "    head.train()\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(amp_dtype in (torch.float16, torch.bfloat16)))\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        _apply_schedules(step)\n",
    "\n",
    "        in_ids = mixer(batch_size).to(device)\n",
    "        with torch.autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype != torch.float32)):\n",
    "            logits, gates, aux = head.forward_with_metrics(in_ids, gate_override=getattr(CFG, \"force_g\", None))\n",
    "            loss = lm_shift_labels(in_ids, logits, tok)\n",
    "\n",
    "            # Optional: mild gate-usage regularizer (same as before)\n",
    "            lam = float(getattr(CFG, \"gate_reg_lambda\", 0.0))\n",
    "            if lam > 0 and isinstance(gates, (list, tuple)) and len(gates) > 0:\n",
    "                reg = 0.0\n",
    "                for g in gates:\n",
    "                    g2 = _reduce_gate_tensor(g)\n",
    "                    # Encourage decisive routing on memory batches only? For now, uniform\n",
    "                    reg = reg + (g2.mean() * 0.0 + (g2 * (1 - g2)).mean())  # small entropy-like penalty\n",
    "                loss = loss + lam * reg\n",
    "\n",
    "        use_scaler = (amp_dtype in (torch.float16, torch.bfloat16))\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optim); scaler.update(); optim.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(head.parameters(), CFG.grad_clip)\n",
    "            optim.step(); optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # --- Logging (TB) ---\n",
    "        if step % log_every == 0 and tblog:\n",
    "            # global scalars\n",
    "            tb.writer.add_scalar(\"train/loss\", float(loss.item()), step)\n",
    "            tb.writer.add_scalar(\"train/lr\", float(scheduler.get_last_lr()[0]), step)\n",
    "\n",
    "            # gate metrics per block (global)\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_mean\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_frac>0.5\", g_frac, step)\n",
    "                    tb.writer.add_scalar(f\"gates/block_{bi}_entropy\", g_ent, step)\n",
    "\n",
    "                # per-task metrics (using the sampler's last batch type)\n",
    "                mix_name = getattr(mixer, \"last_name\", None) or \"unknown\"\n",
    "                tb.writer.add_scalar(f\"loss_by_task/{mix_name}\", float(loss.item()), step)\n",
    "                for bi, g in enumerate(gates):\n",
    "                    g_mean, g_frac, g_ent = _gate_metrics(g)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_mean/{mix_name}\", g_mean, step)\n",
    "                    tb.writer.add_scalar(f\"gates_by_task/block_{bi}_frac>0.5/{mix_name}\", g_frac, step)\n",
    "\n",
    "                # optional: quartiles (block 0)\n",
    "                if len(gates) > 0:\n",
    "                    g0 = _reduce_gate_tensor(gates[0].detach())\n",
    "                    T = g0.size(1); q = max(1, T // 4)\n",
    "                    slices = [(0, q), (q, 2*q), (2*q, 3*q), (3*q, T)]\n",
    "                    for qi, (s, e) in enumerate(slices, start=1):\n",
    "                        tb.writer.add_scalar(f\"gates/block0_q{qi}_mean/{mix_name}\", float(g0[:, s:e].mean().item()), step)\n",
    "\n",
    "            tb.flush()\n",
    "\n",
    "        # --- Console echo (always) ---\n",
    "        if step % log_every == 0:\n",
    "            g_means_print = []\n",
    "            if isinstance(gates, (list, tuple)):\n",
    "                for g in gates:\n",
    "                    g_means_print.append(float(_reduce_gate_tensor(g).mean().item()))\n",
    "            print(f\"step {step} | loss {loss.item():.4f} | lr {scheduler.get_last_lr()[0]:.2e} | \"\n",
    "                  f\"gates={g_means_print} | mix={getattr(mixer,'last_name','?')}\")\n",
    "\n",
    "    # Optional memory viz\n",
    "    if viz_memory_after:\n",
    "        try:\n",
    "            visualize_memory_tb(head, tok, tb.writer, global_step=steps, prompt=viz_prompt, max_T=viz_max_T)\n",
    "        except Exception as e:\n",
    "            print(\"Memory TB viz skipped:\", e)\n",
    "    \n",
    "    # flush log      \n",
    "    if tblog:\n",
    "        tb.flush()\n",
    "\n",
    "    return head, tok\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:36.267409300Z",
     "start_time": "2025-08-21T18:00:36.239822500Z"
    }
   },
   "id": "f8879a898faf706a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.4 Robust forward for ParallelEnrichmentBlock\n",
    "TODO: this is a monkeypatch, non-critical but fold in properly when I have time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569f513137ac4911"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# # --- Patch: robust forward for ParallelEnrichmentBlock ---\n",
    "def _peb_forward(self, x, dnc_state=None, gate_override=None):\n",
    "    \"\"\"\n",
    "    Returns: (out, dnc_state, g)\n",
    "      out: (B,T,D), mixed from vanilla (vt) and DNC (dt)\n",
    "      dnc_state: updated state dict from DNC path\n",
    "      g: (B,T,D) or (B,T,1) gate values in [0,1]\n",
    "    \"\"\"\n",
    "    # 1) Vanilla transformer path\n",
    "    T = x.size(1)\n",
    "    mask = causal_mask(T, device=x.device)  # (T,T) causal attn mask\n",
    "    x_cast = x.to(self.vanilla.ln1.weight.dtype)\n",
    "    vt = self.vanilla(x_cast, attn_mask=mask)    # (B,T,D)\n",
    "\n",
    "    # 2) DNC path (controller+memory)\n",
    "    dt, dnc_state = self.dncblock(x, state=dnc_state)  # dt: (B,T,D)\n",
    "\n",
    "    # 3) Gate computation (optionally LN before gate)\n",
    "    import torch as _t\n",
    "    z = _t.cat([vt, dt], dim=-1)  # (B,T,2D)\n",
    "    h = self.pre_gate_ln(z) if hasattr(self, \"pre_gate_ln\") and self.pre_gate_ln is not None else z\n",
    "    g_pre = self.gate(h)                 # typically (B,T,D) or (B,T,1)\n",
    "    tau = float(getattr(CFG, 'gate_temp', 1.0))\n",
    "    g = torch.sigmoid(g_pre / max(tau, 1e-6))\n",
    "\n",
    "    # Optional ablation override: force g to 0.0 (vanilla) or 1.0 (DNC)\n",
    "    if gate_override is not None:\n",
    "        g = _t.full_like(g, float(gate_override))\n",
    "\n",
    "    # 4) Blend paths\n",
    "    out = g * dt + (1.0 - g) * vt        # (B,T,D)\n",
    "\n",
    "    # 5) Lightweight metrics (only if requested)\n",
    "    self.last_metrics = None\n",
    "    if getattr(self, \"collect_metrics\", False):\n",
    "        try:\n",
    "            eps = 1e-6\n",
    "            gc = g.clamp(min=eps, max=1.0 - eps)\n",
    "            g_entropy = (-(gc * gc.log() + (1 - gc) * (1 - gc).log())).mean()\n",
    "            vt_norm = vt.norm(dim=-1).mean()\n",
    "            dt_norm = dt.norm(dim=-1).mean()\n",
    "            mstats = {}\n",
    "            # Pull aggregated memory stats if present\n",
    "            if isinstance(dnc_state, dict) and isinstance(dnc_state.get(\"stats\", None), dict):\n",
    "                for k in (\"u_mean\", \"rw_max_mean\", \"ww_max_mean\", \"M_norm_mean\"):\n",
    "                    if k in dnc_state[\"stats\"]:\n",
    "                        mstats[k] = dnc_state[\"stats\"][k]\n",
    "            self.last_metrics = {\n",
    "                \"g_mean\": g.mean().detach(),\n",
    "                \"g_entropy\": g_entropy.detach(),\n",
    "                \"vt_norm\": vt_norm.detach(),\n",
    "                \"dt_norm\": dt_norm.detach(),\n",
    "                **mstats,\n",
    "            }\n",
    "        except Exception:\n",
    "            self.last_metrics = None\n",
    "\n",
    "    return out, dnc_state, g\n",
    "\n",
    "# Apply the patch\n",
    "ParallelEnrichmentBlock.forward = _peb_forward\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:37.008610700Z",
     "start_time": "2025-08-21T18:00:36.983868800Z"
    }
   },
   "id": "3eabc7991c7e743d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.5 Guardrails warnings (for __debug__ mode)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "409782a954e2cb0"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "import warnings, numbers, torch\n",
    "\n",
    "def _normalize_weights(ws):\n",
    "    ws = list(map(float, ws))\n",
    "    s = sum(ws)\n",
    "    if s <= 0:\n",
    "        warnings.warn(\"[guard] mixture_weights sum<=0; normalizing to uniform.\")\n",
    "        n = max(1, len(ws)); return [1.0/n]*n\n",
    "    return [w/(s+1e-8) for w in ws]\n",
    "\n",
    "def _check_schedule(name, sched):\n",
    "    if sched is None: return True\n",
    "    if not isinstance(sched, (list, tuple)):\n",
    "        warnings.warn(f\"[guard] {name} must be list[(until,value)]\"); return False\n",
    "    ok = True\n",
    "    for i, it in enumerate(sched):\n",
    "        if not (isinstance(it, (list, tuple)) and len(it)==2):\n",
    "            warnings.warn(f\"[guard] {name}[{i}] not (until,value)\"); ok=False; continue\n",
    "        u,v = it\n",
    "        if u is not None and not isinstance(u, numbers.Number):\n",
    "            warnings.warn(f\"[guard] {name}[{i}].until not a number\"); ok=False\n",
    "    return ok\n",
    "\n",
    "# Wrap train_experiment\n",
    "try:\n",
    "    if 'train_experiment' in globals() and not getattr(train_experiment, \"__guarded__\", False):\n",
    "        _orig_train_experiment = train_experiment\n",
    "        def train_experiment(*args, **kwargs):\n",
    "            if __debug__:\n",
    "                if \"mixture_weights\" in kwargs:\n",
    "                    kwargs[\"mixture_weights\"] = _normalize_weights(kwargs[\"mixture_weights\"])\n",
    "                for nm in (\"mixture_schedule\",\"gate_temp_schedule\",\"gate_reg_schedule\"):\n",
    "                    _check_schedule(nm, kwargs.get(nm))\n",
    "                # Optional: enforce positive gate_temp if present in CFG\n",
    "                if hasattr(CFG, \"gate_temp\") and CFG.gate_temp <= 0:\n",
    "                    warnings.warn(\"[guard] CFG.gate_temp <= 0; routing may degenerate.\")\n",
    "            return _orig_train_experiment(*args, **kwargs)\n",
    "        train_experiment.__guarded__ = True\n",
    "except Exception as e:\n",
    "    print(\"[guard] train_experiment wrapper skipped:\", e)\n",
    "\n",
    "# Wrap DNCFormerHead.forward\n",
    "try:\n",
    "    if 'DNCFormerHead' in globals() and not getattr(DNCFormerHead, \"__guarded__\", False):\n",
    "        _orig_forward = DNCFormerHead.forward\n",
    "        def forward(self, input_ids: torch.Tensor, attention_mask=None):\n",
    "            if __debug__:\n",
    "                if not isinstance(input_ids, torch.Tensor) or input_ids.dtype != torch.long:\n",
    "                    warnings.warn(f\"[guard] input_ids dtype should be torch.long (got {getattr(input_ids,'dtype',None)})\")\n",
    "            return _orig_forward(self, input_ids, attention_mask=attention_mask)\n",
    "        DNCFormerHead.forward = forward\n",
    "        DNCFormerHead.__guarded__ = True\n",
    "except Exception as e:\n",
    "    print(\"[guard] DNCFormerHead wrapper skipped:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:37.508778100Z",
     "start_time": "2025-08-21T18:00:37.488400100Z"
    }
   },
   "id": "c3e7e5f06b1681b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Eval harnesses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9ee90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.1 copy, reverse, needle-in-haystack"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d73218bd54c4f2bc"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_copy(head, tok, batch=4, T=64, vocab=100):\n",
    "    x = make_copy_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == x[:, 1:]).float().mean().item()\n",
    "    print(\"copy acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_reverse(head, tok, batch=4, T=64, vocab=100):\n",
    "    x, y = make_reverse_task(batch, T, vocab=vocab)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, :-1] == y[:, 1:]).float().mean().item()\n",
    "    print(\"reverse acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad() # WARNING - planned for depreciation\n",
    "def eval_needle(head, tok, batch=4, T=128, vocab=200):\n",
    "    x = make_needle_task(batch, T, vocab=vocab).to(device)\n",
    "    with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "        logits, gates = head(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    acc = (preds[:, -1] == x[:, -1]).float().mean().item()\n",
    "    print(\"needle acc:\", acc, \"gates:\", [g.mean().item() for g in gates])\n",
    "    return {\"acc\": acc, \"gates\": [g.mean().item() for g in gates]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_haystack(head, steps: int = 50, batch: int = 16, T: int = 256, vocab: int = 1024,\n",
    "                      tb_step: int = None, fast: bool = False):\n",
    "    \"\"\"\n",
    "    Long-span retrieval probe: ... K V ... K ? => predict V at '?'.\n",
    "    fast=True uses inference_mode() and smaller defaults to speed up sweeps.\n",
    "    \"\"\"\n",
    "    from torch.amp import autocast\n",
    "    head.eval()\n",
    "\n",
    "    # Fast path defaults (can still be overridden by explicit args)\n",
    "    if fast:\n",
    "        steps = min(steps, 10)\n",
    "        batch = min(batch, 8)\n",
    "        T = min(T, 128)\n",
    "\n",
    "    device_ = next(head.parameters()).device\n",
    "    use_amp = (amp_dtype in (torch.float16, torch.bfloat16)) and torch.cuda.is_available()\n",
    "\n",
    "    accs, losses = [], []\n",
    "    ctx = torch.inference_mode() if fast else torch.no_grad()\n",
    "    with ctx:\n",
    "        for _ in range(steps):\n",
    "            x, V, qpos = make_haystack_batch(batch, T=T, vocab=vocab)\n",
    "            x = x.to(device_); V = V.to(device_); qpos = qpos.to(device_)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                    logits, _g = head(x)\n",
    "            else:\n",
    "                logits, _g = head(x)\n",
    "\n",
    "            idx = torch.arange(x.size(0), device=device_)\n",
    "            logits_q = logits[idx, qpos, :].float()\n",
    "            loss = F.cross_entropy(logits_q, V)\n",
    "            pred = logits_q.argmax(dim=-1)\n",
    "\n",
    "            accs.append((pred == V).float().mean().item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    acc_m = float(np.mean(accs)); loss_m = float(np.mean(losses))\n",
    "\n",
    "    if TB_AVAILABLE and ('tb' in globals()):\n",
    "        tb.writer.add_scalar(\"eval/haystack_acc\", acc_m, tb_step if tb_step is not None else 0)\n",
    "        tb.writer.add_scalar(\"eval/haystack_loss\", loss_m, tb_step if tb_step is not None else 0)\n",
    "        tb.flush()\n",
    "\n",
    "    head.train()\n",
    "    print(f\"[Haystack] acc={acc_m:.3f} | loss={loss_m:.3f} | fast={fast}\")\n",
    "    return acc_m, loss_m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:39.126671500Z",
     "start_time": "2025-08-21T18:00:39.093121500Z"
    }
   },
   "id": "efb2dc08fe1942ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Unit tests, smoke tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3036ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.1 basic unit test, model integrity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba6259d196dd13cc"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def run_basic_unit_tests():\n",
    "    B, T = 2, 4\n",
    "    R, W, N = CFG.dnc_read_heads, CFG.dnc_cell_size, CFG.dnc_nr_cells\n",
    "    d_in = d_model = 128\n",
    "\n",
    "    mem = DNCMemory(N, W, R).to(device)\n",
    "    state = mem.reset(B, device=device)\n",
    "    iface = {\n",
    "        \"k_read\": torch.zeros(B,R,W, device=device),\n",
    "        \"beta_read\": torch.ones(B,R,1, device=device),\n",
    "        \"k_write\": torch.zeros(B,W, device=device),\n",
    "        \"beta_write\": torch.ones(B,1, device=device),\n",
    "        \"erase\": torch.sigmoid(torch.randn(B,W, device=device)),\n",
    "        \"write_vec\": torch.randn(B,W, device=device),\n",
    "        \"free_gates\": torch.sigmoid(torch.randn(B,R,1, device=device)),\n",
    "        \"alloc_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"write_gate\": torch.sigmoid(torch.randn(B,1, device=device)),\n",
    "        \"read_mode\": torch.randn(B,R,3, device=device),\n",
    "    }\n",
    "    r, state2 = mem(iface, state)\n",
    "    assert r.shape == (B,R,W)\n",
    "\n",
    "    ctrl = TransformerController(d_in+R*W, d_model, heads=4).to(device)\n",
    "    x = torch.randn(B,T,d_in, device=device)\n",
    "    reads = state2[\"r\"].reshape(B,R*W).unsqueeze(1).expand(B,T,R*W)\n",
    "    h = ctrl(torch.cat([x, reads], dim=-1))\n",
    "    assert h.shape == (B,T,d_model)\n",
    "\n",
    "    dblk = DNCformerBlock(d_in, d_model, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0).to(device)\n",
    "    y, s = dblk(x)\n",
    "    assert y.shape == (B,T,d_model)\n",
    "\n",
    "    pen = ParallelEnrichmentBlock(d_model, d_in, R, W, N, heads=4, dropout=0.1, ffn_mult=4.0, gate_bias_init=-1.0).to(device)\n",
    "    out, s2, g = pen(torch.randn(B,T,d_model, device=device))\n",
    "    print(f\"out: {out}\\ns2: {s2}\\ng: {g}\")\n",
    "    eps = 1e-6\n",
    "    assert torch.isfinite(g).all()\n",
    "    assert ((g > eps) & (g < 1 - eps)).float().mean().item() > 0.95\n",
    "    assert out.shape == (B,T,d_model)\n",
    "    print(\"All unit-like tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:40.965009Z",
     "start_time": "2025-08-21T18:00:40.931961800Z"
    }
   },
   "id": "2cb70f70"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "#run_basic_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:42.026308600Z",
     "start_time": "2025-08-21T18:00:41.991787500Z"
    }
   },
   "id": "f9f2d4e1ea8a1a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.2 Evaluator unit tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b399df35806fe6f5"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class _DummyHead(torch.nn.Module):\n",
    "    def __init__(self, vocab=100, d_model=64, n_blocks=2):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.n_blocks = n_blocks\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T = input_ids.shape\n",
    "        logits = torch.randn(B, T, self.vocab, device=input_ids.device, dtype=torch.float32)\n",
    "        gates = [torch.sigmoid(torch.randn(B, T, self.d_model, device=input_ids.device)) for _ in range(self.n_blocks)]\n",
    "        return logits, gates\n",
    "\n",
    "def run_eval_unit_tests():\n",
    "    dummy = _DummyHead().to(device).eval()\n",
    "    res_copy = eval_copy(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_rev = eval_reverse(dummy, tok=None, batch=2, T=8, vocab=50)\n",
    "    res_needle = eval_needle(dummy, tok=None, batch=2, T=16, vocab=100)\n",
    "    assert all(k in res_copy for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_rev for k in [\"acc\",\"gates\"])\n",
    "    assert all(k in res_needle for k in [\"acc\",\"gates\"])\n",
    "    print(\"Evaluator unit tests passed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:43.046604800Z",
     "start_time": "2025-08-21T18:00:43.023540200Z"
    }
   },
   "id": "db24ba31"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "#run_eval_unit_tests()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:44.476792600Z",
     "start_time": "2025-08-21T18:00:44.439717500Z"
    }
   },
   "id": "4c45b9e3f62413ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.3 GPU VRAM diagnostics + cuda allocation smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683887a5f3501496"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre base-only] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64182b67a713471680556a47ee7706dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after base-only] alloc=27.21 GB | reserved=27.24 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, contextlib\n",
    "\n",
    "def cuda_report(tag=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"[cuda_report] CUDA not available\"); return\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    alloc = torch.cuda.memory_allocated()\n",
    "    reserv = torch.cuda.memory_reserved()\n",
    "    print(f\"[{tag}] alloc={alloc/1e9:.2f} GB | reserved={reserv/1e9:.2f} GB | free={free/1e9:.2f} GB | total={total/1e9:.2f} GB\")\n",
    "\n",
    "def list_head_refs():\n",
    "    # looks for globals named 'head' and count of DNCFormerHead instances\n",
    "    import gc, inspect, sys\n",
    "    heads = [o for o in gc.get_objects() if o.__class__.__name__ == \"DNCFormerHead\"]\n",
    "    print(f\"[liveness] DNCFormerHead instances alive: {len(heads)}\")\n",
    "    if 'head' in globals():\n",
    "        h = globals()['head']\n",
    "        try:\n",
    "            devs = sorted({p.device.type for p in h.parameters()})\n",
    "        except Exception:\n",
    "            devs = [\"<unknown>\"]\n",
    "        print(f\"[liveness] global 'head' present; param devices: {devs}\")\n",
    "    else:\n",
    "        print(\"[liveness] no global 'head'\")\n",
    "        \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gc.collect(); torch.cuda.empty_cache(); cuda_report(\"pre base-only\")\n",
    "tok = AutoTokenizer.from_pretrained(CFG.base_model_id, trust_remote_code=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.base_model_id,\n",
    "    torch_dtype=(torch.bfloat16 if (amp_dtype!=torch.float32 and torch.cuda.is_bf16_supported()) else (torch.float16 if amp_dtype!=torch.float32 else None)),\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "cuda_report(\"after base-only\")\n",
    "del base, tok; gc.collect(); torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:55.100615300Z",
     "start_time": "2025-08-21T18:00:45.263863Z"
    }
   },
   "id": "2af2ed601f9c7606"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.4 TB logger test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d48becf8a7f50a"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB active: True | logdir: ./runs\\dncformer-20250821-110055\n"
     ]
    }
   ],
   "source": [
    "tb = TBLogger(logdir=\"./runs\")\n",
    "try:\n",
    "    tb\n",
    "except NameError:\n",
    "    tb = TBLogger(logdir=\"./runs\")\n",
    "print(\"TB active:\", TB_AVAILABLE, \"| logdir:\", getattr(tb, \"path\", None))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:55.113615500Z",
     "start_time": "2025-08-21T18:00:55.086614300Z"
    }
   },
   "id": "bf494415ac5e8547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.5 Memory tracer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1d5de08007861f"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def run_memory_tracer_smoke(head, tok):\n",
    "    if not TB_AVAILABLE:\n",
    "        print(\"TB not available; skipping image smoke.\")\n",
    "        return\n",
    "    from torch.amp import autocast\n",
    "    with trace_memory(head) as tracer:\n",
    "        x = torch.randint(5, (1, 16), device=device)\n",
    "        with autocast('cuda', dtype=amp_dtype, enabled=(amp_dtype!=torch.float32)):\n",
    "            _ = head(x)\n",
    "    assert len(tracer.frames) > 0, \"No memory frames captured\"\n",
    "    print(\"Tracer captured\", len(tracer.frames), \"steps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:55.806130400Z",
     "start_time": "2025-08-21T18:00:55.774129200Z"
    }
   },
   "id": "d842ad22"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "#run_memory_tracer_smoke()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:58.374990800Z",
     "start_time": "2025-08-21T18:00:58.349977900Z"
    }
   },
   "id": "86a52d1c90f05c57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.6 Data generator smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6097ee7f06d9e87c"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_copy batch shape: torch.Size([3, 128]) | dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mx = int(getattr(CFG, \"max_seq_len\", 256))\n",
    "tok, _base = tok if 'tok' in globals() else (None, None)\n",
    "_pad = getattr(tok, \"pad_token_id\", 0) if tok is not None else 0\n",
    "\n",
    "x = make_repeat_copy(batch=3, T=min(mx, 128), vocab=50, pad_id=_pad)\n",
    "print(\"repeat_copy batch shape:\", x.shape, \"| dtype:\", x.dtype)  # expect (3, <=128), long\n",
    "assert x.dtype == torch.long\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:00:59.247368100Z",
     "start_time": "2025-08-21T18:00:59.213833900Z"
    }
   },
   "id": "be0b78c71554906c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.7 Mixture sampler smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92b9b09dbe07c2b"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "p1: [0.30000001192092896, 0.30000001192092896, 0.25, 0.15000000596046448]\n"
     ]
    }
   ],
   "source": [
    "ms = MixtureSampler(gens=[lambda b: None, lambda b: None, lambda b: None, lambda b: None],\n",
    "                    weights=[0.4,0.2,0.2,0.2],\n",
    "                    names=[\"hf\",\"copy\",\"repeat\",\"nback\"])\n",
    "print(\"p0:\", ms.p.tolist())    # ~[0.4,0.2,0.2,0.2]\n",
    "ms.set_weights([0.3,0.3,0.25,0.15])\n",
    "print(\"p1:\", ms.p.tolist())    # ~[0.3,0.3,0.25,0.15]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:01:00.123281100Z",
     "start_time": "2025-08-21T18:01:00.093663900Z"
    }
   },
   "id": "635b3129c4abe2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.8 Build mixer smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc74e56a0b98ce26"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac46f8093d9b4797b46ab8a7fcb6cf80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n",
      "names: ['hf', 'copy', 'repeat', 'nback'] | p: [0.4000000059604645, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224]\n"
     ]
    }
   ],
   "source": [
    "# 1) With Alpaca (instruction/output)\n",
    "tok, _ = build_model_and_tokenizer()\n",
    "m1 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"tatsu-lab/alpaca\", hf_max_items=500)\n",
    "print(\"names:\", m1.names, \"| p:\", m1.p.tolist())  # expect 'hf' present\n",
    "\n",
    "# 2) With TinyStories (text)\n",
    "m2 = _build_mixer(tok, (0.4,0.2,0.2,0.2), hf_dataset=\"roneneldan/TinyStories\", hf_max_items=500)\n",
    "print(\"names:\", m2.names, \"| p:\", m2.p.tolist())  # expect 'hf' present (text path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:01:26.699419Z",
     "start_time": "2025-08-21T18:01:03.037311100Z"
    }
   },
   "id": "92366ea6aeb2c846"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.9 haystack smoke test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63121cfeae026f7c"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b81fd48b1c4c477ca8282c2142f93ef4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[103], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m tok, head \u001B[38;5;241m=\u001B[39m build_model_and_tokenizer()\n\u001B[1;32m----> 2\u001B[0m acc, loss \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_haystack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHaystack smoke:\u001B[39m\u001B[38;5;124m\"\u001B[39m, acc, loss)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[91], line 62\u001B[0m, in \u001B[0;36mevaluate_haystack\u001B[1;34m(head, steps, batch, T, vocab, tb_step, fast)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_amp:\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast(device_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mamp_dtype):\n\u001B[1;32m---> 62\u001B[0m         logits, _g \u001B[38;5;241m=\u001B[39m \u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m     logits, _g \u001B[38;5;241m=\u001B[39m head(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[90], line 50\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(input_ids, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;129;01mor\u001B[39;00m input_ids\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong:\n\u001B[0;32m     49\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[guard] input_ids dtype should be torch.long (got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mgetattr\u001B[39m(input_ids,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;28;01mNone\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 50\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_orig_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[78], line 41\u001B[0m, in \u001B[0;36mDNCFormerHead.forward\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m     39\u001B[0m gates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks):\n\u001B[1;32m---> 41\u001B[0m     h, dnc_states[i], g \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdnc_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdnc_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     gates\u001B[38;5;241m.\u001B[39mappend(g\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[0;32m     43\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase\u001B[38;5;241m.\u001B[39mlm_head(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_out(h)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase\u001B[38;5;241m.\u001B[39mlm_head\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[89], line 13\u001B[0m, in \u001B[0;36m_peb_forward\u001B[1;34m(self, x, dnc_state, gate_override)\u001B[0m\n\u001B[0;32m     11\u001B[0m mask \u001B[38;5;241m=\u001B[39m causal_mask(T, device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# (T,T) causal attn mask\u001B[39;00m\n\u001B[0;32m     12\u001B[0m x_cast \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvanilla\u001B[38;5;241m.\u001B[39mln1\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m---> 13\u001B[0m vt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvanilla\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_cast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m    \u001B[38;5;66;03m# (B,T,D)\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 2) DNC path (controller+memory)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m dt, dnc_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdncblock(x, state\u001B[38;5;241m=\u001B[39mdnc_state)  \u001B[38;5;66;03m# dt: (B,T,D)\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[77], line 18\u001B[0m, in \u001B[0;36mVanillaTransformerBlock.forward\u001B[1;34m(self, x, attn_mask, gate_override)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor, attn_mask: Optional[torch\u001B[38;5;241m.\u001B[39mTensor]\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, gate_override: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     17\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln1(x)\n\u001B[1;32m---> 18\u001B[0m     a, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     h \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(a)\n\u001B[0;32m     20\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln2(h)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1342\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1343\u001B[0m         query,\n\u001B[0;32m   1344\u001B[0m         key,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1365\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[0;32m   1366\u001B[0m     )\n\u001B[0;32m   1367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1368\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1369\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1370\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1371\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1374\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1375\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1376\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1378\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1380\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1381\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1386\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1388\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\functional.py:6097\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   6093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_separate_proj_weight:\n\u001B[0;32m   6094\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m   6095\u001B[0m         in_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   6096\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 6097\u001B[0m     q, k, v \u001B[38;5;241m=\u001B[39m \u001B[43m_in_projection_packed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6098\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6099\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m   6100\u001B[0m         q_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   6101\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dncformer\\lib\\site-packages\\torch\\nn\\functional.py:5501\u001B[0m, in \u001B[0;36m_in_projection_packed\u001B[1;34m(q, k, v, w, b)\u001B[0m\n\u001B[0;32m   5498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mis\u001B[39;00m v:\n\u001B[0;32m   5499\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m q \u001B[38;5;129;01mis\u001B[39;00m k:\n\u001B[0;32m   5500\u001B[0m         \u001B[38;5;66;03m# self-attention\u001B[39;00m\n\u001B[1;32m-> 5501\u001B[0m         proj \u001B[38;5;241m=\u001B[39m \u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5502\u001B[0m         \u001B[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001B[39;00m\n\u001B[0;32m   5503\u001B[0m         proj \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   5504\u001B[0m             proj\u001B[38;5;241m.\u001B[39munflatten(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m3\u001B[39m, E))\n\u001B[0;32m   5505\u001B[0m             \u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5508\u001B[0m             \u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m   5509\u001B[0m         )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# tok, head = build_model_and_tokenizer()\n",
    "# acc, loss = evaluate_haystack(head, steps=2, batch=4, T=64, vocab=512, tb_step=0)\n",
    "# print(\"Haystack smoke:\", acc, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:06:03.206690500Z",
     "start_time": "2025-08-21T18:01:26.699419Z"
    }
   },
   "id": "dca7ccde4a7a19d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.10 Training run Sanity test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867e3f96209f238d"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a9cc2ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:13:45.796229800Z",
     "start_time": "2025-08-21T18:06:09.623483600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=39.21 GB | reserved=39.22 GB | free=0.00 GB | total=25.77 GB\n",
      "[snapshot: before train_experiment] alloc=39.21 GB | reserved=39.22 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07dc4450f2a7484ebb5b32b51c8befaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\miniconda3\\envs\\dncformer\\lib\\contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.1902 | lr 2.00e-05 | gates=[0.287109375, 0.287109375] | mix=repeat\n",
      "[snapshot: after train_experiment] alloc=48.98 GB | reserved=62.73 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()\n",
    "cuda_report(\"snapshot: before train_experiment\")\n",
    "head, tok = train_experiment(steps=10, warmup_steps=10, mixture_weights=(0.4,0.2,0.2,0.2))\n",
    "cuda_report(\"snapshot: after train_experiment\")\n",
    "#Launch TensorBoard in a terminal: tensorboard --logdir ./runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.11 Integrity panel\n",
    "- shapes check\n",
    "- dtyptes check\n",
    "- forward check"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c77708bd8690c4"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "import torch, contextlib, json\n",
    "\n",
    "def count_params(m: torch.nn.Module, trainable_only=True):\n",
    "    return sum(p.numel() for p in m.parameters() if (p.requires_grad or not trainable_only))\n",
    "\n",
    "def integrity_panel(tok=None, head=None, T: int = 8):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[integrity] device={dev} | amp_dtype={globals().get('amp_dtype', torch.float32)}\")\n",
    "    print(f\"[integrity] base_model_id={getattr(CFG, 'base_model_id', '?')}\")\n",
    "    for k in (\"d_model\", \"N\", \"W\", \"R\", \"num_blocks\"):\n",
    "        if hasattr(CFG, k):\n",
    "            print(f\"[integrity] CFG.{k}={getattr(CFG,k)}\")\n",
    "\n",
    "    if head is not None:\n",
    "        print(f\"[integrity] head params (trainable): {count_params(head):,}\")\n",
    "\n",
    "    # SDPA context visibility\n",
    "    with contextlib.suppress(Exception):\n",
    "        print(f\"[integrity] SDPA_CTX={type(globals().get('SDPA_CTX', None)).__name__}\")\n",
    "\n",
    "    # Tiny forward\n",
    "    try:\n",
    "        if tok is None or head is None:\n",
    "            print(\"[integrity] (skip tiny forward; missing tok/head)\")\n",
    "            return\n",
    "        tok.pad_token = tok.pad_token or tok.eos_token\n",
    "        dummy = tok(\"hello world\", return_tensors=\"pt\")\n",
    "        x = dummy.input_ids.to(dev)\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=globals().get(\"amp_dtype\", torch.float32),\n",
    "                                enabled=(globals().get(\"amp_dtype\", torch.float32) != torch.float32)):\n",
    "                logits, gates = head(x)\n",
    "        print(f\"[integrity] forward ok | logits={tuple(logits.shape)} | gates={[tuple(g.shape) for g in gates]}\")\n",
    "        # Basic gate sanity\n",
    "        for i, g in enumerate(gates):\n",
    "            g_mean = float(torch.sigmoid(g).mean().item()) if g.dtype.is_floating_point else float(g.float().mean().item())\n",
    "            print(f\"[integrity] gate[{i}] mean≈{g_mean:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(\"[integrity] forward failed:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:13:45.837229100Z",
     "start_time": "2025-08-21T18:13:45.794237700Z"
    }
   },
   "id": "f2459475bc37800a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.12 Seed, build integrity, and save/load smoke tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764d7cad6218a8cc"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6e69d3be8b646ef810eab3287638c63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[integrity] device=cuda | amp_dtype=torch.bfloat16\n",
      "[integrity] base_model_id=microsoft/Phi-3-mini-4k-instruct\n",
      "[integrity] CFG.d_model=3072\n",
      "[integrity] head params (trainable): 532,304,538\n",
      "[integrity] SDPA_CTX=NoneType\n",
      "[integrity] forward ok | logits=(1, 2, 32064) | gates=[(1, 2, 3072), (1, 2, 3072)]\n",
      "[integrity] gate[0] mean≈0.570\n",
      "[integrity] gate[1] mean≈0.570\n",
      "[save_head] wrote checkpoints\\smoke.pt and checkpoints\\smoke.meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_203344\\2377763709.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(in_path, map_location=map_location or \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_head] restored successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1) Seed determinism invoked\n",
    "set_seed(getattr(CFG, \"seed\", 42))\n",
    "\n",
    "# 2) Build & integrity\n",
    "tok, head = build_model_and_tokenizer()\n",
    "integrity_panel(tok, head, T=8)\n",
    "\n",
    "# 3) Save/load head (no-op training resume test)\n",
    "save_head(head, \"./checkpoints\", cfg=CFG, run_label=\"smoke\")\n",
    "_ = load_head(head, \"./checkpoints/smoke.pt\", strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:14:11.698063200Z",
     "start_time": "2025-08-21T18:13:45.807230700Z"
    }
   },
   "id": "bc04e917bfc4dc9a"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef00b3f4b2a94b7eb7df5ce8f0795a13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[integrity] device=cuda | amp_dtype=torch.bfloat16\n",
      "[integrity] base_model_id=microsoft/Phi-3-mini-4k-instruct\n",
      "[integrity] CFG.d_model=3072\n",
      "[integrity] head params (trainable): 532,304,538\n",
      "[integrity] SDPA_CTX=NoneType\n",
      "[integrity] forward ok | logits=(1, 2, 32064) | gates=[(1, 2, 3072), (1, 2, 3072)]\n",
      "[integrity] gate[0] mean≈0.570\n",
      "[integrity] gate[1] mean≈0.570\n"
     ]
    }
   ],
   "source": [
    "tok, head = build_model_and_tokenizer()\n",
    "integrity_panel(tok, head, T=8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T18:14:18.519637400Z",
     "start_time": "2025-08-21T18:14:11.698063200Z"
    }
   },
   "id": "a389fb774f0d31e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Experiments - Stage 1 - basic architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5c8e4aa3034c4a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.1 Experiment Set 1 - Medium training sweep, testing parameter / dataset variants\n",
    " - E0: Baseline\n",
    " - E1: memory/algorithmic slanted data mix\n",
    " - E2: gate regularizer (low)\n",
    " - E3: gate regularizer (high)\n",
    " - E4: sharper routing via lower gate temperature\n",
    " - E5: ablations: disable/force DNC path\n",
    "    - 5a: disable DNC path (DNC path disabled)\n",
    "    - 5b: force DNC path (transformer path disabled) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81cbb7a830e97c75"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E0_baseline ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E0_baseline] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "978f5412cfc4409d8fe23562b6b085d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 8.7638 | lr 2.00e-04 | gates=[0.283203125, 0.28125] | mix=repeat\n",
      "step 20 | loss 8.1121 | lr 2.00e-04 | gates=[0.283203125, 0.28515625] | mix=nback\n",
      "step 30 | loss 4.2501 | lr 1.99e-04 | gates=[0.26171875, 0.26953125] | mix=hf\n",
      "step 40 | loss 4.6715 | lr 1.98e-04 | gates=[0.259765625, 0.26953125] | mix=hf\n",
      "step 50 | loss 4.4469 | lr 1.97e-04 | gates=[0.25390625, 0.263671875] | mix=hf\n",
      "step 60 | loss 7.0320 | lr 1.95e-04 | gates=[0.287109375, 0.310546875] | mix=nback\n",
      "step 70 | loss 5.7877 | lr 1.92e-04 | gates=[0.28515625, 0.314453125] | mix=nback\n",
      "step 80 | loss 1.9082 | lr 1.90e-04 | gates=[0.2353515625, 0.248046875] | mix=hf\n",
      "step 90 | loss 1.8775 | lr 1.87e-04 | gates=[0.2294921875, 0.2412109375] | mix=hf\n",
      "step 100 | loss 6.4503 | lr 1.83e-04 | gates=[0.291015625, 0.3203125] | mix=repeat\n",
      "step 110 | loss 5.8166 | lr 1.80e-04 | gates=[0.291015625, 0.326171875] | mix=copy\n",
      "step 120 | loss 1.3207 | lr 1.76e-04 | gates=[0.2197265625, 0.2412109375] | mix=hf\n",
      "step 130 | loss 5.6160 | lr 1.71e-04 | gates=[0.296875, 0.333984375] | mix=copy\n",
      "step 140 | loss 1.8407 | lr 1.67e-04 | gates=[0.201171875, 0.220703125] | mix=hf\n",
      "step 150 | loss 5.2244 | lr 1.62e-04 | gates=[0.28515625, 0.33203125] | mix=nback\n",
      "step 160 | loss 5.0245 | lr 1.57e-04 | gates=[0.283203125, 0.333984375] | mix=nback\n",
      "step 170 | loss 1.6287 | lr 1.51e-04 | gates=[0.1875, 0.2080078125] | mix=hf\n",
      "step 180 | loss 5.7207 | lr 1.46e-04 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 190 | loss 1.8050 | lr 1.40e-04 | gates=[0.177734375, 0.1982421875] | mix=hf\n",
      "step 200 | loss 1.4193 | lr 1.34e-04 | gates=[0.177734375, 0.2001953125] | mix=hf\n",
      "step 210 | loss 5.5786 | lr 1.28e-04 | gates=[0.294921875, 0.345703125] | mix=copy\n",
      "step 220 | loss 1.3877 | lr 1.22e-04 | gates=[0.1728515625, 0.1962890625] | mix=hf\n",
      "step 230 | loss 1.4895 | lr 1.15e-04 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 240 | loss 1.5816 | lr 1.09e-04 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 250 | loss 5.0480 | lr 1.03e-04 | gates=[0.28515625, 0.34375] | mix=nback\n",
      "step 260 | loss 4.8854 | lr 9.62e-05 | gates=[0.28515625, 0.345703125] | mix=nback\n",
      "step 270 | loss 5.4913 | lr 8.98e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 280 | loss 4.7768 | lr 8.34e-05 | gates=[0.3046875, 0.353515625] | mix=repeat\n",
      "step 290 | loss 5.4612 | lr 7.71e-05 | gates=[0.294921875, 0.3515625] | mix=copy\n",
      "step 300 | loss 2.4750 | lr 7.09e-05 | gates=[0.1708984375, 0.203125] | mix=hf\n",
      "step 310 | loss 5.4431 | lr 6.49e-05 | gates=[0.302734375, 0.353515625] | mix=copy\n",
      "step 320 | loss 1.3132 | lr 5.89e-05 | gates=[0.15234375, 0.1767578125] | mix=hf\n",
      "step 330 | loss 1.4544 | lr 5.32e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 340 | loss 5.4393 | lr 4.76e-05 | gates=[0.3046875, 0.35546875] | mix=copy\n",
      "step 350 | loss 5.4470 | lr 4.23e-05 | gates=[0.30078125, 0.353515625] | mix=copy\n",
      "step 360 | loss 1.3796 | lr 3.72e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 370 | loss 4.8311 | lr 3.23e-05 | gates=[0.306640625, 0.357421875] | mix=repeat\n",
      "step 380 | loss 1.3429 | lr 2.77e-05 | gates=[0.1591796875, 0.1845703125] | mix=hf\n",
      "step 390 | loss 5.4305 | lr 2.34e-05 | gates=[0.306640625, 0.359375] | mix=copy\n",
      "step 400 | loss 4.9858 | lr 2.00e-05 | gates=[0.28515625, 0.349609375] | mix=nback\n",
      "step 410 | loss 4.8875 | lr 2.00e-05 | gates=[0.30078125, 0.3515625] | mix=repeat\n",
      "step 420 | loss 1.2746 | lr 2.00e-05 | gates=[0.154296875, 0.1796875] | mix=hf\n",
      "step 430 | loss 1.5069 | lr 2.00e-05 | gates=[0.1591796875, 0.1884765625] | mix=hf\n",
      "step 440 | loss 4.8447 | lr 2.00e-05 | gates=[0.2890625, 0.353515625] | mix=nback\n",
      "step 450 | loss 1.7236 | lr 2.00e-05 | gates=[0.1552734375, 0.1826171875] | mix=hf\n",
      "step 460 | loss 1.2047 | lr 2.00e-05 | gates=[0.15234375, 0.1748046875] | mix=hf\n",
      "step 470 | loss 1.3864 | lr 2.00e-05 | gates=[0.1533203125, 0.177734375] | mix=hf\n",
      "step 480 | loss 4.8177 | lr 2.00e-05 | gates=[0.2890625, 0.353515625] | mix=nback\n",
      "step 490 | loss 1.3794 | lr 2.00e-05 | gates=[0.1484375, 0.1708984375] | mix=hf\n",
      "step 500 | loss 4.9159 | lr 2.00e-05 | gates=[0.291015625, 0.35546875] | mix=nback\n",
      "[after  E0_baseline] alloc=19.56 GB | reserved=39.42 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E1_memory_lean ===\n",
      "mixture=(0.4, 0.3, 0.2, 0.1), gate_reg_lambda=0.0, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E1_memory_lean] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f12008c68214fbdafca7ff355e3e93d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.6762 | lr 2.00e-04 | gates=[0.27734375, 0.275390625] | mix=hf\n",
      "step 20 | loss 7.2456 | lr 2.00e-04 | gates=[0.265625, 0.267578125] | mix=hf\n",
      "step 30 | loss 2.8066 | lr 1.99e-04 | gates=[0.259765625, 0.26171875] | mix=hf\n",
      "step 40 | loss 5.9084 | lr 1.98e-04 | gates=[0.28515625, 0.294921875] | mix=copy\n",
      "step 50 | loss 6.2994 | lr 1.97e-04 | gates=[0.283203125, 0.296875] | mix=nback\n",
      "step 60 | loss 2.0780 | lr 1.95e-04 | gates=[0.240234375, 0.251953125] | mix=hf\n",
      "step 70 | loss 5.9646 | lr 1.92e-04 | gates=[0.2890625, 0.302734375] | mix=repeat\n",
      "step 80 | loss 6.1615 | lr 1.90e-04 | gates=[0.279296875, 0.30078125] | mix=nback\n",
      "step 90 | loss 1.6371 | lr 1.87e-04 | gates=[0.2255859375, 0.2412109375] | mix=hf\n",
      "step 100 | loss 5.8504 | lr 1.83e-04 | gates=[0.287109375, 0.30859375] | mix=copy\n",
      "step 110 | loss 6.1592 | lr 1.80e-04 | gates=[0.287109375, 0.310546875] | mix=copy\n",
      "step 120 | loss 1.6537 | lr 1.76e-04 | gates=[0.2109375, 0.228515625] | mix=hf\n",
      "step 130 | loss 1.3438 | lr 1.71e-04 | gates=[0.2060546875, 0.2255859375] | mix=hf\n",
      "step 140 | loss 1.6747 | lr 1.67e-04 | gates=[0.197265625, 0.216796875] | mix=hf\n",
      "step 150 | loss 7.8639 | lr 1.62e-04 | gates=[0.2890625, 0.3125] | mix=repeat\n",
      "step 160 | loss 7.0060 | lr 1.57e-04 | gates=[0.2890625, 0.318359375] | mix=copy\n",
      "step 170 | loss 1.2355 | lr 1.51e-04 | gates=[0.1904296875, 0.212890625] | mix=hf\n",
      "step 180 | loss 1.5434 | lr 1.46e-04 | gates=[0.181640625, 0.2041015625] | mix=hf\n",
      "step 190 | loss 1.3073 | lr 1.40e-04 | gates=[0.17578125, 0.19921875] | mix=hf\n",
      "step 200 | loss 1.5339 | lr 1.34e-04 | gates=[0.1748046875, 0.19921875] | mix=hf\n",
      "step 210 | loss 1.3171 | lr 1.28e-04 | gates=[0.169921875, 0.193359375] | mix=hf\n",
      "step 220 | loss 5.4045 | lr 1.22e-04 | gates=[0.271484375, 0.318359375] | mix=nback\n",
      "step 230 | loss 1.7062 | lr 1.15e-04 | gates=[0.162109375, 0.1875] | mix=hf\n",
      "step 240 | loss 4.8178 | lr 1.09e-04 | gates=[0.29296875, 0.328125] | mix=repeat\n",
      "step 250 | loss 5.0184 | lr 1.03e-04 | gates=[0.306640625, 0.337890625] | mix=repeat\n",
      "step 260 | loss 5.1333 | lr 9.62e-05 | gates=[0.30078125, 0.333984375] | mix=repeat\n",
      "step 270 | loss 1.5841 | lr 8.98e-05 | gates=[0.16015625, 0.1865234375] | mix=hf\n",
      "step 280 | loss 5.4775 | lr 8.34e-05 | gates=[0.29296875, 0.333984375] | mix=copy\n",
      "step 290 | loss 4.8038 | lr 7.71e-05 | gates=[0.29296875, 0.3359375] | mix=repeat\n",
      "step 300 | loss 5.5080 | lr 7.09e-05 | gates=[0.296875, 0.337890625] | mix=copy\n",
      "step 310 | loss 5.8610 | lr 6.49e-05 | gates=[0.2890625, 0.3359375] | mix=copy\n",
      "step 320 | loss 5.3585 | lr 5.89e-05 | gates=[0.294921875, 0.3359375] | mix=repeat\n",
      "step 330 | loss 0.9718 | lr 5.32e-05 | gates=[0.1591796875, 0.1865234375] | mix=hf\n",
      "step 340 | loss 5.4318 | lr 4.76e-05 | gates=[0.298828125, 0.33984375] | mix=copy\n",
      "step 350 | loss 4.9012 | lr 4.23e-05 | gates=[0.275390625, 0.328125] | mix=nback\n",
      "step 360 | loss 1.0814 | lr 3.72e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 370 | loss 5.0328 | lr 3.23e-05 | gates=[0.28125, 0.33203125] | mix=nback\n",
      "step 380 | loss 4.9236 | lr 2.77e-05 | gates=[0.3046875, 0.341796875] | mix=repeat\n",
      "step 390 | loss 4.7060 | lr 2.34e-05 | gates=[0.302734375, 0.33984375] | mix=repeat\n",
      "step 400 | loss 5.4102 | lr 2.00e-05 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 410 | loss 0.8208 | lr 2.00e-05 | gates=[0.1630859375, 0.1943359375] | mix=hf\n",
      "step 420 | loss 1.2078 | lr 2.00e-05 | gates=[0.1640625, 0.193359375] | mix=hf\n",
      "step 430 | loss 5.4661 | lr 2.00e-05 | gates=[0.29296875, 0.337890625] | mix=copy\n",
      "step 440 | loss 5.0707 | lr 2.00e-05 | gates=[0.306640625, 0.341796875] | mix=repeat\n",
      "step 450 | loss 4.6631 | lr 2.00e-05 | gates=[0.296875, 0.3359375] | mix=repeat\n",
      "step 460 | loss 1.4681 | lr 2.00e-05 | gates=[0.150390625, 0.1748046875] | mix=hf\n",
      "step 470 | loss 1.2772 | lr 2.00e-05 | gates=[0.150390625, 0.173828125] | mix=hf\n",
      "step 480 | loss 1.6525 | lr 2.00e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 490 | loss 5.3703 | lr 2.00e-05 | gates=[0.302734375, 0.34375] | mix=copy\n",
      "step 500 | loss 4.6788 | lr 2.00e-05 | gates=[0.310546875, 0.345703125] | mix=repeat\n",
      "[after  E1_memory_lean] alloc=19.56 GB | reserved=39.73 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E2_gate_reg_low ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0002, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E2_gate_reg_low] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4a8fb8f5e0d481b8372171b7a548ef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.5047 | lr 2.00e-04 | gates=[0.275390625, 0.279296875] | mix=hf\n",
      "step 20 | loss 2.1480 | lr 2.00e-04 | gates=[0.263671875, 0.26953125] | mix=hf\n",
      "step 30 | loss 7.5692 | lr 1.99e-04 | gates=[0.279296875, 0.287109375] | mix=nback\n",
      "step 40 | loss 1.5048 | lr 1.98e-04 | gates=[0.244140625, 0.25390625] | mix=hf\n",
      "step 50 | loss 6.6892 | lr 1.97e-04 | gates=[0.279296875, 0.296875] | mix=copy\n",
      "step 60 | loss 5.5064 | lr 1.95e-04 | gates=[0.287109375, 0.3046875] | mix=repeat\n",
      "step 70 | loss 10.6165 | lr 1.92e-04 | gates=[0.28515625, 0.302734375] | mix=repeat\n",
      "step 80 | loss 6.2255 | lr 1.90e-04 | gates=[0.279296875, 0.302734375] | mix=repeat\n",
      "step 90 | loss 1.4385 | lr 1.87e-04 | gates=[0.2080078125, 0.2255859375] | mix=hf\n",
      "step 100 | loss 1.6988 | lr 1.83e-04 | gates=[0.20703125, 0.2265625] | mix=hf\n",
      "step 110 | loss 1.4504 | lr 1.80e-04 | gates=[0.2021484375, 0.2216796875] | mix=hf\n",
      "step 120 | loss 6.7830 | lr 1.76e-04 | gates=[0.271484375, 0.314453125] | mix=nback\n",
      "step 130 | loss 1.6999 | lr 1.71e-04 | gates=[0.1962890625, 0.21875] | mix=hf\n",
      "step 140 | loss 5.7271 | lr 1.67e-04 | gates=[0.271484375, 0.318359375] | mix=nback\n",
      "step 150 | loss 5.8592 | lr 1.62e-04 | gates=[0.28515625, 0.32421875] | mix=repeat\n",
      "step 160 | loss 1.7528 | lr 1.57e-04 | gates=[0.1845703125, 0.208984375] | mix=hf\n",
      "step 170 | loss 5.2232 | lr 1.51e-04 | gates=[0.28515625, 0.328125] | mix=repeat\n",
      "step 180 | loss 1.5497 | lr 1.46e-04 | gates=[0.18359375, 0.2109375] | mix=hf\n",
      "step 190 | loss 5.1735 | lr 1.40e-04 | gates=[0.2890625, 0.33203125] | mix=repeat\n",
      "step 200 | loss 1.2247 | lr 1.34e-04 | gates=[0.181640625, 0.208984375] | mix=hf\n",
      "step 210 | loss 5.1795 | lr 1.28e-04 | gates=[0.279296875, 0.3359375] | mix=nback\n",
      "step 220 | loss 1.7806 | lr 1.22e-04 | gates=[0.177734375, 0.20703125] | mix=hf\n",
      "step 230 | loss 1.4641 | lr 1.15e-04 | gates=[0.1728515625, 0.2001953125] | mix=hf\n",
      "step 240 | loss 5.2401 | lr 1.09e-04 | gates=[0.271484375, 0.333984375] | mix=nback\n",
      "step 250 | loss 4.8972 | lr 1.03e-04 | gates=[0.287109375, 0.33984375] | mix=repeat\n",
      "step 260 | loss 1.4546 | lr 9.62e-05 | gates=[0.171875, 0.19921875] | mix=hf\n",
      "step 270 | loss 5.1950 | lr 8.98e-05 | gates=[0.271484375, 0.3359375] | mix=nback\n",
      "step 280 | loss 1.6311 | lr 8.34e-05 | gates=[0.1650390625, 0.1904296875] | mix=hf\n",
      "step 290 | loss 5.2266 | lr 7.71e-05 | gates=[0.271484375, 0.3359375] | mix=nback\n",
      "step 300 | loss 1.4167 | lr 7.09e-05 | gates=[0.1748046875, 0.2021484375] | mix=hf\n",
      "step 310 | loss 1.2757 | lr 6.49e-05 | gates=[0.1640625, 0.1884765625] | mix=hf\n",
      "step 320 | loss 1.4606 | lr 5.89e-05 | gates=[0.16015625, 0.18359375] | mix=hf\n",
      "step 330 | loss 5.0209 | lr 5.32e-05 | gates=[0.298828125, 0.349609375] | mix=repeat\n",
      "step 340 | loss 1.3760 | lr 4.76e-05 | gates=[0.162109375, 0.1865234375] | mix=hf\n",
      "step 350 | loss 4.9610 | lr 4.23e-05 | gates=[0.2890625, 0.345703125] | mix=repeat\n",
      "step 360 | loss 4.8731 | lr 3.72e-05 | gates=[0.29296875, 0.34765625] | mix=repeat\n",
      "step 370 | loss 4.8434 | lr 3.23e-05 | gates=[0.29296875, 0.34765625] | mix=repeat\n",
      "step 380 | loss 4.8694 | lr 2.77e-05 | gates=[0.271484375, 0.33984375] | mix=nback\n",
      "step 390 | loss 5.4633 | lr 2.34e-05 | gates=[0.2890625, 0.34765625] | mix=copy\n",
      "step 400 | loss 5.4245 | lr 2.00e-05 | gates=[0.294921875, 0.3515625] | mix=copy\n",
      "step 410 | loss 4.8709 | lr 2.00e-05 | gates=[0.27734375, 0.34375] | mix=nback\n",
      "step 420 | loss 2.0610 | lr 2.00e-05 | gates=[0.15625, 0.18359375] | mix=hf\n",
      "step 430 | loss 1.3637 | lr 2.00e-05 | gates=[0.1611328125, 0.185546875] | mix=hf\n",
      "step 440 | loss 5.3997 | lr 2.00e-05 | gates=[0.29296875, 0.3515625] | mix=copy\n",
      "step 450 | loss 5.4484 | lr 2.00e-05 | gates=[0.291015625, 0.349609375] | mix=copy\n",
      "step 460 | loss 1.6328 | lr 2.00e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 470 | loss 4.9579 | lr 2.00e-05 | gates=[0.28125, 0.34765625] | mix=nback\n",
      "step 480 | loss 1.3012 | lr 2.00e-05 | gates=[0.1640625, 0.193359375] | mix=hf\n",
      "step 490 | loss 1.5084 | lr 2.00e-05 | gates=[0.1552734375, 0.181640625] | mix=hf\n",
      "step 500 | loss 5.4518 | lr 2.00e-05 | gates=[0.28515625, 0.34765625] | mix=copy\n",
      "[after  E2_gate_reg_low] alloc=19.56 GB | reserved=42.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E3_gate_reg_high ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0005, gate_temp=1.0, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E3_gate_reg_high] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf9fa93631f4eb2ba11472793c1e7a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 8.4827 | lr 2.00e-04 | gates=[0.28515625, 0.283203125] | mix=repeat\n",
      "step 20 | loss 7.1305 | lr 2.00e-04 | gates=[0.287109375, 0.28515625] | mix=repeat\n",
      "step 30 | loss 2.0686 | lr 1.99e-04 | gates=[0.26171875, 0.267578125] | mix=hf\n",
      "step 40 | loss 3.8306 | lr 1.98e-04 | gates=[0.248046875, 0.255859375] | mix=hf\n",
      "step 50 | loss 7.1654 | lr 1.97e-04 | gates=[0.28515625, 0.294921875] | mix=copy\n",
      "step 60 | loss 7.3936 | lr 1.95e-04 | gates=[0.28125, 0.296875] | mix=nback\n",
      "step 70 | loss 6.3211 | lr 1.92e-04 | gates=[0.283203125, 0.302734375] | mix=nback\n",
      "step 80 | loss 5.5104 | lr 1.90e-04 | gates=[0.29296875, 0.310546875] | mix=repeat\n",
      "step 90 | loss 5.6180 | lr 1.87e-04 | gates=[0.28515625, 0.3125] | mix=copy\n",
      "step 100 | loss 5.1885 | lr 1.83e-04 | gates=[0.29296875, 0.31640625] | mix=repeat\n",
      "step 110 | loss 5.5819 | lr 1.80e-04 | gates=[0.283203125, 0.314453125] | mix=nback\n",
      "step 120 | loss 1.4206 | lr 1.76e-04 | gates=[0.216796875, 0.2353515625] | mix=hf\n",
      "step 130 | loss 6.0211 | lr 1.71e-04 | gates=[0.283203125, 0.31640625] | mix=nback\n",
      "step 140 | loss 5.0925 | lr 1.67e-04 | gates=[0.294921875, 0.326171875] | mix=repeat\n",
      "step 150 | loss 5.0160 | lr 1.62e-04 | gates=[0.296875, 0.328125] | mix=repeat\n",
      "step 160 | loss 2.2180 | lr 1.57e-04 | gates=[0.203125, 0.2255859375] | mix=hf\n",
      "step 170 | loss 1.7853 | lr 1.51e-04 | gates=[0.2021484375, 0.2265625] | mix=hf\n",
      "step 180 | loss 1.7626 | lr 1.46e-04 | gates=[0.19921875, 0.22265625] | mix=hf\n",
      "step 190 | loss 5.0773 | lr 1.40e-04 | gates=[0.28515625, 0.326171875] | mix=nback\n",
      "step 200 | loss 5.1497 | lr 1.34e-04 | gates=[0.296875, 0.33203125] | mix=repeat\n",
      "step 210 | loss 5.9953 | lr 1.28e-04 | gates=[0.283203125, 0.326171875] | mix=nback\n",
      "step 220 | loss 1.6030 | lr 1.22e-04 | gates=[0.1884765625, 0.2109375] | mix=hf\n",
      "step 230 | loss 5.5216 | lr 1.15e-04 | gates=[0.2890625, 0.33203125] | mix=copy\n",
      "step 240 | loss 4.9902 | lr 1.09e-04 | gates=[0.30078125, 0.337890625] | mix=repeat\n",
      "step 250 | loss 5.0173 | lr 1.03e-04 | gates=[0.298828125, 0.3359375] | mix=repeat\n",
      "step 260 | loss 5.5651 | lr 9.62e-05 | gates=[0.28125, 0.328125] | mix=nback\n",
      "step 270 | loss 1.4995 | lr 8.98e-05 | gates=[0.1748046875, 0.197265625] | mix=hf\n",
      "step 280 | loss 4.9527 | lr 8.34e-05 | gates=[0.3046875, 0.337890625] | mix=repeat\n",
      "step 290 | loss 5.5485 | lr 7.71e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 300 | loss 5.0064 | lr 7.09e-05 | gates=[0.30859375, 0.34375] | mix=repeat\n",
      "step 310 | loss 5.5153 | lr 6.49e-05 | gates=[0.28125, 0.33203125] | mix=nback\n",
      "step 320 | loss 6.0751 | lr 5.89e-05 | gates=[0.296875, 0.337890625] | mix=copy\n",
      "step 330 | loss 1.1022 | lr 5.32e-05 | gates=[0.169921875, 0.1953125] | mix=hf\n",
      "step 340 | loss 5.5156 | lr 4.76e-05 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 350 | loss 4.7582 | lr 4.23e-05 | gates=[0.294921875, 0.3359375] | mix=repeat\n",
      "step 360 | loss 1.3964 | lr 3.72e-05 | gates=[0.1728515625, 0.1982421875] | mix=hf\n",
      "step 370 | loss 1.3397 | lr 3.23e-05 | gates=[0.169921875, 0.1962890625] | mix=hf\n",
      "step 380 | loss 0.8502 | lr 2.77e-05 | gates=[0.1767578125, 0.20703125] | mix=hf\n",
      "step 390 | loss 4.9189 | lr 2.34e-05 | gates=[0.28515625, 0.3359375] | mix=nback\n",
      "step 400 | loss 1.3257 | lr 2.00e-05 | gates=[0.173828125, 0.2001953125] | mix=hf\n",
      "step 410 | loss 1.2752 | lr 2.00e-05 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 420 | loss 4.6597 | lr 2.00e-05 | gates=[0.30078125, 0.341796875] | mix=repeat\n",
      "step 430 | loss 4.8766 | lr 2.00e-05 | gates=[0.30078125, 0.341796875] | mix=repeat\n",
      "step 440 | loss 4.8770 | lr 2.00e-05 | gates=[0.283203125, 0.3359375] | mix=nback\n",
      "step 450 | loss 5.4217 | lr 2.00e-05 | gates=[0.2890625, 0.337890625] | mix=copy\n",
      "step 460 | loss 5.4015 | lr 2.00e-05 | gates=[0.298828125, 0.34375] | mix=copy\n",
      "step 470 | loss 1.5290 | lr 2.00e-05 | gates=[0.166015625, 0.19140625] | mix=hf\n",
      "step 480 | loss 4.8769 | lr 2.00e-05 | gates=[0.283203125, 0.3359375] | mix=nback\n",
      "step 490 | loss 5.3786 | lr 2.00e-05 | gates=[0.298828125, 0.345703125] | mix=copy\n",
      "step 500 | loss 5.4438 | lr 2.00e-05 | gates=[0.298828125, 0.34375] | mix=copy\n",
      "[after  E3_gate_reg_high] alloc=19.56 GB | reserved=42.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E4_gate_temp_0p7 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=0.7, force_g=None\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E4_gate_temp_0p7] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68333fd92e164c9198ccb40f42d54687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.7747 | lr 2.00e-04 | gates=[0.224609375, 0.224609375] | mix=repeat\n",
      "step 20 | loss 10.0938 | lr 2.00e-04 | gates=[0.2294921875, 0.232421875] | mix=copy\n",
      "step 30 | loss 9.4271 | lr 1.99e-04 | gates=[0.23046875, 0.23828125] | mix=nback\n",
      "step 40 | loss 7.5605 | lr 1.98e-04 | gates=[0.2294921875, 0.2412109375] | mix=nback\n",
      "step 50 | loss 7.0946 | lr 1.97e-04 | gates=[0.23046875, 0.2451171875] | mix=nback\n",
      "step 60 | loss 5.3907 | lr 1.95e-04 | gates=[0.236328125, 0.25390625] | mix=repeat\n",
      "step 70 | loss 8.6001 | lr 1.92e-04 | gates=[0.232421875, 0.255859375] | mix=copy\n",
      "step 80 | loss 6.2352 | lr 1.90e-04 | gates=[0.2275390625, 0.259765625] | mix=nback\n",
      "step 90 | loss 2.6570 | lr 1.87e-04 | gates=[0.166015625, 0.19140625] | mix=hf\n",
      "step 100 | loss 2.9589 | lr 1.83e-04 | gates=[0.1591796875, 0.185546875] | mix=hf\n",
      "step 110 | loss 5.4593 | lr 1.80e-04 | gates=[0.2353515625, 0.2734375] | mix=repeat\n",
      "step 120 | loss 5.5796 | lr 1.76e-04 | gates=[0.2275390625, 0.2734375] | mix=nback\n",
      "step 130 | loss 6.1719 | lr 1.71e-04 | gates=[0.236328125, 0.279296875] | mix=copy\n",
      "step 140 | loss 9.6652 | lr 1.67e-04 | gates=[0.2314453125, 0.27734375] | mix=copy\n",
      "step 150 | loss 5.2064 | lr 1.62e-04 | gates=[0.236328125, 0.27734375] | mix=repeat\n",
      "step 160 | loss 5.9392 | lr 1.57e-04 | gates=[0.232421875, 0.27734375] | mix=repeat\n",
      "step 170 | loss 5.7170 | lr 1.51e-04 | gates=[0.236328125, 0.28515625] | mix=copy\n",
      "step 180 | loss 1.5507 | lr 1.46e-04 | gates=[0.1337890625, 0.162109375] | mix=hf\n",
      "step 190 | loss 5.3480 | lr 1.40e-04 | gates=[0.2255859375, 0.28515625] | mix=nback\n",
      "step 200 | loss 5.6519 | lr 1.34e-04 | gates=[0.2333984375, 0.291015625] | mix=copy\n",
      "step 210 | loss 5.5102 | lr 1.28e-04 | gates=[0.234375, 0.291015625] | mix=copy\n",
      "step 220 | loss 5.6864 | lr 1.22e-04 | gates=[0.2451171875, 0.30078125] | mix=copy\n",
      "step 230 | loss 5.9367 | lr 1.15e-04 | gates=[0.244140625, 0.30078125] | mix=repeat\n",
      "step 240 | loss 5.1008 | lr 1.09e-04 | gates=[0.240234375, 0.298828125] | mix=repeat\n",
      "step 250 | loss 5.5583 | lr 1.03e-04 | gates=[0.23828125, 0.302734375] | mix=copy\n",
      "step 260 | loss 1.7395 | lr 9.62e-05 | gates=[0.12060546875, 0.15234375] | mix=hf\n",
      "step 270 | loss 1.3323 | lr 8.98e-05 | gates=[0.12109375, 0.1513671875] | mix=hf\n",
      "step 280 | loss 5.6282 | lr 8.34e-05 | gates=[0.23046875, 0.302734375] | mix=nback\n",
      "step 290 | loss 1.5458 | lr 7.71e-05 | gates=[0.12353515625, 0.1572265625] | mix=hf\n",
      "step 300 | loss 5.5118 | lr 7.09e-05 | gates=[0.2421875, 0.30859375] | mix=copy\n",
      "step 310 | loss 5.4353 | lr 6.49e-05 | gates=[0.2451171875, 0.310546875] | mix=copy\n",
      "step 320 | loss 1.3244 | lr 5.89e-05 | gates=[0.119140625, 0.1513671875] | mix=hf\n",
      "step 330 | loss 5.5029 | lr 5.32e-05 | gates=[0.251953125, 0.31640625] | mix=copy\n",
      "step 340 | loss 4.8675 | lr 4.76e-05 | gates=[0.251953125, 0.314453125] | mix=repeat\n",
      "step 350 | loss 5.4279 | lr 4.23e-05 | gates=[0.2392578125, 0.3125] | mix=copy\n",
      "step 360 | loss 1.3162 | lr 3.72e-05 | gates=[0.11328125, 0.14453125] | mix=hf\n",
      "step 370 | loss 4.7927 | lr 3.23e-05 | gates=[0.259765625, 0.3203125] | mix=repeat\n",
      "step 380 | loss 1.2108 | lr 2.77e-05 | gates=[0.11865234375, 0.1494140625] | mix=hf\n",
      "step 390 | loss 1.2477 | lr 2.34e-05 | gates=[0.1142578125, 0.1455078125] | mix=hf\n",
      "step 400 | loss 1.3056 | lr 2.00e-05 | gates=[0.1162109375, 0.1474609375] | mix=hf\n",
      "step 410 | loss 1.4811 | lr 2.00e-05 | gates=[0.12353515625, 0.158203125] | mix=hf\n",
      "step 420 | loss 5.3813 | lr 2.00e-05 | gates=[0.255859375, 0.3203125] | mix=copy\n",
      "step 430 | loss 5.4253 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=copy\n",
      "step 440 | loss 4.8336 | lr 2.00e-05 | gates=[0.234375, 0.3125] | mix=nback\n",
      "step 450 | loss 1.3101 | lr 2.00e-05 | gates=[0.11181640625, 0.1416015625] | mix=hf\n",
      "step 460 | loss 1.1295 | lr 2.00e-05 | gates=[0.1201171875, 0.1533203125] | mix=hf\n",
      "step 470 | loss 1.1692 | lr 2.00e-05 | gates=[0.1171875, 0.1474609375] | mix=hf\n",
      "step 480 | loss 1.3702 | lr 2.00e-05 | gates=[0.1083984375, 0.1376953125] | mix=hf\n",
      "step 490 | loss 4.6268 | lr 2.00e-05 | gates=[0.255859375, 0.3203125] | mix=repeat\n",
      "step 500 | loss 4.7667 | lr 2.00e-05 | gates=[0.2578125, 0.3203125] | mix=repeat\n",
      "[after  E4_gate_temp_0p7] alloc=19.56 GB | reserved=37.59 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "\n",
      "=== E5a_force_g_0 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=0.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E5a_force_g_0] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "345a21a3d79c486e9a79cffc1ce3664b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.7708 | lr 2.00e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 20 | loss 8.1129 | lr 2.00e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 30 | loss 9.4176 | lr 1.99e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 40 | loss 2.0564 | lr 1.98e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 50 | loss 1.4602 | lr 1.97e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 60 | loss 6.0042 | lr 1.95e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 70 | loss 6.5107 | lr 1.92e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 80 | loss 1.6592 | lr 1.90e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 90 | loss 6.2884 | lr 1.87e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 100 | loss 6.1378 | lr 1.83e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 110 | loss 1.2196 | lr 1.80e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 120 | loss 5.9388 | lr 1.76e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 130 | loss 6.2737 | lr 1.71e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 140 | loss 1.4693 | lr 1.67e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 150 | loss 1.4359 | lr 1.62e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 160 | loss 1.6469 | lr 1.57e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 170 | loss 5.7269 | lr 1.51e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 180 | loss 1.5394 | lr 1.46e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 190 | loss 5.8034 | lr 1.40e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 200 | loss 6.5025 | lr 1.34e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 210 | loss 5.5158 | lr 1.28e-04 | gates=[0.0, 0.0] | mix=nback\n",
      "step 220 | loss 1.1690 | lr 1.22e-04 | gates=[0.0, 0.0] | mix=hf\n",
      "step 230 | loss 4.9982 | lr 1.15e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 240 | loss 5.2309 | lr 1.09e-04 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 250 | loss 5.7406 | lr 1.03e-04 | gates=[0.0, 0.0] | mix=copy\n",
      "step 260 | loss 1.3949 | lr 9.62e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 270 | loss 5.7502 | lr 8.98e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 280 | loss 5.8369 | lr 8.34e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 290 | loss 1.4449 | lr 7.71e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 300 | loss 1.4356 | lr 7.09e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 310 | loss 1.5654 | lr 6.49e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 320 | loss 5.0112 | lr 5.89e-05 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 330 | loss 1.4810 | lr 5.32e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 340 | loss 5.0976 | lr 4.76e-05 | gates=[0.0, 0.0] | mix=repeat\n",
      "step 350 | loss 5.2713 | lr 4.23e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 360 | loss 1.2658 | lr 3.72e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 370 | loss 5.7405 | lr 3.23e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 380 | loss 5.0271 | lr 2.77e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 390 | loss 5.5519 | lr 2.34e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 400 | loss 1.0491 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 410 | loss 1.1208 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 420 | loss 5.5193 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=copy\n",
      "step 430 | loss 1.0712 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 440 | loss 5.1604 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 450 | loss 1.4760 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 460 | loss 1.4285 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 470 | loss 0.9634 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 480 | loss 1.4824 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "step 490 | loss 4.9023 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=nback\n",
      "step 500 | loss 1.1232 | lr 2.00e-05 | gates=[0.0, 0.0] | mix=hf\n",
      "[after  E5a_force_g_0] alloc=19.56 GB | reserved=39.52 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "\n",
      "=== E5b_force_g_1 ===\n",
      "mixture=(0.4, 0.2, 0.2, 0.2), gate_reg_lambda=0.0, gate_temp=1.0, force_g=1.0\n",
      "[after free] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n",
      "[before E5b_force_g_1] alloc=9.79 GB | reserved=9.82 GB | free=14.54 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d16d204e0dd5456ba7cc6bebe60afcd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.4470 | lr 2.00e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 20 | loss 8.4882 | lr 2.00e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 30 | loss 6.6057 | lr 1.99e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 40 | loss 6.8276 | lr 1.98e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 50 | loss 6.6664 | lr 1.97e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 60 | loss 6.3100 | lr 1.95e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 70 | loss 6.7212 | lr 1.92e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 80 | loss 9.0949 | lr 1.90e-04 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 90 | loss 5.5494 | lr 1.87e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 100 | loss 5.9994 | lr 1.83e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 110 | loss 5.6693 | lr 1.80e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 120 | loss 5.1639 | lr 1.76e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 130 | loss 5.7746 | lr 1.71e-04 | gates=[1.0, 1.0] | mix=copy\n",
      "step 140 | loss 4.8143 | lr 1.67e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 150 | loss 5.4356 | lr 1.62e-04 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 160 | loss 4.6245 | lr 1.57e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 170 | loss 4.8690 | lr 1.51e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 180 | loss 5.4418 | lr 1.46e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 190 | loss 4.0093 | lr 1.40e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 200 | loss 7.7539 | lr 1.34e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 210 | loss 4.1556 | lr 1.28e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 220 | loss 4.5632 | lr 1.22e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 230 | loss 4.9013 | lr 1.15e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 240 | loss 4.9961 | lr 1.09e-04 | gates=[1.0, 1.0] | mix=nback\n",
      "step 250 | loss 3.8238 | lr 1.03e-04 | gates=[1.0, 1.0] | mix=hf\n",
      "step 260 | loss 5.6437 | lr 9.62e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 270 | loss 3.8135 | lr 8.98e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 280 | loss 4.0806 | lr 8.34e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 290 | loss 5.2600 | lr 7.71e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 300 | loss 3.6085 | lr 7.09e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 310 | loss 3.5248 | lr 6.49e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 320 | loss 5.3987 | lr 5.89e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 330 | loss 3.3324 | lr 5.32e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 340 | loss 3.0953 | lr 4.76e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 350 | loss 3.5784 | lr 4.23e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 360 | loss 4.8400 | lr 3.72e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 370 | loss 3.5400 | lr 3.23e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 380 | loss 5.4159 | lr 2.77e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 390 | loss 5.1525 | lr 2.34e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 400 | loss 5.1219 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 410 | loss 4.8144 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 420 | loss 2.9476 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 430 | loss 3.6580 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 440 | loss 5.3956 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=copy\n",
      "step 450 | loss 5.1512 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 460 | loss 4.7878 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 470 | loss 5.0965 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=repeat\n",
      "step 480 | loss 4.8268 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "step 490 | loss 3.2123 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=hf\n",
      "step 500 | loss 4.7564 | lr 2.00e-05 | gates=[1.0, 1.0] | mix=nback\n",
      "[after  E5b_force_g_1] alloc=19.56 GB | reserved=38.97 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, torch, gc\n",
    "\n",
    "# run set parameters\n",
    "EXP_STEPS   = 500       # try 1000–5000 for longer curves\n",
    "EXP_WARMUP  = 10       # keep ~steps/20\n",
    "BASE_MIX    = (0.4, 0.2, 0.2, 0.2)  # (hf, copy, repeat, nback)\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "def run_isolate_test(label,\n",
    "            mixture_weights=BASE_MIX,\n",
    "            gate_reg_lambda=None,\n",
    "            gate_temp=None,\n",
    "            force_g=None,\n",
    "            steps=EXP_STEPS,\n",
    "            warmup=EXP_WARMUP):\n",
    "    \"\"\"Run a single train_experiment experiment with explicit knobs.\"\"\"\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    # set experiment-specific knobs (others come from CFG defaults)\n",
    "    if gate_reg_lambda is not None: set_cfg(gate_reg_lambda=float(gate_reg_lambda))\n",
    "    if gate_temp is not None:       set_cfg(gate_temp=float(gate_temp))\n",
    "    set_cfg(force_g=force_g)  # may be None, 0.0, or 1.0\n",
    "\n",
    "    print(f\"mixture={mixture_weights}, gate_reg_lambda={getattr(CFG,'gate_reg_lambda', 0.0)}, \"\n",
    "          f\"gate_temp={getattr(CFG,'gate_temp', 1.0)}, force_g={getattr(CFG,'force_g', None)}\")\n",
    "\n",
    "    # clean slate for VRAM and allocator fragmentation between runs\n",
    "    free_head_and_cache()\n",
    "    cuda_report(f\"before {label}\")\n",
    "    time.sleep(1.2)  # ensure distinct TB run dirs (timestamp granularity)\n",
    "\n",
    "    # run\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=warmup,\n",
    "        mixture_weights=mixture_weights,\n",
    "        viz_memory_after=False,   # keep quick; use visualize_memory_tb() ad-hoc\n",
    "    )\n",
    "\n",
    "    # post-run snapshot + cleanup\n",
    "    cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# === E0 baseline: identical to your sanity run ===\n",
    "run_isolate_test(\"E0_baseline\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E1 memory-leaning mix: more algorithmic exposure ===\n",
    "run_isolate_test(\"E1_memory_lean\",\n",
    "        mixture_weights=(0.4, 0.3, 0.2, 0.1),  # HF, copy, repeat, nback\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E2 gate regularizer (low) ===\n",
    "run_isolate_test(\"E2_gate_reg_low\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=2e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E3 gate regularizer (high) ===\n",
    "run_isolate_test(\"E3_gate_reg_high\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=5e-4,\n",
    "        gate_temp=1.0,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E4 sharper routing via lower gate temperature ===\n",
    "run_isolate_test(\"E4_gate_temp_0p7\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=0.7,\n",
    "        force_g=None,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "# === E5 ablations: disable/force DNC path ===\n",
    "run_isolate_test(\"E5a_force_g_0\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=0.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n",
    "\n",
    "run_isolate_test(\"E5b_force_g_1\",\n",
    "        mixture_weights=(0.4, 0.2, 0.2, 0.2),\n",
    "        gate_reg_lambda=0.0,\n",
    "        gate_temp=1.0,\n",
    "        force_g=1.0,\n",
    "        steps=EXP_STEPS, warmup=EXP_WARMUP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T21:32:12.458599800Z",
     "start_time": "2025-08-20T16:33:53.323296700Z"
    }
   },
   "id": "771c919035840bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-20T21:32:12.613598600Z",
     "start_time": "2025-08-20T21:32:12.457599100Z"
    }
   },
   "id": "f07fc074b58caaa0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.2 Experiment Set 2 - parameter sweeps, based on E2 params from set 1 above\n",
    " - E6: higher gate temp\n",
    " - E7: memory-leaning warm-start\n",
    " - E8: capacity sweep\n",
    " - E9: baseline training, haystack eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a269fb24f3b9075"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E6_temp0p8_seed1337 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250820-143212-E6_temp0p8_seed1337\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed1337] alloc=19.56 GB | reserved=19.60 GB | free=3.89 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ef4156bede44a0485abeae825940206"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 1.8139 | lr 8.80e-05 | gates=[0.2392578125, 0.2431640625] | mix=hf\n",
      "step 20 | loss 5.6580 | lr 1.68e-04 | gates=[0.2451171875, 0.25390625] | mix=nback\n",
      "step 30 | loss 6.5675 | lr 2.00e-04 | gates=[0.25, 0.2578125] | mix=copy\n",
      "step 40 | loss 3.2836 | lr 1.99e-04 | gates=[0.2158203125, 0.224609375] | mix=hf\n",
      "step 50 | loss 1.9842 | lr 1.99e-04 | gates=[0.2080078125, 0.21875] | mix=hf\n",
      "step 60 | loss 2.1618 | lr 1.97e-04 | gates=[0.201171875, 0.21484375] | mix=hf\n",
      "step 70 | loss 2.7611 | lr 1.95e-04 | gates=[0.1923828125, 0.2080078125] | mix=hf\n",
      "step 80 | loss 6.0227 | lr 1.93e-04 | gates=[0.25390625, 0.275390625] | mix=copy\n",
      "step 90 | loss 1.4240 | lr 1.91e-04 | gates=[0.1884765625, 0.2060546875] | mix=hf\n",
      "step 100 | loss 1.8305 | lr 1.88e-04 | gates=[0.177734375, 0.197265625] | mix=hf\n",
      "step 110 | loss 5.7472 | lr 1.84e-04 | gates=[0.2578125, 0.283203125] | mix=copy\n",
      "step 120 | loss 1.5439 | lr 1.81e-04 | gates=[0.1669921875, 0.1904296875] | mix=hf\n",
      "step 130 | loss 5.7418 | lr 1.76e-04 | gates=[0.2470703125, 0.283203125] | mix=nback\n",
      "step 140 | loss 5.3791 | lr 1.72e-04 | gates=[0.267578125, 0.294921875] | mix=repeat\n",
      "step 150 | loss 5.8621 | lr 1.67e-04 | gates=[0.25390625, 0.291015625] | mix=copy\n",
      "step 160 | loss 1.5878 | lr 1.62e-04 | gates=[0.1533203125, 0.1796875] | mix=hf\n",
      "step 170 | loss 5.5724 | lr 1.57e-04 | gates=[0.263671875, 0.294921875] | mix=repeat\n",
      "step 180 | loss 5.0408 | lr 1.51e-04 | gates=[0.26171875, 0.296875] | mix=repeat\n",
      "step 190 | loss 5.2424 | lr 1.46e-04 | gates=[0.24609375, 0.294921875] | mix=nback\n",
      "step 200 | loss 5.5308 | lr 1.40e-04 | gates=[0.2490234375, 0.296875] | mix=nback\n",
      "step 210 | loss 5.4577 | lr 1.33e-04 | gates=[0.26171875, 0.302734375] | mix=repeat\n",
      "step 220 | loss 5.9007 | lr 1.27e-04 | gates=[0.251953125, 0.3046875] | mix=nback\n",
      "step 230 | loss 5.6615 | lr 1.21e-04 | gates=[0.255859375, 0.306640625] | mix=copy\n",
      "step 240 | loss 1.4753 | lr 1.14e-04 | gates=[0.1396484375, 0.169921875] | mix=hf\n",
      "step 250 | loss 5.1542 | lr 1.08e-04 | gates=[0.2470703125, 0.3046875] | mix=nback\n",
      "step 260 | loss 5.3103 | lr 1.01e-04 | gates=[0.255859375, 0.310546875] | mix=nback\n",
      "step 270 | loss 5.5097 | lr 9.44e-05 | gates=[0.26953125, 0.31640625] | mix=copy\n",
      "step 280 | loss 5.7380 | lr 8.78e-05 | gates=[0.25, 0.310546875] | mix=nback\n",
      "step 290 | loss 5.4838 | lr 8.13e-05 | gates=[0.263671875, 0.3125] | mix=repeat\n",
      "step 300 | loss 7.8310 | lr 7.48e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 310 | loss 5.2254 | lr 6.85e-05 | gates=[0.263671875, 0.310546875] | mix=repeat\n",
      "step 320 | loss 5.4695 | lr 6.23e-05 | gates=[0.26953125, 0.318359375] | mix=copy\n",
      "step 330 | loss 5.5201 | lr 5.62e-05 | gates=[0.2734375, 0.3203125] | mix=copy\n",
      "step 340 | loss 5.3968 | lr 5.04e-05 | gates=[0.26171875, 0.318359375] | mix=copy\n",
      "step 350 | loss 5.1314 | lr 4.48e-05 | gates=[0.279296875, 0.322265625] | mix=repeat\n",
      "step 360 | loss 1.7178 | lr 3.94e-05 | gates=[0.12451171875, 0.1533203125] | mix=hf\n",
      "step 370 | loss 5.4563 | lr 3.42e-05 | gates=[0.263671875, 0.3203125] | mix=copy\n",
      "step 380 | loss 1.0962 | lr 2.94e-05 | gates=[0.12890625, 0.158203125] | mix=hf\n",
      "step 390 | loss 5.4683 | lr 2.49e-05 | gates=[0.27734375, 0.326171875] | mix=copy\n",
      "step 400 | loss 1.3760 | lr 2.07e-05 | gates=[0.1259765625, 0.1552734375] | mix=hf\n",
      "step 410 | loss 1.2889 | lr 2.00e-05 | gates=[0.125, 0.15234375] | mix=hf\n",
      "step 420 | loss 4.6192 | lr 2.00e-05 | gates=[0.2734375, 0.32421875] | mix=repeat\n",
      "step 430 | loss 1.4892 | lr 2.00e-05 | gates=[0.12451171875, 0.154296875] | mix=hf\n",
      "step 440 | loss 1.3414 | lr 2.00e-05 | gates=[0.12353515625, 0.1513671875] | mix=hf\n",
      "step 450 | loss 1.2564 | lr 2.00e-05 | gates=[0.126953125, 0.1552734375] | mix=hf\n",
      "step 460 | loss 1.5645 | lr 2.00e-05 | gates=[0.12353515625, 0.15234375] | mix=hf\n",
      "step 470 | loss 5.4328 | lr 2.00e-05 | gates=[0.275390625, 0.326171875] | mix=copy\n",
      "step 480 | loss 1.1220 | lr 2.00e-05 | gates=[0.1259765625, 0.1552734375] | mix=hf\n",
      "step 490 | loss 4.7706 | lr 2.00e-05 | gates=[0.2734375, 0.32421875] | mix=repeat\n",
      "step 500 | loss 5.4319 | lr 2.00e-05 | gates=[0.265625, 0.322265625] | mix=copy\n",
      "[after  E6_temp0p8_seed1337] alloc=29.34 GB | reserved=50.91 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed2027 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250820-151404-E6_temp0p8_seed2027\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed2027] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2104813c18934d6084925ca25a90acfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.7216 | lr 8.80e-05 | gates=[0.240234375, 0.2451171875] | mix=nback\n",
      "step 20 | loss 1.7473 | lr 1.68e-04 | gates=[0.2373046875, 0.2392578125] | mix=hf\n",
      "step 30 | loss 7.0766 | lr 2.00e-04 | gates=[0.2470703125, 0.255859375] | mix=repeat\n",
      "step 40 | loss 7.0362 | lr 1.99e-04 | gates=[0.25, 0.26171875] | mix=repeat\n",
      "step 50 | loss 1.9754 | lr 1.99e-04 | gates=[0.2119140625, 0.21875] | mix=hf\n",
      "step 60 | loss 6.4103 | lr 1.97e-04 | gates=[0.2470703125, 0.263671875] | mix=copy\n",
      "step 70 | loss 6.1247 | lr 1.95e-04 | gates=[0.25, 0.26953125] | mix=copy\n",
      "step 80 | loss 1.6452 | lr 1.93e-04 | gates=[0.1875, 0.2021484375] | mix=hf\n",
      "step 90 | loss 5.8153 | lr 1.91e-04 | gates=[0.2451171875, 0.26953125] | mix=repeat\n",
      "step 100 | loss 1.5125 | lr 1.88e-04 | gates=[0.1708984375, 0.189453125] | mix=hf\n",
      "step 110 | loss 5.8301 | lr 1.84e-04 | gates=[0.25390625, 0.279296875] | mix=repeat\n",
      "step 120 | loss 6.0745 | lr 1.81e-04 | gates=[0.2490234375, 0.279296875] | mix=repeat\n",
      "step 130 | loss 6.5550 | lr 1.76e-04 | gates=[0.2392578125, 0.283203125] | mix=nback\n",
      "step 140 | loss 6.5625 | lr 1.72e-04 | gates=[0.2490234375, 0.291015625] | mix=copy\n",
      "step 150 | loss 5.1010 | lr 1.67e-04 | gates=[0.25390625, 0.29296875] | mix=repeat\n",
      "step 160 | loss 6.3703 | lr 1.62e-04 | gates=[0.2470703125, 0.294921875] | mix=copy\n",
      "step 170 | loss 5.6672 | lr 1.57e-04 | gates=[0.2490234375, 0.294921875] | mix=copy\n",
      "step 180 | loss 6.0536 | lr 1.51e-04 | gates=[0.2578125, 0.3046875] | mix=copy\n",
      "step 190 | loss 6.1479 | lr 1.46e-04 | gates=[0.2578125, 0.3046875] | mix=repeat\n",
      "step 200 | loss 1.3962 | lr 1.40e-04 | gates=[0.1494140625, 0.1787109375] | mix=hf\n",
      "step 210 | loss 5.6449 | lr 1.33e-04 | gates=[0.259765625, 0.306640625] | mix=copy\n",
      "step 220 | loss 5.3002 | lr 1.27e-04 | gates=[0.2431640625, 0.30078125] | mix=nback\n",
      "step 230 | loss 1.3832 | lr 1.21e-04 | gates=[0.138671875, 0.166015625] | mix=hf\n",
      "step 240 | loss 5.1007 | lr 1.14e-04 | gates=[0.267578125, 0.3125] | mix=repeat\n",
      "step 250 | loss 1.7392 | lr 1.08e-04 | gates=[0.1337890625, 0.162109375] | mix=hf\n",
      "step 260 | loss 1.9869 | lr 1.01e-04 | gates=[0.1328125, 0.162109375] | mix=hf\n",
      "step 270 | loss 5.5443 | lr 9.44e-05 | gates=[0.267578125, 0.318359375] | mix=copy\n",
      "step 280 | loss 1.2283 | lr 8.78e-05 | gates=[0.13671875, 0.1650390625] | mix=hf\n",
      "step 290 | loss 1.4291 | lr 8.13e-05 | gates=[0.1328125, 0.1630859375] | mix=hf\n",
      "step 300 | loss 5.5580 | lr 7.48e-05 | gates=[0.2470703125, 0.3125] | mix=nback\n",
      "step 310 | loss 1.5050 | lr 6.85e-05 | gates=[0.130859375, 0.158203125] | mix=hf\n",
      "step 320 | loss 5.1237 | lr 6.23e-05 | gates=[0.2431640625, 0.310546875] | mix=nback\n",
      "step 330 | loss 5.1306 | lr 5.62e-05 | gates=[0.2470703125, 0.3125] | mix=nback\n",
      "step 340 | loss 4.8957 | lr 5.04e-05 | gates=[0.2578125, 0.3125] | mix=repeat\n",
      "step 350 | loss 1.4053 | lr 4.48e-05 | gates=[0.1259765625, 0.15234375] | mix=hf\n",
      "step 360 | loss 5.0109 | lr 3.94e-05 | gates=[0.25, 0.314453125] | mix=nback\n",
      "step 370 | loss 1.1974 | lr 3.42e-05 | gates=[0.1337890625, 0.16015625] | mix=hf\n",
      "step 380 | loss 1.4149 | lr 2.94e-05 | gates=[0.130859375, 0.1611328125] | mix=hf\n",
      "step 390 | loss 4.8075 | lr 2.49e-05 | gates=[0.25, 0.31640625] | mix=nback\n",
      "step 400 | loss 5.4349 | lr 2.07e-05 | gates=[0.25390625, 0.318359375] | mix=copy\n",
      "step 410 | loss 4.7952 | lr 2.00e-05 | gates=[0.2734375, 0.326171875] | mix=repeat\n",
      "step 420 | loss 1.1273 | lr 2.00e-05 | gates=[0.134765625, 0.162109375] | mix=hf\n",
      "step 430 | loss 4.9395 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=nback\n",
      "step 440 | loss 1.2522 | lr 2.00e-05 | gates=[0.126953125, 0.1552734375] | mix=hf\n",
      "step 450 | loss 5.4420 | lr 2.00e-05 | gates=[0.263671875, 0.322265625] | mix=copy\n",
      "step 460 | loss 5.4315 | lr 2.00e-05 | gates=[0.265625, 0.32421875] | mix=copy\n",
      "step 470 | loss 1.1288 | lr 2.00e-05 | gates=[0.1298828125, 0.1572265625] | mix=hf\n",
      "step 480 | loss 4.8761 | lr 2.00e-05 | gates=[0.25, 0.318359375] | mix=nback\n",
      "step 490 | loss 5.4577 | lr 2.00e-05 | gates=[0.265625, 0.32421875] | mix=copy\n",
      "step 500 | loss 0.9094 | lr 2.00e-05 | gates=[0.13671875, 0.1689453125] | mix=hf\n",
      "[after  E6_temp0p8_seed2027] alloc=29.34 GB | reserved=49.33 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E6_temp0p8_seed4242 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250820-160508-E6_temp0p8_seed4242\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E6_temp0p8_seed4242] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f2117d39ac142059d34b3c28a1b7296"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.5768 | lr 8.80e-05 | gates=[0.248046875, 0.2470703125] | mix=repeat\n",
      "step 20 | loss 5.6260 | lr 1.68e-04 | gates=[0.251953125, 0.255859375] | mix=nback\n",
      "step 30 | loss 2.4026 | lr 2.00e-04 | gates=[0.2373046875, 0.240234375] | mix=hf\n",
      "step 40 | loss 7.2257 | lr 1.99e-04 | gates=[0.2578125, 0.263671875] | mix=repeat\n",
      "step 50 | loss 2.8536 | lr 1.99e-04 | gates=[0.2236328125, 0.228515625] | mix=hf\n",
      "step 60 | loss 6.7658 | lr 1.97e-04 | gates=[0.25390625, 0.271484375] | mix=nback\n",
      "step 70 | loss 6.8637 | lr 1.95e-04 | gates=[0.255859375, 0.275390625] | mix=copy\n",
      "step 80 | loss 6.1135 | lr 1.93e-04 | gates=[0.259765625, 0.279296875] | mix=copy\n",
      "step 90 | loss 1.7781 | lr 1.91e-04 | gates=[0.19921875, 0.2119140625] | mix=hf\n",
      "step 100 | loss 5.3427 | lr 1.88e-04 | gates=[0.259765625, 0.28125] | mix=repeat\n",
      "step 110 | loss 1.6011 | lr 1.84e-04 | gates=[0.185546875, 0.2041015625] | mix=hf\n",
      "step 120 | loss 1.5415 | lr 1.81e-04 | gates=[0.1845703125, 0.2060546875] | mix=hf\n",
      "step 130 | loss 1.5450 | lr 1.76e-04 | gates=[0.1845703125, 0.208984375] | mix=hf\n",
      "step 140 | loss 9.2277 | lr 1.72e-04 | gates=[0.259765625, 0.291015625] | mix=copy\n",
      "step 150 | loss 6.0319 | lr 1.67e-04 | gates=[0.26171875, 0.29296875] | mix=copy\n",
      "step 160 | loss 1.7149 | lr 1.62e-04 | gates=[0.1669921875, 0.189453125] | mix=hf\n",
      "step 170 | loss 5.7346 | lr 1.57e-04 | gates=[0.2578125, 0.29296875] | mix=copy\n",
      "step 180 | loss 1.4303 | lr 1.51e-04 | gates=[0.1650390625, 0.1875] | mix=hf\n",
      "step 190 | loss 5.6727 | lr 1.46e-04 | gates=[0.2451171875, 0.29296875] | mix=nback\n",
      "step 200 | loss 5.4288 | lr 1.40e-04 | gates=[0.2451171875, 0.29296875] | mix=nback\n",
      "step 210 | loss 5.1370 | lr 1.33e-04 | gates=[0.2451171875, 0.296875] | mix=nback\n",
      "step 220 | loss 1.1836 | lr 1.27e-04 | gates=[0.162109375, 0.19140625] | mix=hf\n",
      "step 230 | loss 1.4738 | lr 1.21e-04 | gates=[0.146484375, 0.1748046875] | mix=hf\n",
      "step 240 | loss 4.8534 | lr 1.14e-04 | gates=[0.267578125, 0.306640625] | mix=repeat\n",
      "step 250 | loss 5.1738 | lr 1.08e-04 | gates=[0.25, 0.30078125] | mix=nback\n",
      "step 260 | loss 5.5302 | lr 1.01e-04 | gates=[0.26171875, 0.306640625] | mix=copy\n",
      "step 270 | loss 5.4974 | lr 9.44e-05 | gates=[0.259765625, 0.30859375] | mix=copy\n",
      "step 280 | loss 5.2618 | lr 8.78e-05 | gates=[0.251953125, 0.306640625] | mix=nback\n",
      "step 290 | loss 4.9981 | lr 8.13e-05 | gates=[0.263671875, 0.30859375] | mix=repeat\n",
      "step 300 | loss 1.3768 | lr 7.48e-05 | gates=[0.1357421875, 0.162109375] | mix=hf\n",
      "step 310 | loss 1.3271 | lr 6.85e-05 | gates=[0.1357421875, 0.1630859375] | mix=hf\n",
      "step 320 | loss 4.7898 | lr 6.23e-05 | gates=[0.26171875, 0.30859375] | mix=repeat\n",
      "step 330 | loss 1.4903 | lr 5.62e-05 | gates=[0.1396484375, 0.166015625] | mix=hf\n",
      "step 340 | loss 4.6092 | lr 5.04e-05 | gates=[0.2734375, 0.31640625] | mix=repeat\n",
      "step 350 | loss 1.3502 | lr 4.48e-05 | gates=[0.134765625, 0.1611328125] | mix=hf\n",
      "step 360 | loss 5.6208 | lr 3.94e-05 | gates=[0.26171875, 0.3125] | mix=copy\n",
      "step 370 | loss 4.9553 | lr 3.42e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 380 | loss 1.1219 | lr 2.94e-05 | gates=[0.138671875, 0.1669921875] | mix=hf\n",
      "step 390 | loss 4.9555 | lr 2.49e-05 | gates=[0.255859375, 0.3125] | mix=nback\n",
      "step 400 | loss 4.9078 | lr 2.07e-05 | gates=[0.271484375, 0.31640625] | mix=repeat\n",
      "step 410 | loss 4.8760 | lr 2.00e-05 | gates=[0.25390625, 0.3125] | mix=nback\n",
      "step 420 | loss 1.3089 | lr 2.00e-05 | gates=[0.134765625, 0.1630859375] | mix=hf\n",
      "step 430 | loss 5.4493 | lr 2.00e-05 | gates=[0.26953125, 0.3203125] | mix=copy\n",
      "step 440 | loss 5.4301 | lr 2.00e-05 | gates=[0.275390625, 0.322265625] | mix=copy\n",
      "step 450 | loss 1.2898 | lr 2.00e-05 | gates=[0.1328125, 0.16015625] | mix=hf\n",
      "step 460 | loss 1.4554 | lr 2.00e-05 | gates=[0.13671875, 0.1630859375] | mix=hf\n",
      "step 470 | loss 1.5372 | lr 2.00e-05 | gates=[0.1328125, 0.1591796875] | mix=hf\n",
      "step 480 | loss 5.4075 | lr 2.00e-05 | gates=[0.2734375, 0.322265625] | mix=copy\n",
      "step 490 | loss 4.7853 | lr 2.00e-05 | gates=[0.27734375, 0.322265625] | mix=repeat\n",
      "step 500 | loss 1.4105 | lr 2.00e-05 | gates=[0.130859375, 0.15625] | mix=hf\n",
      "[after  E6_temp0p8_seed4242] alloc=29.34 GB | reserved=50.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed1337 | seed=1337 ===\n",
      "TB run started: ./runs\\dncformer-20250820-164415-E7_warmstart_seed1337\n",
      "CFG.gate_temp: 0.8 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed1337] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b8481efdf704907a3646928fa44b7e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 7.1638 | lr 8.80e-05 | gates=[0.2431640625, 0.2490234375] | mix=repeat\n",
      "step 20 | loss 1.8687 | lr 1.68e-04 | gates=[0.2373046875, 0.2431640625] | mix=hf\n",
      "step 30 | loss 6.0424 | lr 2.00e-04 | gates=[0.25390625, 0.259765625] | mix=nback\n",
      "step 40 | loss 2.2922 | lr 1.99e-04 | gates=[0.2236328125, 0.232421875] | mix=hf\n",
      "step 50 | loss 1.4852 | lr 1.99e-04 | gates=[0.21484375, 0.2255859375] | mix=hf\n",
      "step 60 | loss 6.5158 | lr 1.97e-04 | gates=[0.291015625, 0.30078125] | mix=copy\n",
      "step 70 | loss 5.9229 | lr 1.95e-04 | gates=[0.2890625, 0.298828125] | mix=repeat\n",
      "step 80 | loss 5.7869 | lr 1.93e-04 | gates=[0.291015625, 0.3046875] | mix=copy\n",
      "step 90 | loss 1.7243 | lr 1.91e-04 | gates=[0.2294921875, 0.2431640625] | mix=hf\n",
      "step 100 | loss 5.8018 | lr 1.88e-04 | gates=[0.28515625, 0.306640625] | mix=nback\n",
      "step 110 | loss 5.0903 | lr 1.84e-04 | gates=[0.296875, 0.3125] | mix=repeat\n",
      "step 120 | loss 5.2232 | lr 1.81e-04 | gates=[0.283203125, 0.30859375] | mix=nback\n",
      "step 130 | loss 5.4257 | lr 1.76e-04 | gates=[0.28515625, 0.310546875] | mix=nback\n",
      "step 140 | loss 6.6510 | lr 1.72e-04 | gates=[0.283203125, 0.3125] | mix=nback\n",
      "step 150 | loss 1.5875 | lr 1.67e-04 | gates=[0.1982421875, 0.2177734375] | mix=hf\n",
      "step 160 | loss 1.4418 | lr 1.62e-04 | gates=[0.19140625, 0.2109375] | mix=hf\n",
      "step 170 | loss 1.5913 | lr 1.57e-04 | gates=[0.1845703125, 0.205078125] | mix=hf\n",
      "step 180 | loss 5.6345 | lr 1.51e-04 | gates=[0.28515625, 0.31640625] | mix=nback\n",
      "step 190 | loss 5.8077 | lr 1.46e-04 | gates=[0.291015625, 0.322265625] | mix=copy\n",
      "step 200 | loss 6.2038 | lr 1.40e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 210 | loss 5.4467 | lr 1.33e-04 | gates=[0.30078125, 0.33203125] | mix=copy\n",
      "step 220 | loss 5.2100 | lr 1.27e-04 | gates=[0.287109375, 0.326171875] | mix=nback\n",
      "step 230 | loss 5.2107 | lr 1.21e-04 | gates=[0.302734375, 0.33203125] | mix=repeat\n",
      "step 240 | loss 5.2596 | lr 1.14e-04 | gates=[0.28515625, 0.328125] | mix=nback\n",
      "step 250 | loss 1.4311 | lr 1.08e-04 | gates=[0.166015625, 0.1904296875] | mix=hf\n",
      "step 260 | loss 5.0308 | lr 1.01e-04 | gates=[0.310546875, 0.337890625] | mix=repeat\n",
      "step 270 | loss 1.5158 | lr 9.44e-05 | gates=[0.1611328125, 0.1845703125] | mix=hf\n",
      "step 280 | loss 5.7483 | lr 8.78e-05 | gates=[0.28125, 0.328125] | mix=nback\n",
      "step 290 | loss 5.5913 | lr 8.13e-05 | gates=[0.294921875, 0.33203125] | mix=repeat\n",
      "step 300 | loss 7.1839 | lr 7.48e-05 | gates=[0.287109375, 0.330078125] | mix=nback\n",
      "step 310 | loss 5.3411 | lr 6.85e-05 | gates=[0.296875, 0.330078125] | mix=repeat\n",
      "step 320 | loss 5.5067 | lr 6.23e-05 | gates=[0.30078125, 0.337890625] | mix=copy\n",
      "step 330 | loss 5.5551 | lr 5.62e-05 | gates=[0.3046875, 0.33984375] | mix=copy\n",
      "step 340 | loss 5.5801 | lr 5.04e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 350 | loss 5.1894 | lr 4.48e-05 | gates=[0.3125, 0.341796875] | mix=repeat\n",
      "step 360 | loss 1.7356 | lr 3.94e-05 | gates=[0.150390625, 0.1728515625] | mix=hf\n",
      "step 370 | loss 5.5183 | lr 3.42e-05 | gates=[0.29296875, 0.3359375] | mix=copy\n",
      "step 380 | loss 1.1121 | lr 2.94e-05 | gates=[0.154296875, 0.177734375] | mix=hf\n",
      "step 390 | loss 5.4260 | lr 2.49e-05 | gates=[0.30859375, 0.34375] | mix=copy\n",
      "step 400 | loss 1.3752 | lr 2.07e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 410 | loss 1.3046 | lr 2.00e-05 | gates=[0.150390625, 0.1728515625] | mix=hf\n",
      "step 420 | loss 4.6520 | lr 2.00e-05 | gates=[0.3046875, 0.341796875] | mix=repeat\n",
      "step 430 | loss 1.4948 | lr 2.00e-05 | gates=[0.1494140625, 0.173828125] | mix=hf\n",
      "step 440 | loss 1.3455 | lr 2.00e-05 | gates=[0.1484375, 0.171875] | mix=hf\n",
      "step 450 | loss 1.2662 | lr 2.00e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 460 | loss 1.5739 | lr 2.00e-05 | gates=[0.1484375, 0.1728515625] | mix=hf\n",
      "step 470 | loss 5.4359 | lr 2.00e-05 | gates=[0.3046875, 0.341796875] | mix=copy\n",
      "step 480 | loss 1.1348 | lr 2.00e-05 | gates=[0.1513671875, 0.1748046875] | mix=hf\n",
      "step 490 | loss 4.8031 | lr 2.00e-05 | gates=[0.302734375, 0.33984375] | mix=repeat\n",
      "step 500 | loss 5.4466 | lr 2.00e-05 | gates=[0.294921875, 0.337890625] | mix=copy\n",
      "[after  E7_warmstart_seed1337] alloc=29.34 GB | reserved=50.91 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed2027 | seed=2027 ===\n",
      "TB run started: ./runs\\dncformer-20250820-174016-E7_warmstart_seed2027\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed2027] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b569bcf4e7d47c68114ab593acb7201"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.4950 | lr 8.80e-05 | gates=[0.244140625, 0.24609375] | mix=hf\n",
      "step 20 | loss 5.4397 | lr 1.68e-04 | gates=[0.240234375, 0.2421875] | mix=hf\n",
      "step 30 | loss 5.8121 | lr 2.00e-04 | gates=[0.248046875, 0.255859375] | mix=nback\n",
      "step 40 | loss 8.2393 | lr 1.99e-04 | gates=[0.251953125, 0.263671875] | mix=copy\n",
      "step 50 | loss 6.9483 | lr 1.99e-04 | gates=[0.251953125, 0.265625] | mix=copy\n",
      "step 60 | loss 2.1542 | lr 1.97e-04 | gates=[0.25390625, 0.26171875] | mix=hf\n",
      "step 70 | loss 1.6823 | lr 1.95e-04 | gates=[0.248046875, 0.259765625] | mix=hf\n",
      "step 80 | loss 6.2565 | lr 1.93e-04 | gates=[0.28515625, 0.306640625] | mix=copy\n",
      "step 90 | loss 8.1904 | lr 1.91e-04 | gates=[0.28125, 0.306640625] | mix=nback\n",
      "step 100 | loss 6.0802 | lr 1.88e-04 | gates=[0.291015625, 0.3125] | mix=repeat\n",
      "step 110 | loss 6.4359 | lr 1.84e-04 | gates=[0.283203125, 0.310546875] | mix=copy\n",
      "step 120 | loss 1.4914 | lr 1.81e-04 | gates=[0.2236328125, 0.2451171875] | mix=hf\n",
      "step 130 | loss 5.9994 | lr 1.76e-04 | gates=[0.287109375, 0.318359375] | mix=copy\n",
      "step 140 | loss 5.9462 | lr 1.72e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 150 | loss 6.0262 | lr 1.67e-04 | gates=[0.279296875, 0.322265625] | mix=nback\n",
      "step 160 | loss 1.4679 | lr 1.62e-04 | gates=[0.2041015625, 0.23046875] | mix=hf\n",
      "step 170 | loss 5.2189 | lr 1.57e-04 | gates=[0.29296875, 0.330078125] | mix=repeat\n",
      "step 180 | loss 7.6009 | lr 1.51e-04 | gates=[0.275390625, 0.32421875] | mix=nback\n",
      "step 190 | loss 1.4652 | lr 1.46e-04 | gates=[0.1875, 0.21484375] | mix=hf\n",
      "step 200 | loss 1.3550 | lr 1.40e-04 | gates=[0.1884765625, 0.216796875] | mix=hf\n",
      "step 210 | loss 1.0956 | lr 1.33e-04 | gates=[0.193359375, 0.2216796875] | mix=hf\n",
      "step 220 | loss 5.3350 | lr 1.27e-04 | gates=[0.271484375, 0.32421875] | mix=nback\n",
      "step 230 | loss 1.4243 | lr 1.21e-04 | gates=[0.1728515625, 0.1982421875] | mix=hf\n",
      "step 240 | loss 5.6708 | lr 1.14e-04 | gates=[0.296875, 0.337890625] | mix=repeat\n",
      "step 250 | loss 1.6800 | lr 1.08e-04 | gates=[0.166015625, 0.193359375] | mix=hf\n",
      "step 260 | loss 1.8363 | lr 1.01e-04 | gates=[0.1650390625, 0.193359375] | mix=hf\n",
      "step 270 | loss 5.4899 | lr 9.44e-05 | gates=[0.294921875, 0.341796875] | mix=copy\n",
      "step 280 | loss 1.2525 | lr 8.78e-05 | gates=[0.1689453125, 0.197265625] | mix=hf\n",
      "step 290 | loss 1.4398 | lr 8.13e-05 | gates=[0.1650390625, 0.193359375] | mix=hf\n",
      "step 300 | loss 5.2471 | lr 7.48e-05 | gates=[0.275390625, 0.333984375] | mix=nback\n",
      "step 310 | loss 1.5230 | lr 6.85e-05 | gates=[0.1611328125, 0.1875] | mix=hf\n",
      "step 320 | loss 5.0540 | lr 6.23e-05 | gates=[0.271484375, 0.33203125] | mix=nback\n",
      "step 330 | loss 5.0565 | lr 5.62e-05 | gates=[0.275390625, 0.3359375] | mix=nback\n",
      "step 340 | loss 4.9982 | lr 5.04e-05 | gates=[0.287109375, 0.337890625] | mix=repeat\n",
      "step 350 | loss 1.4297 | lr 4.48e-05 | gates=[0.15625, 0.181640625] | mix=hf\n",
      "step 360 | loss 4.8631 | lr 3.94e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 370 | loss 1.2115 | lr 3.42e-05 | gates=[0.1640625, 0.189453125] | mix=hf\n",
      "step 380 | loss 1.4938 | lr 2.94e-05 | gates=[0.16015625, 0.189453125] | mix=hf\n",
      "step 390 | loss 4.8127 | lr 2.49e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 400 | loss 5.4351 | lr 2.07e-05 | gates=[0.28125, 0.33984375] | mix=copy\n",
      "step 410 | loss 4.8386 | lr 2.00e-05 | gates=[0.30078125, 0.34765625] | mix=repeat\n",
      "step 420 | loss 1.1400 | lr 2.00e-05 | gates=[0.1640625, 0.19140625] | mix=hf\n",
      "step 430 | loss 4.9045 | lr 2.00e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 440 | loss 1.2898 | lr 2.00e-05 | gates=[0.15625, 0.1826171875] | mix=hf\n",
      "step 450 | loss 5.4476 | lr 2.00e-05 | gates=[0.2890625, 0.34375] | mix=copy\n",
      "step 460 | loss 5.4332 | lr 2.00e-05 | gates=[0.291015625, 0.34375] | mix=copy\n",
      "step 470 | loss 1.1479 | lr 2.00e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 480 | loss 4.8677 | lr 2.00e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 490 | loss 5.4633 | lr 2.00e-05 | gates=[0.291015625, 0.345703125] | mix=copy\n",
      "step 500 | loss 0.9382 | lr 2.00e-05 | gates=[0.166015625, 0.1982421875] | mix=hf\n",
      "[after  E7_warmstart_seed2027] alloc=29.34 GB | reserved=49.33 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E7_warmstart_seed4242 | seed=4242 ===\n",
      "TB run started: ./runs\\dncformer-20250820-181956-E7_warmstart_seed4242\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E7_warmstart_seed4242] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d003beeac4048cba6ddf625019b320a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 6.8860 | lr 8.80e-05 | gates=[0.248046875, 0.2470703125] | mix=copy\n",
      "step 20 | loss 6.7967 | lr 1.68e-04 | gates=[0.25390625, 0.255859375] | mix=nback\n",
      "step 30 | loss 5.8866 | lr 2.00e-04 | gates=[0.2578125, 0.2578125] | mix=copy\n",
      "step 40 | loss 5.5299 | lr 1.99e-04 | gates=[0.259765625, 0.26171875] | mix=repeat\n",
      "step 50 | loss 1.4707 | lr 1.99e-04 | gates=[0.220703125, 0.228515625] | mix=hf\n",
      "step 60 | loss 5.3748 | lr 1.97e-04 | gates=[0.291015625, 0.30078125] | mix=nback\n",
      "step 70 | loss 8.4891 | lr 1.95e-04 | gates=[0.29296875, 0.30078125] | mix=copy\n",
      "step 80 | loss 6.0569 | lr 1.93e-04 | gates=[0.296875, 0.302734375] | mix=copy\n",
      "step 90 | loss 1.6448 | lr 1.91e-04 | gates=[0.228515625, 0.236328125] | mix=hf\n",
      "step 100 | loss 5.4202 | lr 1.88e-04 | gates=[0.294921875, 0.3046875] | mix=repeat\n",
      "step 110 | loss 1.7415 | lr 1.84e-04 | gates=[0.2138671875, 0.224609375] | mix=hf\n",
      "step 120 | loss 3.8637 | lr 1.81e-04 | gates=[0.212890625, 0.2255859375] | mix=hf\n",
      "step 130 | loss 1.7198 | lr 1.76e-04 | gates=[0.2119140625, 0.224609375] | mix=hf\n",
      "step 140 | loss 10.8270 | lr 1.72e-04 | gates=[0.291015625, 0.310546875] | mix=copy\n",
      "step 150 | loss 6.0492 | lr 1.67e-04 | gates=[0.29296875, 0.3125] | mix=copy\n",
      "step 160 | loss 2.5608 | lr 1.62e-04 | gates=[0.1982421875, 0.2158203125] | mix=hf\n",
      "step 170 | loss 5.8301 | lr 1.57e-04 | gates=[0.2890625, 0.31640625] | mix=copy\n",
      "step 180 | loss 1.5077 | lr 1.51e-04 | gates=[0.1982421875, 0.21484375] | mix=hf\n",
      "step 190 | loss 5.8626 | lr 1.46e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 200 | loss 5.3078 | lr 1.40e-04 | gates=[0.275390625, 0.314453125] | mix=nback\n",
      "step 210 | loss 5.4471 | lr 1.33e-04 | gates=[0.27734375, 0.31640625] | mix=nback\n",
      "step 220 | loss 1.2853 | lr 1.27e-04 | gates=[0.1923828125, 0.2158203125] | mix=hf\n",
      "step 230 | loss 1.5224 | lr 1.21e-04 | gates=[0.177734375, 0.2001953125] | mix=hf\n",
      "step 240 | loss 4.9142 | lr 1.14e-04 | gates=[0.296875, 0.32421875] | mix=repeat\n",
      "step 250 | loss 5.1006 | lr 1.08e-04 | gates=[0.27734375, 0.318359375] | mix=nback\n",
      "step 260 | loss 5.5793 | lr 1.01e-04 | gates=[0.291015625, 0.32421875] | mix=copy\n",
      "step 270 | loss 5.5309 | lr 9.44e-05 | gates=[0.2890625, 0.32421875] | mix=copy\n",
      "step 280 | loss 5.3767 | lr 8.78e-05 | gates=[0.279296875, 0.322265625] | mix=nback\n",
      "step 290 | loss 5.5246 | lr 8.13e-05 | gates=[0.29296875, 0.32421875] | mix=repeat\n",
      "step 300 | loss 1.3964 | lr 7.48e-05 | gates=[0.1650390625, 0.185546875] | mix=hf\n",
      "step 310 | loss 1.3540 | lr 6.85e-05 | gates=[0.1650390625, 0.1865234375] | mix=hf\n",
      "step 320 | loss 4.9202 | lr 6.23e-05 | gates=[0.2890625, 0.322265625] | mix=repeat\n",
      "step 330 | loss 1.5136 | lr 5.62e-05 | gates=[0.1669921875, 0.1884765625] | mix=hf\n",
      "step 340 | loss 4.7757 | lr 5.04e-05 | gates=[0.30078125, 0.33203125] | mix=repeat\n",
      "step 350 | loss 1.3729 | lr 4.48e-05 | gates=[0.162109375, 0.18359375] | mix=hf\n",
      "step 360 | loss 5.5856 | lr 3.94e-05 | gates=[0.287109375, 0.32421875] | mix=copy\n",
      "step 370 | loss 4.9658 | lr 3.42e-05 | gates=[0.279296875, 0.32421875] | mix=nback\n",
      "step 380 | loss 1.1322 | lr 2.94e-05 | gates=[0.166015625, 0.1884765625] | mix=hf\n",
      "step 390 | loss 4.9446 | lr 2.49e-05 | gates=[0.28125, 0.32421875] | mix=nback\n",
      "step 400 | loss 4.9412 | lr 2.07e-05 | gates=[0.296875, 0.330078125] | mix=repeat\n",
      "step 410 | loss 4.8818 | lr 2.00e-05 | gates=[0.27734375, 0.322265625] | mix=nback\n",
      "step 420 | loss 1.3139 | lr 2.00e-05 | gates=[0.162109375, 0.1845703125] | mix=hf\n",
      "step 430 | loss 5.4501 | lr 2.00e-05 | gates=[0.294921875, 0.33203125] | mix=copy\n",
      "step 440 | loss 5.4297 | lr 2.00e-05 | gates=[0.298828125, 0.333984375] | mix=copy\n",
      "step 450 | loss 1.3043 | lr 2.00e-05 | gates=[0.1591796875, 0.181640625] | mix=hf\n",
      "step 460 | loss 1.4742 | lr 2.00e-05 | gates=[0.1630859375, 0.1845703125] | mix=hf\n",
      "step 470 | loss 1.5606 | lr 2.00e-05 | gates=[0.1591796875, 0.1806640625] | mix=hf\n",
      "step 480 | loss 5.4087 | lr 2.00e-05 | gates=[0.296875, 0.333984375] | mix=copy\n",
      "step 490 | loss 4.7991 | lr 2.00e-05 | gates=[0.302734375, 0.3359375] | mix=repeat\n",
      "step 500 | loss 1.4304 | lr 2.00e-05 | gates=[0.1572265625, 0.177734375] | mix=hf\n",
      "[after  E7_warmstart_seed4242] alloc=29.34 GB | reserved=50.40 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.37 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N64 | seed=777 ===\n",
      "TB run started: ./runs\\dncformer-20250820-191327-E8_capacity_N64\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E8_capacity_N64] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5470c79e6ba412fabd39ac3c8ba54f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.6379 | lr 8.80e-05 | gates=[0.28125, 0.279296875] | mix=hf\n",
      "step 20 | loss 6.7689 | lr 1.68e-04 | gates=[0.279296875, 0.287109375] | mix=copy\n",
      "step 30 | loss 6.5771 | lr 2.00e-04 | gates=[0.28125, 0.291015625] | mix=nback\n",
      "step 40 | loss 6.0427 | lr 1.99e-04 | gates=[0.28515625, 0.294921875] | mix=repeat\n",
      "step 50 | loss 7.1813 | lr 1.99e-04 | gates=[0.28125, 0.294921875] | mix=nback\n",
      "step 60 | loss 2.1033 | lr 1.97e-04 | gates=[0.240234375, 0.24609375] | mix=hf\n",
      "step 70 | loss 1.5742 | lr 1.95e-04 | gates=[0.232421875, 0.240234375] | mix=hf\n",
      "step 80 | loss 8.6676 | lr 1.93e-04 | gates=[0.28125, 0.302734375] | mix=nback\n",
      "step 90 | loss 1.8938 | lr 1.91e-04 | gates=[0.2099609375, 0.22265625] | mix=hf\n",
      "step 100 | loss 5.6929 | lr 1.88e-04 | gates=[0.275390625, 0.306640625] | mix=nback\n",
      "step 110 | loss 1.7913 | lr 1.84e-04 | gates=[0.1982421875, 0.2138671875] | mix=hf\n",
      "step 120 | loss 7.8458 | lr 1.81e-04 | gates=[0.287109375, 0.31640625] | mix=copy\n",
      "step 130 | loss 9.1497 | lr 1.76e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 140 | loss 5.9620 | lr 1.72e-04 | gates=[0.279296875, 0.31640625] | mix=copy\n",
      "step 150 | loss 1.0452 | lr 1.67e-04 | gates=[0.1923828125, 0.21484375] | mix=hf\n",
      "step 160 | loss 5.2955 | lr 1.62e-04 | gates=[0.28125, 0.32421875] | mix=nback\n",
      "step 170 | loss 5.7708 | lr 1.57e-04 | gates=[0.287109375, 0.326171875] | mix=copy\n",
      "step 180 | loss 5.1257 | lr 1.51e-04 | gates=[0.296875, 0.33203125] | mix=repeat\n",
      "step 190 | loss 1.9651 | lr 1.46e-04 | gates=[0.17578125, 0.2021484375] | mix=hf\n",
      "step 200 | loss 5.2592 | lr 1.40e-04 | gates=[0.28125, 0.330078125] | mix=nback\n",
      "step 210 | loss 4.9860 | lr 1.33e-04 | gates=[0.28125, 0.333984375] | mix=nback\n",
      "step 220 | loss 5.6707 | lr 1.27e-04 | gates=[0.30078125, 0.341796875] | mix=copy\n",
      "step 230 | loss 5.6167 | lr 1.21e-04 | gates=[0.291015625, 0.33984375] | mix=copy\n",
      "step 240 | loss 5.2426 | lr 1.14e-04 | gates=[0.283203125, 0.337890625] | mix=nback\n",
      "step 250 | loss 5.2357 | lr 1.08e-04 | gates=[0.302734375, 0.345703125] | mix=repeat\n",
      "step 260 | loss 1.2203 | lr 1.01e-04 | gates=[0.16796875, 0.193359375] | mix=hf\n",
      "step 270 | loss 1.2332 | lr 9.44e-05 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 280 | loss 5.6227 | lr 8.78e-05 | gates=[0.29296875, 0.34375] | mix=copy\n",
      "step 290 | loss 5.0960 | lr 8.13e-05 | gates=[0.279296875, 0.33984375] | mix=nback\n",
      "step 300 | loss 4.8855 | lr 7.48e-05 | gates=[0.28125, 0.341796875] | mix=nback\n",
      "step 310 | loss 1.3677 | lr 6.85e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 320 | loss 1.3331 | lr 6.23e-05 | gates=[0.154296875, 0.1787109375] | mix=hf\n",
      "step 330 | loss 5.4267 | lr 5.62e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 340 | loss 4.9618 | lr 5.04e-05 | gates=[0.28515625, 0.34375] | mix=nback\n",
      "step 350 | loss 1.0984 | lr 4.48e-05 | gates=[0.16015625, 0.1865234375] | mix=hf\n",
      "step 360 | loss 4.8881 | lr 3.94e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 370 | loss 1.4181 | lr 3.42e-05 | gates=[0.1513671875, 0.17578125] | mix=hf\n",
      "step 380 | loss 1.2786 | lr 2.94e-05 | gates=[0.158203125, 0.185546875] | mix=hf\n",
      "step 390 | loss 1.4918 | lr 2.49e-05 | gates=[0.15234375, 0.1767578125] | mix=hf\n",
      "step 400 | loss 1.4856 | lr 2.07e-05 | gates=[0.15625, 0.1826171875] | mix=hf\n",
      "step 410 | loss 5.4803 | lr 2.00e-05 | gates=[0.3046875, 0.353515625] | mix=copy\n",
      "step 420 | loss 5.4546 | lr 2.00e-05 | gates=[0.294921875, 0.349609375] | mix=copy\n",
      "step 430 | loss 1.3307 | lr 2.00e-05 | gates=[0.16015625, 0.18359375] | mix=hf\n",
      "step 440 | loss 4.8551 | lr 2.00e-05 | gates=[0.28125, 0.34375] | mix=nback\n",
      "step 450 | loss 1.5435 | lr 2.00e-05 | gates=[0.1513671875, 0.173828125] | mix=hf\n",
      "step 460 | loss 1.4390 | lr 2.00e-05 | gates=[0.150390625, 0.173828125] | mix=hf\n",
      "step 470 | loss 4.8235 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 480 | loss 4.8741 | lr 2.00e-05 | gates=[0.283203125, 0.345703125] | mix=nback\n",
      "step 490 | loss 4.8593 | lr 2.00e-05 | gates=[0.298828125, 0.349609375] | mix=repeat\n",
      "step 500 | loss 1.0074 | lr 2.00e-05 | gates=[0.1572265625, 0.181640625] | mix=hf\n",
      "[after  E8_capacity_N64] alloc=29.34 GB | reserved=54.00 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.38 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E8_capacity_N128 | seed=777 ===\n",
      "TB run started: ./runs\\dncformer-20250820-195242-E8_capacity_N128\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E8_capacity_N128] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cbdec81063d4f7786b23319a21bf5c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.6384 | lr 8.80e-05 | gates=[0.28125, 0.279296875] | mix=hf\n",
      "step 20 | loss 6.7687 | lr 1.68e-04 | gates=[0.279296875, 0.287109375] | mix=copy\n",
      "step 30 | loss 6.5741 | lr 2.00e-04 | gates=[0.28125, 0.291015625] | mix=nback\n",
      "step 40 | loss 6.0421 | lr 1.99e-04 | gates=[0.28515625, 0.294921875] | mix=repeat\n",
      "step 50 | loss 6.5700 | lr 1.99e-04 | gates=[0.28125, 0.294921875] | mix=nback\n",
      "step 60 | loss 2.1646 | lr 1.97e-04 | gates=[0.240234375, 0.24609375] | mix=hf\n",
      "step 70 | loss 1.6074 | lr 1.95e-04 | gates=[0.232421875, 0.240234375] | mix=hf\n",
      "step 80 | loss 5.7022 | lr 1.93e-04 | gates=[0.28125, 0.302734375] | mix=nback\n",
      "step 90 | loss 1.8592 | lr 1.91e-04 | gates=[0.2099609375, 0.22265625] | mix=hf\n",
      "step 100 | loss 6.3857 | lr 1.88e-04 | gates=[0.275390625, 0.3046875] | mix=nback\n",
      "step 110 | loss 1.9222 | lr 1.84e-04 | gates=[0.19921875, 0.21484375] | mix=hf\n",
      "step 120 | loss 5.9504 | lr 1.81e-04 | gates=[0.2890625, 0.314453125] | mix=copy\n",
      "step 130 | loss 7.1080 | lr 1.76e-04 | gates=[0.27734375, 0.314453125] | mix=nback\n",
      "step 140 | loss 5.8286 | lr 1.72e-04 | gates=[0.283203125, 0.322265625] | mix=copy\n",
      "step 150 | loss 1.2142 | lr 1.67e-04 | gates=[0.1962890625, 0.2197265625] | mix=hf\n",
      "step 160 | loss 5.6951 | lr 1.62e-04 | gates=[0.283203125, 0.328125] | mix=nback\n",
      "step 170 | loss 6.0265 | lr 1.57e-04 | gates=[0.287109375, 0.328125] | mix=copy\n",
      "step 180 | loss 5.3573 | lr 1.51e-04 | gates=[0.294921875, 0.333984375] | mix=repeat\n",
      "step 190 | loss 2.1462 | lr 1.46e-04 | gates=[0.1806640625, 0.20703125] | mix=hf\n",
      "step 200 | loss 5.3493 | lr 1.40e-04 | gates=[0.279296875, 0.330078125] | mix=nback\n",
      "step 210 | loss 5.3006 | lr 1.33e-04 | gates=[0.279296875, 0.33203125] | mix=nback\n",
      "step 220 | loss 5.8976 | lr 1.27e-04 | gates=[0.296875, 0.33984375] | mix=copy\n",
      "step 230 | loss 5.7644 | lr 1.21e-04 | gates=[0.2890625, 0.337890625] | mix=copy\n",
      "step 240 | loss 5.5860 | lr 1.14e-04 | gates=[0.279296875, 0.333984375] | mix=nback\n",
      "step 250 | loss 5.0976 | lr 1.08e-04 | gates=[0.298828125, 0.341796875] | mix=repeat\n",
      "step 260 | loss 1.2219 | lr 1.01e-04 | gates=[0.171875, 0.1962890625] | mix=hf\n",
      "step 270 | loss 1.2188 | lr 9.44e-05 | gates=[0.169921875, 0.1953125] | mix=hf\n",
      "step 280 | loss 5.5387 | lr 8.78e-05 | gates=[0.291015625, 0.341796875] | mix=copy\n",
      "step 290 | loss 4.9039 | lr 8.13e-05 | gates=[0.27734375, 0.33984375] | mix=nback\n",
      "step 300 | loss 4.8545 | lr 7.48e-05 | gates=[0.28125, 0.341796875] | mix=nback\n",
      "step 310 | loss 1.3759 | lr 6.85e-05 | gates=[0.1611328125, 0.1875] | mix=hf\n",
      "step 320 | loss 1.3384 | lr 6.23e-05 | gates=[0.1572265625, 0.181640625] | mix=hf\n",
      "step 330 | loss 5.5661 | lr 5.62e-05 | gates=[0.291015625, 0.34765625] | mix=copy\n",
      "step 340 | loss 5.1252 | lr 5.04e-05 | gates=[0.283203125, 0.34375] | mix=nback\n",
      "step 350 | loss 1.0931 | lr 4.48e-05 | gates=[0.162109375, 0.1884765625] | mix=hf\n",
      "step 360 | loss 4.9137 | lr 3.94e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 370 | loss 1.4183 | lr 3.42e-05 | gates=[0.154296875, 0.177734375] | mix=hf\n",
      "step 380 | loss 1.2778 | lr 2.94e-05 | gates=[0.16015625, 0.1875] | mix=hf\n",
      "step 390 | loss 1.4921 | lr 2.49e-05 | gates=[0.1552734375, 0.1787109375] | mix=hf\n",
      "step 400 | loss 1.4885 | lr 2.07e-05 | gates=[0.158203125, 0.1845703125] | mix=hf\n",
      "step 410 | loss 5.5278 | lr 2.00e-05 | gates=[0.302734375, 0.353515625] | mix=copy\n",
      "step 420 | loss 5.4768 | lr 2.00e-05 | gates=[0.29296875, 0.349609375] | mix=copy\n",
      "step 430 | loss 1.3314 | lr 2.00e-05 | gates=[0.162109375, 0.185546875] | mix=hf\n",
      "step 440 | loss 4.8825 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 450 | loss 1.5504 | lr 2.00e-05 | gates=[0.1533203125, 0.17578125] | mix=hf\n",
      "step 460 | loss 1.4431 | lr 2.00e-05 | gates=[0.15234375, 0.17578125] | mix=hf\n",
      "step 470 | loss 4.8245 | lr 2.00e-05 | gates=[0.279296875, 0.34375] | mix=nback\n",
      "step 480 | loss 4.8751 | lr 2.00e-05 | gates=[0.28125, 0.345703125] | mix=nback\n",
      "step 490 | loss 4.8706 | lr 2.00e-05 | gates=[0.296875, 0.34765625] | mix=repeat\n",
      "step 500 | loss 1.0085 | lr 2.00e-05 | gates=[0.1591796875, 0.18359375] | mix=hf\n",
      "[after  E8_capacity_N128] alloc=29.34 GB | reserved=54.00 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.38 GB | free=0.00 GB | total=25.77 GB\n",
      "\n",
      "=== E9_baseline_haystack | seed=31415 ===\n",
      "TB run started: ./runs\\dncformer-20250820-210448-E9_baseline_haystack\n",
      "CFG.gate_temp: 1.0 | CFG.gate_reg_lambda: 0.0 | mixture: (0.4, 0.2, 0.2, 0.2)\n",
      "[after free] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n",
      "[before E9_baseline_haystack] alloc=19.56 GB | reserved=19.60 GB | free=4.77 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba751c396da24aa28042857a8f84a616"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10 | loss 2.5027 | lr 8.80e-05 | gates=[0.27734375, 0.279296875] | mix=hf\n",
      "step 20 | loss 2.2774 | lr 1.68e-04 | gates=[0.26953125, 0.271484375] | mix=hf\n",
      "step 30 | loss 6.3625 | lr 2.00e-04 | gates=[0.28125, 0.2890625] | mix=nback\n",
      "step 40 | loss 6.3541 | lr 1.99e-04 | gates=[0.28515625, 0.29296875] | mix=copy\n",
      "step 50 | loss 1.6552 | lr 1.99e-04 | gates=[0.240234375, 0.2470703125] | mix=hf\n",
      "step 60 | loss 1.6690 | lr 1.97e-04 | gates=[0.23828125, 0.2451171875] | mix=hf\n",
      "step 70 | loss 2.0622 | lr 1.95e-04 | gates=[0.2275390625, 0.2373046875] | mix=hf\n",
      "step 80 | loss 1.6111 | lr 1.93e-04 | gates=[0.220703125, 0.232421875] | mix=hf\n",
      "step 90 | loss 1.6116 | lr 1.91e-04 | gates=[0.2158203125, 0.2294921875] | mix=hf\n",
      "step 100 | loss 5.7697 | lr 1.88e-04 | gates=[0.28125, 0.30859375] | mix=copy\n",
      "step 110 | loss 3.2152 | lr 1.84e-04 | gates=[0.2021484375, 0.2216796875] | mix=hf\n",
      "step 120 | loss 2.9168 | lr 1.81e-04 | gates=[0.20703125, 0.2294921875] | mix=hf\n",
      "step 130 | loss 5.2972 | lr 1.76e-04 | gates=[0.2890625, 0.318359375] | mix=repeat\n",
      "step 140 | loss 6.2627 | lr 1.72e-04 | gates=[0.2890625, 0.32421875] | mix=copy\n",
      "step 150 | loss 3.5212 | lr 1.67e-04 | gates=[0.201171875, 0.2265625] | mix=hf\n",
      "step 160 | loss 5.0936 | lr 1.62e-04 | gates=[0.275390625, 0.3203125] | mix=nback\n",
      "step 170 | loss 5.3806 | lr 1.57e-04 | gates=[0.27734375, 0.32421875] | mix=nback\n",
      "step 180 | loss 1.3807 | lr 1.51e-04 | gates=[0.1904296875, 0.21484375] | mix=hf\n",
      "step 190 | loss 5.8375 | lr 1.46e-04 | gates=[0.275390625, 0.326171875] | mix=nback\n",
      "step 200 | loss 4.9613 | lr 1.40e-04 | gates=[0.29296875, 0.33203125] | mix=repeat\n",
      "step 210 | loss 5.1688 | lr 1.33e-04 | gates=[0.296875, 0.333984375] | mix=repeat\n",
      "step 220 | loss 5.1036 | lr 1.27e-04 | gates=[0.291015625, 0.33203125] | mix=repeat\n",
      "step 230 | loss 5.0059 | lr 1.21e-04 | gates=[0.28515625, 0.33203125] | mix=repeat\n",
      "step 240 | loss 1.7511 | lr 1.14e-04 | gates=[0.1806640625, 0.2060546875] | mix=hf\n",
      "step 250 | loss 1.6581 | lr 1.08e-04 | gates=[0.181640625, 0.20703125] | mix=hf\n",
      "step 260 | loss 5.3927 | lr 1.01e-04 | gates=[0.294921875, 0.337890625] | mix=repeat\n",
      "step 270 | loss 1.3165 | lr 9.44e-05 | gates=[0.17578125, 0.201171875] | mix=hf\n",
      "step 280 | loss 5.2877 | lr 8.78e-05 | gates=[0.298828125, 0.33984375] | mix=repeat\n",
      "step 290 | loss 5.0323 | lr 8.13e-05 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 300 | loss 1.3564 | lr 7.48e-05 | gates=[0.169921875, 0.1943359375] | mix=hf\n",
      "step 310 | loss 4.8378 | lr 6.85e-05 | gates=[0.296875, 0.341796875] | mix=repeat\n",
      "step 320 | loss 1.4902 | lr 6.23e-05 | gates=[0.1728515625, 0.19921875] | mix=hf\n",
      "step 330 | loss 4.8779 | lr 5.62e-05 | gates=[0.298828125, 0.34375] | mix=repeat\n",
      "step 340 | loss 1.4196 | lr 5.04e-05 | gates=[0.1669921875, 0.1923828125] | mix=hf\n",
      "step 350 | loss 1.3047 | lr 4.48e-05 | gates=[0.16796875, 0.193359375] | mix=hf\n",
      "step 360 | loss 1.2860 | lr 3.94e-05 | gates=[0.171875, 0.1962890625] | mix=hf\n",
      "step 370 | loss 5.0669 | lr 3.42e-05 | gates=[0.275390625, 0.337890625] | mix=nback\n",
      "step 380 | loss 1.4352 | lr 2.94e-05 | gates=[0.1591796875, 0.1826171875] | mix=hf\n",
      "step 390 | loss 4.8232 | lr 2.49e-05 | gates=[0.28125, 0.33984375] | mix=nback\n",
      "step 400 | loss 1.1356 | lr 2.07e-05 | gates=[0.1630859375, 0.1865234375] | mix=hf\n",
      "step 410 | loss 4.8544 | lr 2.00e-05 | gates=[0.27734375, 0.337890625] | mix=nback\n",
      "step 420 | loss 5.4727 | lr 2.00e-05 | gates=[0.296875, 0.345703125] | mix=copy\n",
      "step 430 | loss 1.4443 | lr 2.00e-05 | gates=[0.15625, 0.1787109375] | mix=hf\n",
      "step 440 | loss 1.3751 | lr 2.00e-05 | gates=[0.1669921875, 0.19140625] | mix=hf\n",
      "step 450 | loss 1.3825 | lr 2.00e-05 | gates=[0.1591796875, 0.1826171875] | mix=hf\n",
      "step 460 | loss 5.4045 | lr 2.00e-05 | gates=[0.291015625, 0.34375] | mix=copy\n",
      "step 470 | loss 4.7521 | lr 2.00e-05 | gates=[0.30078125, 0.34765625] | mix=repeat\n",
      "step 480 | loss 5.4408 | lr 2.00e-05 | gates=[0.29296875, 0.345703125] | mix=copy\n",
      "step 490 | loss 1.0640 | lr 2.00e-05 | gates=[0.16015625, 0.1845703125] | mix=hf\n",
      "step 500 | loss 1.2654 | lr 2.00e-05 | gates=[0.15234375, 0.1748046875] | mix=hf\n",
      "[Haystack] acc=0.000 | loss=15.411 | fast=True\n",
      "[after  E9_baseline_haystack] alloc=29.34 GB | reserved=50.04 GB | free=0.00 GB | total=25.77 GB\n",
      "[after free] alloc=29.34 GB | reserved=29.36 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DNCFormerHead(\n   (base): Phi3ForCausalLM(\n     (model): Phi3Model(\n       (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n       (embed_dropout): Dropout(p=0.0, inplace=False)\n       (layers): ModuleList(\n         (0-31): 32 x Phi3DecoderLayer(\n           (self_attn): Phi3Attention(\n             (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n             (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n             (rotary_emb): Phi3RotaryEmbedding()\n           )\n           (mlp): Phi3MLP(\n             (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n             (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n             (activation_fn): SiLU()\n           )\n           (input_layernorm): Phi3RMSNorm()\n           (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n           (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n           (post_attention_layernorm): Phi3RMSNorm()\n         )\n       )\n       (norm): Phi3RMSNorm()\n     )\n     (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n   )\n   (blocks): ModuleList(\n     (0-1): 2 x ParallelEnrichmentBlock(\n       (vanilla): VanillaTransformerBlock(\n         (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (attn): MultiheadAttention(\n           (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n         )\n         (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n         (ff): Sequential(\n           (0): Linear(in_features=3072, out_features=12288, bias=True)\n           (1): GELU(approximate='none')\n           (2): Dropout(p=0.1, inplace=False)\n           (3): Linear(in_features=12288, out_features=3072, bias=True)\n         )\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n       (dncblock): DNCformerBlock(\n         (ctrl): TransformerController(\n           (proj_in): Linear(in_features=3200, out_features=3072, bias=True)\n           (ln1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (attn): MultiheadAttention(\n             (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n           )\n           (ln2): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n           (ff): Sequential(\n             (0): Linear(in_features=3072, out_features=12288, bias=True)\n             (1): GELU(approximate='none')\n             (2): Dropout(p=0.1, inplace=False)\n             (3): Linear(in_features=12288, out_features=3072, bias=True)\n           )\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (if_head): DNCInterfaceHead(\n           (proj): Linear(in_features=3072, out_features=333, bias=True)\n         )\n         (mem): DNCMemory()\n         (out_proj): Linear(in_features=3200, out_features=3072, bias=True)\n       )\n       (pre_gate_ln): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n       (gate): Linear(in_features=6144, out_features=3072, bias=True)\n     )\n   )\n   (proj_out): Identity()\n ),\n LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n \t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n \t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n }\n ))"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, time\n",
    "\n",
    "def set_cfg(**kv):\n",
    "    for k, v in kv.items():\n",
    "        setattr(CFG, k, v)\n",
    "\n",
    "\n",
    "def run_one_labeled(label, steps, mixture_weights, seed=1234,\n",
    "                    mixture_schedule=None, gate_temp_schedule=None, gate_reg_schedule=None,\n",
    "                    post_haystack=False):\n",
    "    print(f\"\\n=== {label} | seed={seed} ===\")\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed) \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_cfg(force_g=None)  # ensure no ablation\n",
    "    # unique TB run for each experiment label\n",
    "    start_tb_run(label)\n",
    "\n",
    "    # echo run metadata\n",
    "    if TB_AVAILABLE and 'tb' in globals():\n",
    "        import json\n",
    "        tb.add_text(\"run/meta\", json.dumps({\n",
    "            \"label\": label,\n",
    "            \"steps\": steps,\n",
    "            \"mixture_weights\": list(mixture_weights),\n",
    "            \"mixture_schedule\": mixture_schedule,\n",
    "            \"gate_temp_schedule\": gate_temp_schedule,\n",
    "            \"gate_reg_schedule\": gate_reg_schedule,\n",
    "        }, indent=2), 0)\n",
    "    \n",
    "    # print config stub\n",
    "    print(\"CFG.gate_temp:\", getattr(CFG, \"gate_temp\", 1.0),\n",
    "          \"| CFG.gate_reg_lambda:\", getattr(CFG, \"gate_reg_lambda\", 0.0),\n",
    "          \"| mixture:\", mixture_weights)\n",
    "\n",
    "    free_head_and_cache()\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"before {label}\")\n",
    "\n",
    "    head, tok = train_experiment(\n",
    "        steps=steps,\n",
    "        warmup_steps=max(10, steps//20),\n",
    "        mixture_weights=mixture_weights,\n",
    "        mixture_schedule=mixture_schedule,\n",
    "        gate_temp_schedule=gate_temp_schedule,\n",
    "        gate_reg_schedule=gate_reg_schedule,\n",
    "        viz_memory_after=False,\n",
    "    )\n",
    "\n",
    "    if post_haystack:\n",
    "        evaluate_haystack(head, steps=50, batch=16, T=256, vocab=1024, tb_step=steps, fast=True)\n",
    "\n",
    "    if 'cuda_report' in globals(): cuda_report(f\"after  {label}\")\n",
    "    free_head_and_cache()\n",
    "    return head, tok\n",
    "\n",
    "# Common settings\n",
    "EXP_STEPS = 500\n",
    "BASE_MIX  = (0.4, 0.2, 0.2, 0.2)\n",
    "SEEDS     = [1337, 2027, 4242]\n",
    "\n",
    "# === E6: gate_temp=0.8 (3 seeds), otherwise baseline ===\n",
    "set_cfg(gate_reg_lambda=getattr(CFG, \"gate_reg_lambda\", 2e-4))  # low-λ default\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E6_temp0p8_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    gate_temp_schedule=[(None, 0.8)])\n",
    "\n",
    "# === E7: memory-leaning warm-start (first 10% steps), then baseline; 3 seeds ===\n",
    "warm_steps = max(50, EXP_STEPS // 10)\n",
    "mix_warm   = (0.3, 0.3, 0.25, 0.15)  # a bit more memory-heavy than baseline\n",
    "mix_main   = BASE_MIX\n",
    "for s in SEEDS:\n",
    "    run_one_labeled(f\"E7_warmstart_seed{s}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=s,\n",
    "                    mixture_schedule=[(warm_steps, mix_warm), (None, mix_main)],\n",
    "                    gate_temp_schedule=[(warm_steps, 0.8), (None, 1.0)],\n",
    "                    gate_reg_schedule=[(warm_steps, max(2e-4, getattr(CFG,'gate_reg_lambda', 2e-4))), (None, getattr(CFG,'gate_reg_lambda', 2e-4))])\n",
    "\n",
    "# === E8: capacity sweep N=64 vs N=128 (1 seed each) ===\n",
    "for N_val in (64, 128):\n",
    "    set_cfg(N=N_val)  # assumes your DNC block reads CFG.N at construction\n",
    "    run_one_labeled(f\"E8_capacity_N{N_val}\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                    seed=777,\n",
    "                    gate_temp_schedule=[(None, getattr(CFG, 'gate_temp', 1.0))])\n",
    "\n",
    "# Reset N if changed\n",
    "set_cfg(N=getattr(CFG, 'N', 128))\n",
    "\n",
    "# === E9: baseline with haystack eval ===\n",
    "run_one_labeled(\"E9_baseline_haystack\", steps=EXP_STEPS, mixture_weights=BASE_MIX,\n",
    "                seed=31415, post_haystack=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:19.682094200Z",
     "start_time": "2025-08-20T21:32:12.614600900Z"
    }
   },
   "id": "ff510cbc0c8eb5c5"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after free] alloc=29.34 GB | reserved=29.36 GB | free=0.00 GB | total=25.77 GB\n"
     ]
    }
   ],
   "source": [
    "free_head_and_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:19.837094500Z",
     "start_time": "2025-08-21T04:40:19.679095Z"
    }
   },
   "id": "823dd4035914de8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Experiments - Stage 2 - tiered/parallel memory systems"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e4b05cfd4e94265"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:19.882095600Z",
     "start_time": "2025-08-21T04:40:19.837094500Z"
    }
   },
   "id": "da253ebbc27abf4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 00. Misc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a94ff038fb1b7b65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensorboard log dump"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c869b656957faac2"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard version: 2.20.0\n",
      "Discovered 16 run(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_203344\\2495429331.py:115: RuntimeWarning: Mean of empty slice\n",
      "  return float(np.nanmean(arr[-k:]))\n"
     ]
    },
    {
     "data": {
      "text/plain": "                        label  \\\n0   dncformer-20250817-184608   \n1   dncformer-20250817-192155   \n2   dncformer-20250817-193701   \n3   dncformer-20250817-195856   \n4   dncformer-20250817-201426   \n5   dncformer-20250817-203016   \n6   dncformer-20250820-093245   \n7         E6_temp0p8_seed1337   \n8         E6_temp0p8_seed2027   \n9         E6_temp0p8_seed4242   \n10      E7_warmstart_seed1337   \n11      E7_warmstart_seed2027   \n12      E7_warmstart_seed4242   \n13            E8_capacity_N64   \n14           E8_capacity_N128   \n15       E9_baseline_haystack   \n\n                                             run_id  n_event_files  \\\n0                         dncformer-20250817-184608              1   \n1                         dncformer-20250817-192155              1   \n2                         dncformer-20250817-193701              1   \n3                         dncformer-20250817-195856              1   \n4                         dncformer-20250817-201426              1   \n5                         dncformer-20250817-203016              1   \n6                         dncformer-20250820-093245              1   \n7     dncformer-20250820-143212-E6_temp0p8_seed1337              1   \n8     dncformer-20250820-151404-E6_temp0p8_seed2027              1   \n9     dncformer-20250820-160508-E6_temp0p8_seed4242              1   \n10  dncformer-20250820-164415-E7_warmstart_seed1337              1   \n11  dncformer-20250820-174016-E7_warmstart_seed2027              1   \n12  dncformer-20250820-181956-E7_warmstart_seed4242              1   \n13        dncformer-20250820-191327-E8_capacity_N64              1   \n14       dncformer-20250820-195242-E8_capacity_N128              1   \n15   dncformer-20250820-210448-E9_baseline_haystack              1   \n\n    steps_logged  loss_start~5  loss_end~10  loss_delta  lr_last  \\\n0            500     11.381193     3.251993    8.129200  0.00002   \n1            500     15.573140     2.726629   12.846511  0.00002   \n2            500      8.996346     3.851235    5.145111  0.00002   \n3            500     10.652820     3.229619    7.423202  0.00002   \n4            500      8.507831     3.762837    4.744994  0.00002   \n5            500      9.940354     6.097676    3.842678  0.00002   \n6            500      7.006995     4.464672    2.542323  0.00002   \n7            500      3.861429     2.831694    1.029735  0.00002   \n8            500      4.911413     3.535975    1.375438  0.00002   \n9            500      4.936950     3.295007    1.641943  0.00002   \n10           500      3.770464     2.845740    0.924724  0.00002   \n11           500      5.786885     3.547074    2.239811  0.00002   \n12           500      5.313995     3.305286    2.008709  0.00002   \n13           500      5.841568     3.566753    2.274815  0.00002   \n14           500      5.718674     3.579048    2.139626  0.00002   \n15           500      3.830387     3.245566    0.584821  0.00002   \n\n    haystack_acc_last  haystack_loss_last  ... gmean_b1_copy  gmean_b0_repeat  \\\n0                 NaN                 NaN  ...           NaN              NaN   \n1                 NaN                 NaN  ...           NaN              NaN   \n2                 NaN                 NaN  ...           NaN              NaN   \n3                 NaN                 NaN  ...           NaN              NaN   \n4                 NaN                 NaN  ...           NaN              NaN   \n5                 NaN                 NaN  ...           NaN              NaN   \n6                 0.0           10.111837  ...      0.337891         0.524740   \n7                 NaN                 NaN  ...      0.324870         0.273438   \n8                 NaN                 NaN  ...      0.323568         0.265625   \n9                 NaN                 NaN  ...      0.322266         0.274414   \n10                NaN                 NaN  ...      0.341146         0.303711   \n11                NaN                 NaN  ...      0.344727         0.300781   \n12                NaN                 NaN  ...      0.333333         0.299805   \n13                NaN                 NaN  ...      0.351562         0.298828   \n14                NaN                 NaN  ...      0.351562         0.296875   \n15                0.0           15.411427  ...      0.345703         0.299805   \n\n    gmean_b1_repeat  gmean_b0_nback  gmean_b1_nback  g_b0_Q1_mean  \\\n0               NaN             NaN             NaN           NaN   \n1               NaN             NaN             NaN           NaN   \n2               NaN             NaN             NaN           NaN   \n3               NaN             NaN             NaN           NaN   \n4               NaN             NaN             NaN           NaN   \n5               NaN             NaN             NaN           NaN   \n6          0.557509        0.485156        0.503320      0.427930   \n7          0.324219        0.252930        0.311523      0.223108   \n8          0.319336        0.250000        0.318359      0.221973   \n9          0.319336        0.254883        0.312500      0.225903   \n10         0.340820        0.284180        0.329102      0.252124   \n11         0.347656        0.276042        0.337891      0.249561   \n12         0.333008        0.279297        0.323242      0.252271   \n13         0.349609        0.280762        0.344238      0.246509   \n14         0.347656        0.279785        0.344238      0.246680   \n15         0.345703        0.277344        0.337891      0.249878   \n\n    g_b0_Q2_mean  g_b0_Q3_mean  g_b0_Q4_mean  \\\n0            NaN           NaN           NaN   \n1            NaN           NaN           NaN   \n2            NaN           NaN           NaN   \n3            NaN           NaN           NaN   \n4            NaN           NaN           NaN   \n5            NaN           NaN           NaN   \n6       0.433643      0.434521      0.434229   \n7       0.230737      0.232422      0.234937   \n8       0.229004      0.231018      0.231470   \n9       0.233276      0.234692      0.236499   \n10      0.260229      0.261621      0.264258   \n11      0.256470      0.258862      0.258691   \n12      0.259546      0.261426      0.262646   \n13      0.259937      0.261890      0.263354   \n14      0.259277      0.261084      0.262524   \n15      0.258081      0.259570      0.260278   \n\n                                              run_dir  \n0                      runs\\dncformer-20250817-184608  \n1                      runs\\dncformer-20250817-192155  \n2                      runs\\dncformer-20250817-193701  \n3                      runs\\dncformer-20250817-195856  \n4                      runs\\dncformer-20250817-201426  \n5                      runs\\dncformer-20250817-203016  \n6                      runs\\dncformer-20250820-093245  \n7   runs\\dncformer-20250820-143212-E6_temp0p8_seed...  \n8   runs\\dncformer-20250820-151404-E6_temp0p8_seed...  \n9   runs\\dncformer-20250820-160508-E6_temp0p8_seed...  \n10  runs\\dncformer-20250820-164415-E7_warmstart_se...  \n11  runs\\dncformer-20250820-174016-E7_warmstart_se...  \n12  runs\\dncformer-20250820-181956-E7_warmstart_se...  \n13     runs\\dncformer-20250820-191327-E8_capacity_N64  \n14    runs\\dncformer-20250820-195242-E8_capacity_N128  \n15  runs\\dncformer-20250820-210448-E9_baseline_hay...  \n\n[16 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>run_id</th>\n      <th>n_event_files</th>\n      <th>steps_logged</th>\n      <th>loss_start~5</th>\n      <th>loss_end~10</th>\n      <th>loss_delta</th>\n      <th>lr_last</th>\n      <th>haystack_acc_last</th>\n      <th>haystack_loss_last</th>\n      <th>...</th>\n      <th>gmean_b1_copy</th>\n      <th>gmean_b0_repeat</th>\n      <th>gmean_b1_repeat</th>\n      <th>gmean_b0_nback</th>\n      <th>gmean_b1_nback</th>\n      <th>g_b0_Q1_mean</th>\n      <th>g_b0_Q2_mean</th>\n      <th>g_b0_Q3_mean</th>\n      <th>g_b0_Q4_mean</th>\n      <th>run_dir</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dncformer-20250817-184608</td>\n      <td>dncformer-20250817-184608</td>\n      <td>1</td>\n      <td>500</td>\n      <td>11.381193</td>\n      <td>3.251993</td>\n      <td>8.129200</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-184608</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dncformer-20250817-192155</td>\n      <td>dncformer-20250817-192155</td>\n      <td>1</td>\n      <td>500</td>\n      <td>15.573140</td>\n      <td>2.726629</td>\n      <td>12.846511</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-192155</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dncformer-20250817-193701</td>\n      <td>dncformer-20250817-193701</td>\n      <td>1</td>\n      <td>500</td>\n      <td>8.996346</td>\n      <td>3.851235</td>\n      <td>5.145111</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-193701</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dncformer-20250817-195856</td>\n      <td>dncformer-20250817-195856</td>\n      <td>1</td>\n      <td>500</td>\n      <td>10.652820</td>\n      <td>3.229619</td>\n      <td>7.423202</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-195856</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dncformer-20250817-201426</td>\n      <td>dncformer-20250817-201426</td>\n      <td>1</td>\n      <td>500</td>\n      <td>8.507831</td>\n      <td>3.762837</td>\n      <td>4.744994</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-201426</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>dncformer-20250817-203016</td>\n      <td>dncformer-20250817-203016</td>\n      <td>1</td>\n      <td>500</td>\n      <td>9.940354</td>\n      <td>6.097676</td>\n      <td>3.842678</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>runs\\dncformer-20250817-203016</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dncformer-20250820-093245</td>\n      <td>dncformer-20250820-093245</td>\n      <td>1</td>\n      <td>500</td>\n      <td>7.006995</td>\n      <td>4.464672</td>\n      <td>2.542323</td>\n      <td>0.00002</td>\n      <td>0.0</td>\n      <td>10.111837</td>\n      <td>...</td>\n      <td>0.337891</td>\n      <td>0.524740</td>\n      <td>0.557509</td>\n      <td>0.485156</td>\n      <td>0.503320</td>\n      <td>0.427930</td>\n      <td>0.433643</td>\n      <td>0.434521</td>\n      <td>0.434229</td>\n      <td>runs\\dncformer-20250820-093245</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>1</td>\n      <td>500</td>\n      <td>3.861429</td>\n      <td>2.831694</td>\n      <td>1.029735</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.324870</td>\n      <td>0.273438</td>\n      <td>0.324219</td>\n      <td>0.252930</td>\n      <td>0.311523</td>\n      <td>0.223108</td>\n      <td>0.230737</td>\n      <td>0.232422</td>\n      <td>0.234937</td>\n      <td>runs\\dncformer-20250820-143212-E6_temp0p8_seed...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>E6_temp0p8_seed2027</td>\n      <td>dncformer-20250820-151404-E6_temp0p8_seed2027</td>\n      <td>1</td>\n      <td>500</td>\n      <td>4.911413</td>\n      <td>3.535975</td>\n      <td>1.375438</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.323568</td>\n      <td>0.265625</td>\n      <td>0.319336</td>\n      <td>0.250000</td>\n      <td>0.318359</td>\n      <td>0.221973</td>\n      <td>0.229004</td>\n      <td>0.231018</td>\n      <td>0.231470</td>\n      <td>runs\\dncformer-20250820-151404-E6_temp0p8_seed...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>E6_temp0p8_seed4242</td>\n      <td>dncformer-20250820-160508-E6_temp0p8_seed4242</td>\n      <td>1</td>\n      <td>500</td>\n      <td>4.936950</td>\n      <td>3.295007</td>\n      <td>1.641943</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.322266</td>\n      <td>0.274414</td>\n      <td>0.319336</td>\n      <td>0.254883</td>\n      <td>0.312500</td>\n      <td>0.225903</td>\n      <td>0.233276</td>\n      <td>0.234692</td>\n      <td>0.236499</td>\n      <td>runs\\dncformer-20250820-160508-E6_temp0p8_seed...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>E7_warmstart_seed1337</td>\n      <td>dncformer-20250820-164415-E7_warmstart_seed1337</td>\n      <td>1</td>\n      <td>500</td>\n      <td>3.770464</td>\n      <td>2.845740</td>\n      <td>0.924724</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.341146</td>\n      <td>0.303711</td>\n      <td>0.340820</td>\n      <td>0.284180</td>\n      <td>0.329102</td>\n      <td>0.252124</td>\n      <td>0.260229</td>\n      <td>0.261621</td>\n      <td>0.264258</td>\n      <td>runs\\dncformer-20250820-164415-E7_warmstart_se...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>E7_warmstart_seed2027</td>\n      <td>dncformer-20250820-174016-E7_warmstart_seed2027</td>\n      <td>1</td>\n      <td>500</td>\n      <td>5.786885</td>\n      <td>3.547074</td>\n      <td>2.239811</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.344727</td>\n      <td>0.300781</td>\n      <td>0.347656</td>\n      <td>0.276042</td>\n      <td>0.337891</td>\n      <td>0.249561</td>\n      <td>0.256470</td>\n      <td>0.258862</td>\n      <td>0.258691</td>\n      <td>runs\\dncformer-20250820-174016-E7_warmstart_se...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>E7_warmstart_seed4242</td>\n      <td>dncformer-20250820-181956-E7_warmstart_seed4242</td>\n      <td>1</td>\n      <td>500</td>\n      <td>5.313995</td>\n      <td>3.305286</td>\n      <td>2.008709</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.333333</td>\n      <td>0.299805</td>\n      <td>0.333008</td>\n      <td>0.279297</td>\n      <td>0.323242</td>\n      <td>0.252271</td>\n      <td>0.259546</td>\n      <td>0.261426</td>\n      <td>0.262646</td>\n      <td>runs\\dncformer-20250820-181956-E7_warmstart_se...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>E8_capacity_N64</td>\n      <td>dncformer-20250820-191327-E8_capacity_N64</td>\n      <td>1</td>\n      <td>500</td>\n      <td>5.841568</td>\n      <td>3.566753</td>\n      <td>2.274815</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.351562</td>\n      <td>0.298828</td>\n      <td>0.349609</td>\n      <td>0.280762</td>\n      <td>0.344238</td>\n      <td>0.246509</td>\n      <td>0.259937</td>\n      <td>0.261890</td>\n      <td>0.263354</td>\n      <td>runs\\dncformer-20250820-191327-E8_capacity_N64</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>E8_capacity_N128</td>\n      <td>dncformer-20250820-195242-E8_capacity_N128</td>\n      <td>1</td>\n      <td>500</td>\n      <td>5.718674</td>\n      <td>3.579048</td>\n      <td>2.139626</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.351562</td>\n      <td>0.296875</td>\n      <td>0.347656</td>\n      <td>0.279785</td>\n      <td>0.344238</td>\n      <td>0.246680</td>\n      <td>0.259277</td>\n      <td>0.261084</td>\n      <td>0.262524</td>\n      <td>runs\\dncformer-20250820-195242-E8_capacity_N128</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>E9_baseline_haystack</td>\n      <td>dncformer-20250820-210448-E9_baseline_haystack</td>\n      <td>1</td>\n      <td>500</td>\n      <td>3.830387</td>\n      <td>3.245566</td>\n      <td>0.584821</td>\n      <td>0.00002</td>\n      <td>0.0</td>\n      <td>15.411427</td>\n      <td>...</td>\n      <td>0.345703</td>\n      <td>0.299805</td>\n      <td>0.345703</td>\n      <td>0.277344</td>\n      <td>0.337891</td>\n      <td>0.249878</td>\n      <td>0.258081</td>\n      <td>0.259570</td>\n      <td>0.260278</td>\n      <td>runs\\dncformer-20250820-210448-E9_baseline_hay...</td>\n    </tr>\n  </tbody>\n</table>\n<p>16 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: analysis\\run_level_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  label                                         run_id  task  \\\n0   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n1   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n2   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n3   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n4   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n5   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n6   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n7   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n8   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n9   E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n10  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n11  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n12  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n13  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n14  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n15  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n16  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n17  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n18  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n19  E6_temp0p8_seed1337  dncformer-20250820-143212-E6_temp0p8_seed1337  copy   \n\n    block  step      loss    g_mean  g_frac>0.5        lr   g_b0_Q1   g_b0_Q2  \\\n0       0    30  6.567498  0.250000         0.0  0.000200  0.248047  0.251953   \n1       1    30  6.567498  0.257812         0.0  0.000200       NaN       NaN   \n2       0    80  6.022730  0.253906         0.0  0.000193  0.251953  0.255859   \n3       1    80  6.022730  0.275391         0.0  0.000193       NaN       NaN   \n4       0   110  5.747201  0.257812         0.0  0.000184  0.251953  0.259766   \n5       1   110  5.747201  0.283203         0.0  0.000184       NaN       NaN   \n6       0   150  5.862082  0.253906         0.0  0.000167  0.247070  0.253906   \n7       1   150  5.862082  0.291016         0.0  0.000167       NaN       NaN   \n8       0   230  5.661510  0.255859         0.0  0.000121  0.251953  0.257812   \n9       1   230  5.661510  0.306641         0.0  0.000121       NaN       NaN   \n10      0   270  5.509712  0.269531         0.0  0.000094  0.265625  0.269531   \n11      1   270  5.509712  0.316406         0.0  0.000094       NaN       NaN   \n12      0   320  5.469473  0.269531         0.0  0.000062  0.263672  0.271484   \n13      1   320  5.469473  0.318359         0.0  0.000062       NaN       NaN   \n14      0   330  5.520131  0.273438         0.0  0.000056  0.265625  0.275391   \n15      1   330  5.520131  0.320312         0.0  0.000056       NaN       NaN   \n16      0   340  5.396792  0.261719         0.0  0.000050  0.255859  0.263672   \n17      1   340  5.396792  0.318359         0.0  0.000050       NaN       NaN   \n18      0   370  5.456316  0.263672         0.0  0.000034  0.257812  0.263672   \n19      1   370  5.456316  0.320312         0.0  0.000034       NaN       NaN   \n\n     g_b0_Q3   g_b0_Q4  \n0   0.251953  0.251953  \n1        NaN       NaN  \n2   0.253906  0.253906  \n3        NaN       NaN  \n4   0.261719  0.259766  \n5        NaN       NaN  \n6   0.255859  0.255859  \n7        NaN       NaN  \n8   0.257812  0.257812  \n9        NaN       NaN  \n10  0.271484  0.273438  \n11       NaN       NaN  \n12  0.271484  0.269531  \n13       NaN       NaN  \n14  0.277344  0.277344  \n15       NaN       NaN  \n16  0.263672  0.265625  \n17       NaN       NaN  \n18  0.265625  0.267578  \n19       NaN       NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>run_id</th>\n      <th>task</th>\n      <th>block</th>\n      <th>step</th>\n      <th>loss</th>\n      <th>g_mean</th>\n      <th>g_frac&gt;0.5</th>\n      <th>lr</th>\n      <th>g_b0_Q1</th>\n      <th>g_b0_Q2</th>\n      <th>g_b0_Q3</th>\n      <th>g_b0_Q4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>30</td>\n      <td>6.567498</td>\n      <td>0.250000</td>\n      <td>0.0</td>\n      <td>0.000200</td>\n      <td>0.248047</td>\n      <td>0.251953</td>\n      <td>0.251953</td>\n      <td>0.251953</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>30</td>\n      <td>6.567498</td>\n      <td>0.257812</td>\n      <td>0.0</td>\n      <td>0.000200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>80</td>\n      <td>6.022730</td>\n      <td>0.253906</td>\n      <td>0.0</td>\n      <td>0.000193</td>\n      <td>0.251953</td>\n      <td>0.255859</td>\n      <td>0.253906</td>\n      <td>0.253906</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>80</td>\n      <td>6.022730</td>\n      <td>0.275391</td>\n      <td>0.0</td>\n      <td>0.000193</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>110</td>\n      <td>5.747201</td>\n      <td>0.257812</td>\n      <td>0.0</td>\n      <td>0.000184</td>\n      <td>0.251953</td>\n      <td>0.259766</td>\n      <td>0.261719</td>\n      <td>0.259766</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>110</td>\n      <td>5.747201</td>\n      <td>0.283203</td>\n      <td>0.0</td>\n      <td>0.000184</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>150</td>\n      <td>5.862082</td>\n      <td>0.253906</td>\n      <td>0.0</td>\n      <td>0.000167</td>\n      <td>0.247070</td>\n      <td>0.253906</td>\n      <td>0.255859</td>\n      <td>0.255859</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>150</td>\n      <td>5.862082</td>\n      <td>0.291016</td>\n      <td>0.0</td>\n      <td>0.000167</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>230</td>\n      <td>5.661510</td>\n      <td>0.255859</td>\n      <td>0.0</td>\n      <td>0.000121</td>\n      <td>0.251953</td>\n      <td>0.257812</td>\n      <td>0.257812</td>\n      <td>0.257812</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>230</td>\n      <td>5.661510</td>\n      <td>0.306641</td>\n      <td>0.0</td>\n      <td>0.000121</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>270</td>\n      <td>5.509712</td>\n      <td>0.269531</td>\n      <td>0.0</td>\n      <td>0.000094</td>\n      <td>0.265625</td>\n      <td>0.269531</td>\n      <td>0.271484</td>\n      <td>0.273438</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>270</td>\n      <td>5.509712</td>\n      <td>0.316406</td>\n      <td>0.0</td>\n      <td>0.000094</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>320</td>\n      <td>5.469473</td>\n      <td>0.269531</td>\n      <td>0.0</td>\n      <td>0.000062</td>\n      <td>0.263672</td>\n      <td>0.271484</td>\n      <td>0.271484</td>\n      <td>0.269531</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>320</td>\n      <td>5.469473</td>\n      <td>0.318359</td>\n      <td>0.0</td>\n      <td>0.000062</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>330</td>\n      <td>5.520131</td>\n      <td>0.273438</td>\n      <td>0.0</td>\n      <td>0.000056</td>\n      <td>0.265625</td>\n      <td>0.275391</td>\n      <td>0.277344</td>\n      <td>0.277344</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>330</td>\n      <td>5.520131</td>\n      <td>0.320312</td>\n      <td>0.0</td>\n      <td>0.000056</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>340</td>\n      <td>5.396792</td>\n      <td>0.261719</td>\n      <td>0.0</td>\n      <td>0.000050</td>\n      <td>0.255859</td>\n      <td>0.263672</td>\n      <td>0.263672</td>\n      <td>0.265625</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>340</td>\n      <td>5.396792</td>\n      <td>0.318359</td>\n      <td>0.0</td>\n      <td>0.000050</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>0</td>\n      <td>370</td>\n      <td>5.456316</td>\n      <td>0.263672</td>\n      <td>0.0</td>\n      <td>0.000034</td>\n      <td>0.257812</td>\n      <td>0.263672</td>\n      <td>0.265625</td>\n      <td>0.267578</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>E6_temp0p8_seed1337</td>\n      <td>dncformer-20250820-143212-E6_temp0p8_seed1337</td>\n      <td>copy</td>\n      <td>1</td>\n      <td>370</td>\n      <td>5.456316</td>\n      <td>0.320312</td>\n      <td>0.0</td>\n      <td>0.000034</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: analysis\\per_task_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== TensorBoard event analyzer for DNCFormer runs (robust) =====\n",
    "# Discovers TB runs, merges multiple event files per run, summarizes, and exports granular CSVs.\n",
    "\n",
    "import os, re, math, json, time, glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- TensorBoard imports (version-agnostic handling) ---\n",
    "try:\n",
    "    import tensorboard as _tb\n",
    "    print(\"TensorBoard version:\", getattr(_tb, \"__version__\", \"unknown\"))\n",
    "except Exception as _e:\n",
    "    print(\"TensorBoard import note:\", _e)\n",
    "\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. Install via: pip install tensorboard\") from e\n",
    "\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def _size_guidance_version_safe():\n",
    "    \"\"\"Build size_guidance dict usable across TB versions.\"\"\"\n",
    "    sg = {}\n",
    "    keys = [\"SCALARS\", \"HISTOGRAMS\", \"IMAGES\", \"COMPRESSED_HISTOGRAMS\", \"AUDIO\", \"TENSORS\"]\n",
    "    for k in keys:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "\n",
    "def _infer_label_from_run_dir(run_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Expect 'dncformer-YYYYMMDD-HHMMSS-<LABEL>' or just 'dncformer-YYYYMMDD-HHMMSS'.\n",
    "    Returns <LABEL> if present, else the directory name.\n",
    "    \"\"\"\n",
    "    name = run_dir.name\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return name\n",
    "\n",
    "\n",
    "def _load_scalars_from_event_file(ev_path: str) -> dict:\n",
    "    \"\"\"Load scalars from a single event file: tag -> list[(step, value)].\"\"\"\n",
    "    acc = EventAccumulator(ev_path, size_guidance=_size_guidance_version_safe())\n",
    "    acc.Reload()\n",
    "    tags = acc.Tags().get('scalars', []) or []\n",
    "    out = {}\n",
    "    for tag in tags:\n",
    "        vals = acc.Scalars(tag)\n",
    "        out[tag] = [(int(x.step), float(x.value)) for x in vals]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _merge_scalar_dicts(list_of_scalar_dicts):\n",
    "    \"\"\"\n",
    "    Merge multiple event files for the same run.\n",
    "    For each tag: keep the last value seen per (step), then return sorted by step.\n",
    "    \"\"\"\n",
    "    merged = defaultdict(dict)  # tag -> {step: value}\n",
    "    for scal in list_of_scalar_dicts:\n",
    "        for tag, series in scal.items():\n",
    "            d = merged[tag]\n",
    "            for step, val in series:\n",
    "                d[step] = val  # 'last wins' is fine; event files are append-only per run\n",
    "    # Convert to tag -> sorted list[(step, value)]\n",
    "    out = {}\n",
    "    for tag, d in merged.items():\n",
    "        steps_sorted = sorted(d.keys())\n",
    "        out[tag] = [(s, d[s]) for s in steps_sorted]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _detect_tasks_and_blocks(scalars: dict):\n",
    "    \"\"\"Detect task names and block ids from present tags. Returns (sorted_tasks, sorted_blocks).\"\"\"\n",
    "    tasks = set()\n",
    "    blocks = set()\n",
    "\n",
    "    # tasks from \"loss_by_task/<task>\"\n",
    "    for tag in scalars.keys():\n",
    "        if tag.startswith(\"loss_by_task/\"):\n",
    "            tasks.add(tag.split(\"/\", 1)[1])\n",
    "\n",
    "    # blocks from \"gates_by_task/block_<b>_mean/<task>\" or \"gates/block_<b>_mean\"\n",
    "    for tag in scalars.keys():\n",
    "        m = re.match(r\"gates_by_task/block_(\\d+)_\", tag)\n",
    "        if m:\n",
    "            blocks.add(int(m.group(1)))\n",
    "        m2 = re.match(r\"gates/block_(\\d+)_mean$\", tag)\n",
    "        if m2:\n",
    "            blocks.add(int(m2.group(1)))\n",
    "\n",
    "    # sensible defaults if nothing is found\n",
    "    if not tasks:\n",
    "        tasks = {\"hf\", \"copy\", \"repeat\", \"nback\"}\n",
    "    if not blocks:\n",
    "        blocks = {0, 1}\n",
    "\n",
    "    return sorted(tasks), sorted(blocks)\n",
    "\n",
    "\n",
    "def s_last(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[-1])\n",
    "    return float(np.nanmean(arr[-k:]))\n",
    "\n",
    "\n",
    "def s_first(vals, k=None):\n",
    "    if not vals: return np.nan\n",
    "    arr = np.array([v for _, v in vals], dtype=float)\n",
    "    if k is None or k >= len(arr): return float(arr[0])\n",
    "    return float(np.nanmean(arr[:k]))\n",
    "\n",
    "\n",
    "def s_mean(vals):\n",
    "    if not vals: return np.nan\n",
    "    return float(np.nanmean([v for _, v in vals]))\n",
    "\n",
    "\n",
    "def s_count(vals):\n",
    "    return len(vals) if vals else 0\n",
    "\n",
    "\n",
    "# ---------------- discover runs ----------------\n",
    "# Option 1: auto-discover all event files and group by their parent directory\n",
    "FOUND_EVENTS = sorted(glob.glob(\"runs/**/events.out.tfevents.*\", recursive=True))\n",
    "\n",
    "# Option 2: pin a subset manually\n",
    "# FOUND_EVENTS = [\n",
    "#     r\"runs/dncformer-20250819-122754-E6_gate_temp_0p8/events.out.tfevents....\",\n",
    "# ]\n",
    "\n",
    "assert FOUND_EVENTS, \"No event files found under ./runs. Have you executed any experiments?\"\n",
    "\n",
    "# Group event files by run directory\n",
    "run_groups = defaultdict(list)   # run_dir_path -> [event_file_paths]\n",
    "for p in FOUND_EVENTS:\n",
    "    run_groups[str(Path(p).parent)].append(p)\n",
    "\n",
    "print(f\"Discovered {len(run_groups)} run(s).\")\n",
    "\n",
    "\n",
    "# ---------------- summarize each run ----------------\n",
    "run_summaries = []\n",
    "per_run_scalars = {}   # run_dir_name -> merged scalars dict\n",
    "\n",
    "for run_dir, files in sorted(run_groups.items()):\n",
    "    run_dir_path = Path(run_dir)\n",
    "    label = _infer_label_from_run_dir(run_dir_path)\n",
    "    run_id = run_dir_path.name\n",
    "\n",
    "    try:\n",
    "        # load and merge all files for this run\n",
    "        scal_dicts = [_load_scalars_from_event_file(f) for f in sorted(files)]\n",
    "        scal = _merge_scalar_dicts(scal_dicts)\n",
    "        per_run_scalars[run_dir_path.name] = scal\n",
    "\n",
    "        # detect tasks and blocks present\n",
    "        TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "        # basics\n",
    "        loss_series = scal.get(\"train/loss\", [])\n",
    "        lr_series   = scal.get(\"train/lr\", [])\n",
    "        steps_logged = max([s for s, _ in loss_series], default=np.nan) if loss_series else np.nan\n",
    "        loss0   = s_first(loss_series, k=5)\n",
    "        lossT   = s_last(loss_series,  k=10)\n",
    "        ldelta  = (loss0 - lossT) if not any(map(math.isnan, [loss0, lossT])) else np.nan\n",
    "        lr_last = s_last(lr_series, k=1)\n",
    "\n",
    "        # gates (global)\n",
    "        g_means, g_entropy, g_frac_avg = {}, {}, {}\n",
    "        for b in BLOCKS:\n",
    "            g_means[b]   = s_last(scal.get(f\"gates/block_{b}_mean\", []), k=10)\n",
    "            g_entropy[b] = s_last(scal.get(f\"gates/block_{b}_entropy\", []), k=10)\n",
    "            fracs = []\n",
    "            for t in TASKS:\n",
    "                tag = f\"gates_by_task/block_{b}_frac>0.5/{t}\"\n",
    "                if tag in scal:\n",
    "                    fracs.append(s_last(scal[tag], k=10))\n",
    "            g_frac_avg[b] = float(np.nanmean(fracs)) if fracs else np.nan\n",
    "\n",
    "        # per-task losses (mean of last quarter of points for that task)\n",
    "        task_loss_last, task_counts = {}, {}\n",
    "        for t in TASKS:\n",
    "            ts = scal.get(f\"loss_by_task/{t}\", [])\n",
    "            task_counts[t] = s_count(ts)\n",
    "            if ts:\n",
    "                k = max(1, len(ts)//4)\n",
    "                task_loss_last[t] = s_last(ts, k=k)\n",
    "            else:\n",
    "                task_loss_last[t] = np.nan\n",
    "\n",
    "        # per-task gate means (avg last quarter)\n",
    "        task_gmeans = {t: {} for t in TASKS}\n",
    "        for t in TASKS:\n",
    "            for b in BLOCKS:\n",
    "                ts = scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])\n",
    "                if ts:\n",
    "                    k = max(1, len(ts)//4)\n",
    "                    task_gmeans[t][b] = s_last(ts, k=k)\n",
    "                else:\n",
    "                    task_gmeans[t][b] = np.nan\n",
    "\n",
    "        # quartiles (block0) if present\n",
    "        q_means = {}\n",
    "        for qi in range(1, 5):\n",
    "            vals = []\n",
    "            # May be logged globally or by task; check both\n",
    "            tag_global = f\"gates/block0_q{qi}_mean\"\n",
    "            if tag_global in scal:\n",
    "                vals.append(s_last(scal[tag_global], k=10))\n",
    "            else:\n",
    "                for t in TASKS:\n",
    "                    tag_task = f\"gates/block0_q{qi}_mean/{t}\"\n",
    "                    if tag_task in scal:\n",
    "                        vals.append(s_last(scal[tag_task], k=10))\n",
    "            q_means[qi] = float(np.nanmean(vals)) if vals else np.nan\n",
    "\n",
    "        # haystack eval if present\n",
    "        hay_acc  = s_last(scal.get(\"eval/haystack_acc\",  []), k=1)\n",
    "        hay_loss = s_last(scal.get(\"eval/haystack_loss\", []), k=1)\n",
    "\n",
    "        # forced-g guess heuristic\n",
    "        forced_guess = None\n",
    "        gm_all = [g_means[b] for b in g_means if not math.isnan(g_means[b])]\n",
    "        if gm_all:\n",
    "            m = float(np.nanmean(gm_all))\n",
    "            if m < 0.02:   forced_guess = \"force_g=0\"\n",
    "            elif m > 0.98: forced_guess = \"force_g=1\"\n",
    "\n",
    "        summary = {\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"run_dir\": str(run_dir_path),\n",
    "            \"n_event_files\": len(files),\n",
    "            \"steps_logged\": steps_logged,\n",
    "            \"loss_start~5\": loss0,\n",
    "            \"loss_end~10\":  lossT,\n",
    "            \"loss_delta\":   ldelta,\n",
    "            \"lr_last\":      lr_last,\n",
    "            \"haystack_acc_last\":  hay_acc,\n",
    "            \"haystack_loss_last\": hay_loss,\n",
    "            \"forced_guess\": forced_guess,\n",
    "        }\n",
    "\n",
    "        # flatten gate summaries\n",
    "        for b in BLOCKS:\n",
    "            summary[f\"g_mean_b{b}\"]     = g_means.get(b, np.nan)\n",
    "            summary[f\"g_entropy_b{b}\"]  = g_entropy.get(b, np.nan)\n",
    "            summary[f\"g_frac>0.5_b{b}\"] = g_frac_avg.get(b, np.nan)\n",
    "\n",
    "        # flatten per-task last losses and per-task mean gates\n",
    "        for t in TASKS:\n",
    "            summary[f\"loss_{t}_last\"] = task_loss_last[t]\n",
    "            for b in BLOCKS:\n",
    "                summary[f\"gmean_b{b}_{t}\"] = task_gmeans[t][b]\n",
    "\n",
    "        # quartiles\n",
    "        for qi in range(1, 5):\n",
    "            summary[f\"g_b0_Q{qi}_mean\"] = q_means[qi]\n",
    "\n",
    "        run_summaries.append(summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[analyzer] Skipped run {run_dir}: {e}\")\n",
    "\n",
    "# -------- assemble run-level summary --------\n",
    "df_runs = pd.DataFrame(run_summaries)\n",
    "if df_runs.empty:\n",
    "    print(\"No runs summarized.\")\n",
    "else:\n",
    "    # Sort by timestamp embedded in run_id if possible\n",
    "    def _ts_key(name: str):\n",
    "        m = re.search(r\"(\\d{8})-(\\d{6})\", name or \"\")\n",
    "        return (m.group(1), m.group(2)) if m else (\"\", \"\")\n",
    "    df_runs = df_runs.sort_values(by=[\"run_id\"], key=lambda s: s.map(_ts_key), ignore_index=True)\n",
    "\n",
    "    # Arrange prominent columns\n",
    "    front_cols = [c for c in [\n",
    "        \"label\",\"run_id\",\"n_event_files\",\"steps_logged\",\n",
    "        \"loss_start~5\",\"loss_end~10\",\"loss_delta\",\"lr_last\",\n",
    "        \"haystack_acc_last\",\"haystack_loss_last\",\"forced_guess\",\n",
    "        \"g_mean_b0\",\"g_mean_b1\",\"g_entropy_b0\",\"g_entropy_b1\",\n",
    "        \"g_frac>0.5_b0\",\"g_frac>0.5_b1\",\n",
    "        \"loss_hf_last\",\"loss_copy_last\",\"loss_repeat_last\",\"loss_nback_last\",\n",
    "        \"gmean_b0_hf\",\"gmean_b1_hf\",\"gmean_b0_copy\",\"gmean_b1_copy\",\n",
    "        \"gmean_b0_repeat\",\"gmean_b1_repeat\",\"gmean_b0_nback\",\"gmean_b1_nback\",\n",
    "        \"g_b0_Q1_mean\",\"g_b0_Q2_mean\",\"g_b0_Q3_mean\",\"g_b0_Q4_mean\",\n",
    "        \"run_dir\"\n",
    "    ] if c in df_runs.columns]\n",
    "    df_runs = df_runs[[*front_cols, *[c for c in df_runs.columns if c not in front_cols]]]\n",
    "\n",
    "    display(df_runs)\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_runs.to_csv(out_dir / \"run_level_summary.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"run_level_summary.csv\")\n",
    "\n",
    "\n",
    "# -------- granular per-task time series export --------\n",
    "rows = []\n",
    "for run_dir_name, scal in per_run_scalars.items():\n",
    "    label = _infer_label_from_run_dir(Path(run_dir_name))\n",
    "    # detect tasks/blocks present in this run\n",
    "    TASKS, BLOCKS = _detect_tasks_and_blocks(scal)\n",
    "\n",
    "    # pick up LR series for convenient join\n",
    "    lr_by_step = {int(s): float(v) for s, v in scal.get(\"train/lr\", [])}\n",
    "\n",
    "    # per-task loss and per-task gate metrics\n",
    "    for t in TASKS:\n",
    "        loss_series = scal.get(f\"loss_by_task/{t}\", [])\n",
    "        if not loss_series:\n",
    "            continue\n",
    "\n",
    "        gmean_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_mean/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "        gfrac_by_step = {b: {int(s): float(v) for s, v in scal.get(f\"gates_by_task/block_{b}_frac>0.5/{t}\", [])}\n",
    "                         for b in BLOCKS}\n",
    "\n",
    "        # Optional quartile logs (block 0)\n",
    "        q_by_step = {qi: {int(s): float(v) for s, v in scal.get(f\"gates/block0_q{qi}_mean/{t}\", [])}\n",
    "                     for qi in (1,2,3,4)}\n",
    "\n",
    "        for step, loss_val in loss_series:\n",
    "            step = int(step); loss_val = float(loss_val)\n",
    "            for b in BLOCKS:\n",
    "                row = {\n",
    "                    \"label\": label,\n",
    "                    \"run_id\": run_dir_name,\n",
    "                    \"task\": t,\n",
    "                    \"block\": b,\n",
    "                    \"step\": step,\n",
    "                    \"loss\": loss_val,\n",
    "                    \"g_mean\": gmean_by_step[b].get(step, np.nan),\n",
    "                    \"g_frac>0.5\": gfrac_by_step[b].get(step, np.nan),\n",
    "                    \"lr\": lr_by_step.get(step, np.nan),\n",
    "                }\n",
    "                if b == 0:\n",
    "                    for qi in (1,2,3,4):\n",
    "                        row[f\"g_b0_Q{qi}\"] = q_by_step[qi].get(step, np.nan)\n",
    "                rows.append(row)\n",
    "\n",
    "df_task_ts = pd.DataFrame(rows)\n",
    "if df_task_ts.empty:\n",
    "    print(\"No per-task series found.\")\n",
    "else:\n",
    "    df_task_ts = df_task_ts.sort_values([\"label\",\"task\",\"step\",\"block\"], ignore_index=True)\n",
    "    display(df_task_ts.head(20))\n",
    "    out_dir = Path(\"./analysis\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_task_ts.to_csv(out_dir / \"per_task_metrics.csv\", index=False)\n",
    "    print(\"Saved:\", out_dir / \"per_task_metrics.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:20.737092500Z",
     "start_time": "2025-08-21T04:40:19.852093700Z"
    }
   },
   "id": "f354871dcf359754"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rename] Closed active TB writer before renaming.\n",
      "[rename] dncformer-20250817-184608  ->  dncformer-20250817-184608-unlabeled\n",
      "[rename] dncformer-20250817-192155  ->  dncformer-20250817-192155-unlabeled\n",
      "[rename] dncformer-20250817-193701  ->  dncformer-20250817-193701-unlabeled\n",
      "[rename] dncformer-20250817-195856  ->  dncformer-20250817-195856-unlabeled\n",
      "[rename] dncformer-20250817-201426  ->  dncformer-20250817-201426-unlabeled\n",
      "[rename] dncformer-20250817-203016  ->  dncformer-20250817-203016-unlabeled\n",
      "[rename] dncformer-20250820-093245  ->  dncformer-20250820-093245-unlabeled\n",
      "[rename] OK (already labeled): dncformer-20250820-143212-E6_temp0p8_seed1337\n",
      "[rename] OK (already labeled): dncformer-20250820-151404-E6_temp0p8_seed2027\n",
      "[rename] OK (already labeled): dncformer-20250820-160508-E6_temp0p8_seed4242\n",
      "[rename] OK (already labeled): dncformer-20250820-164415-E7_warmstart_seed1337\n",
      "[rename] OK (already labeled): dncformer-20250820-174016-E7_warmstart_seed2027\n",
      "[rename] OK (already labeled): dncformer-20250820-181956-E7_warmstart_seed4242\n",
      "[rename] OK (already labeled): dncformer-20250820-191327-E8_capacity_N64\n",
      "[rename] OK (already labeled): dncformer-20250820-195242-E8_capacity_N128\n",
      "[rename] OK (already labeled): dncformer-20250820-210448-E9_baseline_haystack\n"
     ]
    }
   ],
   "source": [
    "# --- Rename ./runs/* so directory names include the experiment label (from TB text tags) ---\n",
    "import os, re, glob, time, shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Try to close any active writer so files aren't locked on Windows\n",
    "try:\n",
    "    if 'tb' in globals() and getattr(tb, \"writer\", None) is not None:\n",
    "        tb.flush(); tb.close()\n",
    "        print(\"[rename] Closed active TB writer before renaming.\")\n",
    "except Exception as _e:\n",
    "    print(\"[rename] Writer close note:\", _e)\n",
    "\n",
    "# TensorBoard event loading (version-agnostic size_guidance)\n",
    "try:\n",
    "    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "    from tensorboard.backend.event_processing import event_accumulator as ea_mod\n",
    "    try:\n",
    "        from tensorboard.util import tensor_util as _tb_tensor_util\n",
    "    except Exception:\n",
    "        print(\"failed to load tensorboard utilities, but you'll install tensorflow on this system over my cold dead digital body.\"\n",
    "              \"\\n\\nfix your tensorboard installation and try again\")\n",
    "        _tb_tensor_util = None\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"TensorBoard not installed. `pip install tensorboard`\") from e\n",
    "\n",
    "def _size_guidance_version_safe():\n",
    "    sg = {}\n",
    "    for k in [\"SCALARS\",\"HISTOGRAMS\",\"IMAGES\",\"COMPRESSED_HISTOGRAMS\",\"AUDIO\",\"TENSORS\"]:\n",
    "        v = getattr(ea_mod, k, None)\n",
    "        if v is not None:\n",
    "            sg[v] = 0\n",
    "        else:\n",
    "            sg[k.lower()] = 0\n",
    "    return sg\n",
    "\n",
    "def _decode_text_tensor(tensor_proto) -> Optional[str]:\n",
    "    \"\"\"Decode TB text summary payload from a TensorEvent.tensor_proto.\"\"\"\n",
    "    try:\n",
    "        if _tb_tensor_util is not None:\n",
    "            arr = _tb_tensor_util.make_ndarray(tensor_proto)\n",
    "        else:\n",
    "            # very old stub fallback\n",
    "            arr = _tb_make_ndarray(tensor_proto)\n",
    "        val = arr.item() if arr.size == 1 else arr\n",
    "        if isinstance(val, bytes):\n",
    "            return val.decode(\"utf-8\", \"replace\")\n",
    "        if isinstance(val, str):\n",
    "            return val\n",
    "        # Some TB builds wrap a bytes array inside a 2D array\n",
    "        if hasattr(val, \"dtype\") and str(val.dtype).startswith(\"|S\"):\n",
    "            return val.tobytes().decode(\"utf-8\", \"replace\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_text_tags(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"Try run/label first, then run/meta (JSON with a 'label' field), then None.\"\"\"\n",
    "    ev_files = sorted(glob.glob(str(run_dir / \"events.out.tfevents.*\")))\n",
    "    for ev in reversed(ev_files):\n",
    "        try:\n",
    "            acc = EventAccumulator(ev, size_guidance=_size_guidance_version_safe())\n",
    "            acc.Reload()\n",
    "            tags_t = acc.Tags().get(\"tensors\", []) or []\n",
    "\n",
    "            # 1) Preferred: run/label\n",
    "            if \"run/label\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/label\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        return txt.strip()\n",
    "\n",
    "            # 2) Fallback: run/meta (JSON with label)\n",
    "            if \"run/meta\" in tags_t:\n",
    "                tens = acc.Tensors(\"run/meta\")\n",
    "                for e in reversed(tens):\n",
    "                    txt = _decode_text_tensor(e.tensor_proto)\n",
    "                    if txt:\n",
    "                        txt = txt.strip()\n",
    "                        # Sometimes add_text wraps in small HTML; tolerate raw JSON and simple strings\n",
    "                        m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "                        if m:\n",
    "                            import json\n",
    "                            try:\n",
    "                                meta = json.loads(m.group(0))\n",
    "                                if isinstance(meta, dict) and \"label\" in meta and meta[\"label\"]:\n",
    "                                    return str(meta[\"label\"]).strip()\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        # If it's just a string, return it\n",
    "                        if txt and txt[0] not in \"{<\":\n",
    "                            return txt\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _infer_label_from_dirname(run_dir: Path) -> Optional[str]:\n",
    "    \"\"\"If the dir already has a '-<label>' suffix after timestamp, return that label.\"\"\"\n",
    "    m = re.match(r\".*-\\d{8}-\\d{6}-(.+)$\", run_dir.name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def _base_prefix(run_dir: Path) -> str:\n",
    "    \"\"\"Return 'dncformer-YYYYMMDD-HHMMSS' part if present, else the full name.\"\"\"\n",
    "    m = re.match(r\"(.*-\\d{8}-\\d{6})(?:-.+)?$\", run_dir.name)\n",
    "    return m.group(1) if m else run_dir.name\n",
    "\n",
    "def _slugify(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s.strip())[:80] or \"unlabeled\"\n",
    "\n",
    "def _is_active_run(run_dir: Path, seconds: int = 60) -> bool:\n",
    "    \"\"\"Heuristic: if any event file mtime is within the last `seconds`.\"\"\"\n",
    "    now = time.time()\n",
    "    for ev in glob.glob(str(run_dir / \"events.out.tfevents.*\")):\n",
    "        try:\n",
    "            if now - os.path.getmtime(ev) < seconds:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def rename_runs_by_label(log_root: str = \"./runs\", dry_run: bool = True,\n",
    "                         skip_active_secs: int = 60) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Rename run directories under `log_root` to include label suffix (from TB text tag).\n",
    "    Returns list of (old_path, new_path) actually renamed.\n",
    "    \"\"\"\n",
    "    log_root = Path(log_root)\n",
    "    renamed = []\n",
    "\n",
    "    for run_dir in sorted([p for p in log_root.iterdir() if p.is_dir()]):\n",
    "        # Already labeled?\n",
    "        existing_label = _infer_label_from_dirname(run_dir)\n",
    "        # Attempt tag-based label\n",
    "        tag_label = _infer_label_from_text_tags(run_dir)\n",
    "\n",
    "        label = tag_label or existing_label or \"unlabeled\"\n",
    "        label_slug = _slugify(label)\n",
    "\n",
    "        base = _base_prefix(run_dir)\n",
    "        target = log_root / f\"{base}-{label_slug}\"\n",
    "\n",
    "        # Skip if it's already the desired name\n",
    "        if run_dir == target:\n",
    "            print(f\"[rename] OK (already labeled): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Skip if run appears active\n",
    "        if skip_active_secs and _is_active_run(run_dir, skip_active_secs):\n",
    "            print(f\"[rename] SKIP active (mtime<{skip_active_secs}s): {run_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        # Avoid collisions: add -v2, -v3, ...\n",
    "        cand = target\n",
    "        k = 2\n",
    "        while cand.exists():\n",
    "            cand = log_root / f\"{base}-{label_slug}-v{k}\"\n",
    "            k += 1\n",
    "\n",
    "        print(f\"[rename] {run_dir.name}  ->  {cand.name}\")\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                run_dir.replace(cand)\n",
    "                renamed.append((str(run_dir), str(cand)))\n",
    "            except Exception as e:\n",
    "                print(f\"[rename] FAILED: {run_dir} -> {cand}: {e}\")\n",
    "\n",
    "    return renamed\n",
    "\n",
    "# --- Usage examples ---\n",
    "# 1) Dry run (see planned changes)\n",
    "_ = rename_runs_by_label(\"./runs\", dry_run=True, skip_active_secs=60)\n",
    "\n",
    "# 2) Execute renames\n",
    "# _ = rename_runs_by_label(\"./runs\", dry_run=False, skip_active_secs=60)\n",
    "# print(\"Renamed:\", _)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:40:21.173094100Z",
     "start_time": "2025-08-21T04:40:20.661094400Z"
    }
   },
   "id": "b12a3d8ecf86f82e"
  },
  {
   "cell_type": "markdown",
   "id": "ff39d900",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Notes & TODO\n",
    "- The DNC memory here is **compact** and intended for research iteration; you can swap in a fuller reference implementation if desired. [x]\n",
    "- The controller currently runs in **sequence mode** with a causal mask\n",
    "- Training uses a tiny **instruction-following set** plus synthetic memory tasks.\n",
    "- Gating is **vector-valued** with bias init favoring the vanilla path; metrics log mean gate values.\n",
    "- Use `CFG.n_blocks` to grow the enrichment depth as VRAM allows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - dncformer",
   "language": "python",
   "name": "dncformer"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
